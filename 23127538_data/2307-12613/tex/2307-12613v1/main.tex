\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}

\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}
\renewcommand\qedsymbol{$\blacksquare$}

\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\renewcommand{\r}{\mathbf{r}}
\newcommand{\St}{\mathbb{S}}
\newcommand{\T}{\mathbf{T}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\U}{\mathbf{U}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\Z}{\mathbf{Z}}

\newcommand{\GAM}{\boldsymbol{\Gamma}}
\newcommand{\ALPH}{\boldsymbol{\Phi}}
\newcommand{\BET}{\boldsymbol{\Psi}}

\newcommand{\Nc}{\mathcal{N}}

\newcommand{\DElta}{{\boldsymbol{\delta}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\id}{\mathbf{Id}}
\newcommand{\LAMBDA}{\boldsymbol{\Lambda}}
\newcommand{\Mu}{\boldsymbol{\mu}}
\newcommand{\SIgma}{\boldsymbol{\sigma}}
\newcommand{\SIGMA}{\boldsymbol{\Sigma}}
%\newcommand{\SIGMADITH}{\tilde{\boldsymbol{\Sigma}}^{\operatorname{dith}}}
\newcommand{\SIGMADITH}{\boldsymbol{\Sigma}^{\operatorname{dith}}}
%\newcommand{\SIGMAADAP}{\tilde{\boldsymbol{\Sigma}}^{\operatorname{adap}}}
\newcommand{\SIGMAADAP}{\boldsymbol{\Sigma}^{\operatorname{adap}}}
\newcommand{\Tau}{\boldsymbol{\tau}}
\newcommand{\ttheta}{\boldsymbol{\theta}}
\newcommand{\XI}{\boldsymbol{\Xi}}
\newcommand{\THETA}{\boldsymbol{\Theta}}
\newcommand{\DELTA}{\boldsymbol{\Delta}}
\newcommand{\0}{\boldsymbol{0}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\W}{\boldsymbol{W}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\EE}[2][]{\;\left[\mathbb{E}_{#1}\!#2\right]}
\newcommand{\EEE}[2][]{\;\mathbb{E}_{#1}\!#2}
\renewcommand{\P}[2][]{\;\mathbb{P}_{#1}\!\left[#2\right]}
\newcommand{\pnorm}[2]{\left\| #1 \right\|_{#2}}
\newcommand{\tnorm}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
%\newcommand\tnorm[1]{\vert\vert\vert\mskip2mu #1\mskip2mu \vert\vert\vert}
\newcommand{\abs}[2]{\left| #1 \right|_{#2}}
\newcommand{\inner}[2]{\left\langle #1 \right\rangle_{#2}}
\newcommand{\round}[1]{\left( #1 \right)}
\renewcommand{\square}[1]{\left[ #1 \right]}
\newcommand{\curly}[1]{\left\{ #1 \right\} }

\newcommand{\1}[1]{\chi_{ \left\{ #1 \right\} }}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\dx}[1]{\; \mathrm{d} #1}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Om}{\Omega}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}

\DeclareMathOperator*{\argmax}{argmax}

\definecolor{tumor}{HTML}{E37222}
\definecolor{tumred}{RGB}{205,32,44}
\definecolor{tumblue}{RGB}{00,115,207}
\definecolor{tumgreen}{RGB}{162,173,000}
\newcommand{\red}[1]{{\color{tumred} #1}}
%\newcommand{\sjoerd}[1]{{\color{blue}{#1}}}
%\newcommand{\sjoerdred}[1]{{\color{red}{#1}}}
%\newcommand{\sjoerdgreen}[1]{{\color{green}{#1}}}
%\newcommand{\johannes}[1]{\textcolor{purple}{#1}}
%\newcommand{\red}[1]{{#1}}
\newcommand{\sjoerd}[1]{{{#1}}}
\newcommand{\sjoerdred}[1]{{{#1}}}
\newcommand{\sjoerdnewred}[1]{\textcolor{red}{#1}}
\newcommand{\sjoerdgreen}[1]{{{#1}}}
\newcommand{\johannes}[1]{{#1}}
\newcommand{\johannesnew}[1]{{\textcolor{green}{#1}}}

\newcommand{\noteJ}[1]{{\textcolor{teal}{\textbf{JM:} #1 }}}
\newcommand{\todo}[1]{ \textcolor{red}{\textbf{ToDo:} #1 } }

% Reviewer
%\newcommand{\rev}[1]{{\color{red} #1}}
\newcommand{\rev}[1]{#1}
% Associate Editor
%\renewcommand{\ae}[1]{{\color{blue} #1}}
\renewcommand{\ae}[1]{#1}


\allowdisplaybreaks

%\title{Covariance Estimation under One-bit Quantization}
%\author{Sjoerd Dirksen, Johannes Maly, Holger Rauhut}
%\date{}

%\title{Adaptive dithering for quantized covariance estimation}
\title{Tuning-free one-bit covariance estimation \\ using data-driven dithering}
\author[$\star$]{Sjoerd Dirksen}
\author[$\dagger$,$\ddagger$]{Johannes Maly}
\affil[$\star$]{\normalsize Mathematical Institute, Utrecht University, The Netherlands}
\affil[$\dagger$]{Department of Mathematics, LMU Munich, Germany}
\affil[$\ddagger$]{Munich Center for Machine Learning (MCML)}
\date{}


\begin{document}

\maketitle

\begin{abstract}
\noindent We consider covariance estimation of any subgaussian distribution from finitely many i.i.d.\ samples that are quantized to one bit of information per entry. Recent work has shown that a reliable estimator can be constructed if uniformly distributed dithers on $[-\lambda,\lambda]$ are used in the one-bit quantizer. This estimator enjoys near-minimax optimal, non-asymptotic error estimates in the operator and Frobenius norms if $\lambda$ is chosen proportional to the largest variance of the distribution. However, this quantity is not known a-priori, and in practice $\lambda$ needs to be carefully tuned to achieve good performance. In this work we resolve this problem by introducing a tuning-free variant of this estimator, which replaces $\lambda$ by a data-driven quantity. We prove that this estimator satisfies the same non-asymptotic error estimates --- up to small (logarithmic) losses and a slightly worse probability estimate. Our proof relies on a new version of the Burkholder-Rosenthal inequalities for matrix martingales, which is expected to be of independent interest.   
\end{abstract}
  
%    Only recently it has been shown that the covariance matrix of any subgaussian distribution can be reliably estimated from finitely many iid samples that are quantized to two bits of information per entry. Optimal performance of the corresponding estimator however requires  tuning of hyperparameters based on oracle knowledge of the unknown covariance matrix.
%    In the present paper, we propose and analyze a novel data-adaptive variant of the existing two-bit estimator. This new variant is parameter-tuning free and comes with comparable non-asymptotic error bounds.
%\end{abstract}
%
%\red{Some things to keep in mind/Todos:
%\begin{itemize}
%    \item Title?
%    \item Acknowledgements
%\end{itemize}
%}

\section{Introduction}
\label{sec:Introduction}

A classical problem in multivariate statistics is to estimate covariance matrices of high-dimensional probability distributions from a finite number of samples. Application areas in which covariance estimation is of relevance include financial mathematics \cite{ledoit2003improved}, pattern recognition \cite{dahmen2000structured}, and signal processing and transmission \sjoerd{\cite{haghighatshoar2018low,krim1996two}}. In the past decades, a whole line of research has been devoted to quantifying how many i.i.d.\ samples $\X_1,...,\X_n \overset{\mathrm{d}}{\sim} \X$ of a mean-zero\footnote{Since estimating and subtracting the mean of the distribution can be considered as a data pre-processing step, most works assume that $\X$ is zero mean for simplicity.} random vector $\X \in \R^p$ suffice to reliably approximate the covariance matrix $\SIGMA= \E(\X\X^T) \in \R^{p\times p}$ up to a given accuracy. It is widely known that the covariance matrix of well-behaved distributions, such as Gaussian distributions, can be estimated via the sample covariance matrix with a near-optimal sampling rate $n \sim p$ \cite{cai2010optimal}. In the last two decades, more sophisticated estimators have been devised to obtain similar results for heavy-tailed distributions (see, e.g., \cite{ke2019user,mendelson2018robust} and the references therein) 
%\cite{adamczak2010quantitative,adamczak2011sharp,cai2010optimal,ke2019user,koltchinskii2017,mendelson2018robust,srivastava2013covariance,vershynin2012close}
or to further reduce the sample complexity by leveraging structures of $\SIGMA$ such as sparsity \cite{bickel2008covariance,chen2012masked,levina2012partial}, low rank \cite{ke2019user}, or Toeplitz structure \cite{cai2013optimal,kabanava2017masked,Maly2022}. 
%\begin{align} \label{eq:SampleMean}
%\hat{\SIGMA}_n = \frac{1}{n} \sum_{k=1}^n \X^k (\X^k)^T
%\end{align}

In our recent work \cite{dirksen2021covariance}, we considered a very different challenge in covariance estimation, namely \textit{coarse quantization} of the samples, i.e., each entry of the vectors $\X_k$ is mapped to one or two bits of information by a chosen \textit{quantizer}. Coarse quantization occurs naturally in various applications. In signal processing, analog samples are collected via sensors and hence need to be quantized to finitely many bits before they can be digitally transmitted. The number of quantization bits determines the energy efficiency of a sensor to a significant degree. If the number of sensors is large, fine quantization can be prohibitive to use. Even in purely digital systems, coarse quantization can be attractive to reduce storage and processing costs, e.g., in the context of neural networks, see for instance the surveys \cite{guo2018survey,  deng2020model, gholami2021survey} and the recent rigorous mathematical studies \cite{lybrand2021greedy,maly2022simple}. Our current work is motivated by covariance estimation problems arising in signal processing, especially with large antenna arrays \cite{jacovitti1994estimation,roth2015covariance,bar2002doa,choi2016near,li2017channel}. We discuss this direction in more detail in Section~\ref{sec:RelatedWork}. 

In this paper we focus on memoryless one-bit quantizers of the form $Q:\R^p\to\{-1,1\}^p$, $Q(\X) = \sign(\X+\Tau)$, where $\sign$ is the sign function acting element-wise on its input and $\Tau\in \R^p$ represents a vector of quantization thresholds.\footnote{This is an example of a \emph{memoryless scalar quantizer}, which means that vectors are quantized entry-wise and vector entries are quantized independently of each other. This type of quantization is very popular in the literature, as it allows for a simple, energy efficient hardware implementation. For alternative, more elaborate vector quantization schemes we refer to, e.g., \cite{gersho2012vector,gray1987oversampled,tewksbury1978oversampled}.} If $\Tau$ is random, then this method is called one-bit quantization with \emph{dithering} in engineering literature. Our starting point is the work \cite{dirksen2021covariance}, which proposed a setting in which each sample $\X_k$ is quantized entry-wise to two bits
\begin{align} \label{eq:TwoBitSamples}
    \sign(\X_k + \Tau_k) \quad \text{ and } \quad \sign(\X_k + \bar{\Tau}_k),
\end{align}
where the dithering vectors $\Tau_1,\bar{\Tau}_1,\ldots,\Tau_n,\bar{\Tau}_n$ are independent and uniformly distributed in $[-\lambda,\lambda]^p$, for fixed $\lambda>0$. The authors of \cite{dirksen2021covariance} proposed to recover $\SIGMA$ by the estimator
\begin{align} \label{eq:TwoBitEstimator}
\SIGMADITH_n = \frac{1}{2}\hat{\SIGMA}_n + \frac{1}{2}(\hat{\SIGMA}_n)^T,
\end{align}
where
\begin{align} \label{eq:AsymmetricEstimator}
\hat{\SIGMA}_n = \frac{\lambda^2}{n}\sum_{k=1}^n \sign(\X_k + \Tau_k)\sign(\X_k + \bar{\Tau}_k)^T
\end{align}
is a re-scaled and asymmetric version of the sample covariance matrix of the samples in \eqref{eq:TwoBitSamples}. To formulate the estimation error estimate from \cite{dirksen2021covariance}, we denote for $\Z \in \R^{p\times p}$ its operator norm by $\pnorm{\Z}{}~=~\sup_{\u \in \mathbb{S}^{p-1}} \pnorm{\Z\u}{2}$, its entry-wise max-norm by $\pnorm{\Z}{\infty} = \max_{i,j} \abs{Z_{i,j}}{}$, and its maximum column norm \sjoerd{by} $\pnorm{\Z}{1\rightarrow 2} = \max_{j \in [p]} \pnorm{\z_j}{2}$, where $\z_j$ denotes the $j$-th column of $\Z$. We use $\lesssim_K$ to write an inequality that hides a multiplicative positive constant that only depends on $K$ and write $a \simeq_K b$ if $a \lesssim_K b \lesssim_K a$.

\begin{theorem}[{\cite[Theorem 4]{dirksen2021covariance}}] \label{thm:OperatorDitheredMask_Old}
%	Let $\X$ be a mean-zero, $K$-subgaussian\footnote{The formal definition of a subgaussian random vector can be found in Section \ref{sec:Notation} below.} vector with covariance matrix \sjoerd{$\SIGMA$}. Let $\X^1,...,\X^n \overset{\mathrm{\sjoerd{i.i.d.}}}{\sim} \X$. If $\lambda^2 \gtrsim_{\sjoerdgreen{K}} \log(n) \|\SIGMA\|_{\infty}$, then with probability at least $1-e^{-t}$, 
Let $\X$ be a mean-zero, $K$-subgaussian\footnote{The formal definition of a subgaussian random vector is provided in Section \ref{sec:Notation} below.} vector with covariance matrix \sjoerd{$\SIGMA$}. Let $\X_1,...,\X_n \overset{\mathrm{\sjoerd{i.i.d.}}}{\sim} \X$. Let $\M \in [0,1]^{p\times p}$ be a fixed symmetric mask. If $\lambda^2 \gtrsim_{\sjoerdgreen{K}} \log(n) \|\SIGMA\|_{\infty}$, then with probability at least $1-e^{-t}$,  
	$$\pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{}\lesssim_{\sjoerdgreen{K}}
	\|\M\|_{1\to 2}(\lambda\|\SIGMA\|^{1/2}+\lambda^2)\sqrt{\frac{\log(p)+t}{n}} + \lambda^2\|\M\| \frac{\log(p)+t}{n},$$
    where $\odot$ denotes the Hadamard (i.e., entry-wise) product.
	In particular, if $\lambda^2 \simeq_{\sjoerdgreen{K}} \log(n) \|\SIGMA\|_{\infty}$, then
	\begin{align}
	\label{eqn:OperatorDitheredMask}
	\begin{split}
	    &\pnorm{\M \odot  \SIGMADITH_n - \M \odot \SIGMA}{}  \lesssim_{\sjoerdgreen{K}}
	    \log(n) \|\M\|_{1\to 2}\sqrt{\frac{\|\SIGMA\| \ \|\SIGMA\|_{\infty}(\log(p)+t)}{n}} + \log(n)\|\M\|\|\SIGMA\|_{\infty}\frac{\log(p)+t}{n}. 
	\end{split}
	\end{align}	
%	
%	
%	
%	% and $\| \cdot \|_\infty$ denotes the entry-wise max-norm, 
%	$$\pnorm{\SIGMADITH_n - \SIGMA}{}\lesssim_{\sjoerdgreen{K}}
%	(\lambda\|\SIGMA\|^{1/2}+\lambda^2)\sqrt{\frac{p(\log(p)+t)}{n}} + \lambda^2 \frac{p(\log(p)+t)}{n}.$$
%%    where $\| \cdot \|$ denotes the operator norm.
%    In particular, if $\lambda^2 \simeq_{\sjoerdgreen{K}} \log(n) \|\SIGMA\|_{\infty}$, then
%	\begin{align}
%	\label{eqn:OperatorDitheredMask}
%%	\begin{split}
%	    \pnorm{  \SIGMADITH_n - \SIGMA}{} \lesssim_{\sjoerdgreen{K}}
%	    \log(n) \sqrt{\frac{\|\SIGMA\| \ \|\SIGMA\|_{\infty} p (\log(p)+t)}{n}} + \log(n)\|\SIGMA\|_{\infty}\frac{p(\log(p)+t)}{n}. 
%%	\end{split}
%	\end{align}
\end{theorem}

Note that this result incorporates sparsity priors on $\SIGMA$ in the form of the mask $\M$ and is of the same shape as state-of-the-art performance estimates for masked estimators in the `unquantized' setting \cite{chen2012masked}. If $\SIGMA$ has no structure, then one can take a trivial all-ones mask (in which case $\|\M\|_{1\to 2}=\sqrt{p}$ and $\|\M\|=p$) to recover the minimax optimal rate up to log-factors. Analogous results for Frobenius- and entry-wise max-norm estimation errors can be found in \cite{yang2023plug}.

\subsection{Main results}
\label{sec:MainResults}

As was already discussed in \cite{dirksen2021covariance}, the main weakness of Theorem \ref{thm:OperatorDitheredMask_Old} lies in the required oracle knowledge of $\SIGMA$ for tuning the hyperparameter $\lambda$. Indeed, any choice of $\lambda$ that is too small or too large will either break the guarantees or deteriorate the error bound, and this reflects the actual behavior of \eqref{eq:TwoBitEstimator}: as $\lambda$ increases a U-shaped performance curve can consistently be observed in numerical experiments \cite{dirksen2021covariance,yang2023plug}. The goal of this work is to resolve this issue.
%
%
%  In this work, we thus propose and analyze data adaptive variants of \eqref{eq:TwoBitSamples} and \eqref{eq:TwoBitEstimator} to remove the dependence on oracle knowledge from Theorem \ref{thm:OperatorDitheredMask_Old}. 
%
%To be more precise, we present our new data adaptive estimator in Section \ref{sec:MainResults} together with non-asymptotic error bounds in terms of entry-wise max-norm, Frobenius norm, and operator norm, see Theorems~\ref{thm:FrobeniusDitheredMask}~and~\ref{thm:OperatorDitheredMask}. With Theorem~\ref{thm:matrixBRintro} the section furthermore provides a new matrix version of the Burkholder-Rosenthal inequality for martingale differences that we use to derive our estimates and which will be of independent interest to some readers. Section~\ref{sec:RelatedWork} then discusses our results in light of recent research on covariance estimation from quantized samples and Section~\ref{sec:Proofs} contains the proofs of all our claims. 
%%We conclude in Section~\ref{sec:Conclusion} with a brief summary.
%
%
%
%\subsection{Data adaptive quantization and estimation}
%
%Theorem \ref{thm:OperatorDitheredMask_Old} illustrates that if the measurement model in \eqref{eq:TwoBitSamples} is applied for fixed dithering parameter $\lambda > 0$, the estimator in \eqref{eq:TwoBitEstimator} requires a careful and oracle knowledge based tuning of $\lambda$.
%To overcome this, we consider 
The key is to consider a quantizer with \emph{data-adaptive dithering}: instead of using uniform dithers with a fixed parameter $\lambda$ that scales in terms of the (unknown) maximal variance $\|\SIGMA\|_{\infty}$, we will use dithering parameters that estimate (a scaled version of) $\|\SIGMA\|_{\infty}$. Specifically, we assume\footnote{Note that we added an additional sample $\X_0$ for convenience, which is only used to provide an initial estimate of the adaptive dither.} that $\X_0,...,\X_n \overset{\mathrm{i.i.d.}}{\sim} \X$ and again consider quantized samples of the form
\begin{align} \label{eq:TwoBitSamplesAdaptive}
    \sign(\X_k + \Tau_k) \quad \text{ and } \quad \sign(\X_k + \bar{\Tau}_k),
\end{align}
for $1 \le k \le n$.
However, now the dithering vectors $\Tau_k,\bar{\Tau}_k$ are independent and uniformly distributed in $[-\tilde\lambda_k,\tilde\lambda_k]^p$, where $\tilde\lambda_k = c \log(k) \cdot \lambda_k$, for $c > 0$ being an absolute constant (to be specified later) and $\lambda_k$ being defined by 
\begin{align} \label{eq:lambdaAdaptive}
    \lambda_{k}
    = \frac{1}{k} \sum_{j=0}^{k-1} \| \X_j \|_\infty
    = \frac{1}{k} \left( (k-1) \lambda_{k-1} + \| \X_{k-1} \|_\infty \right), \qquad 1\leq k\leq n.
\end{align}
%
%recursively via $\lambda_1 = 1$ and
%\begin{align} \label{eq:lambdaAdaptive}
%    \lambda_{k+1}
%    = \frac{1}{k} \sum_{j=1}^{k} \| \X^j \|_\infty
%    = \frac{1}{k} \left( (k-1) \lambda_{k} + \| \X^{k} \|_\infty \right),
%\end{align}
%for $k\ge 2$. 
Note that each $\lambda_k$ is an estimator of the maximal variance $\| \SIGMA \|_\infty$ of $\X$ based on the first $k$ samples. In particular, the dithers $\Tau_k,\bar{\Tau}_k$ used to quantize the $k$-th sample $\X_k$ only depend on the previous samples via $\tilde\lambda_k$ but are independent of $\X_k$ and all future samples. Since $\lambda_{k}$ can be computed in real time from the previous iterate $\lambda_{k-1}$ and the sample $\X_{k-1}$ before quantization, cf.\ \eqref{eq:lambdaAdaptive}, the dithering parameters can be efficiently updated in hardware implementation. 

Based on the samples in \eqref{eq:TwoBitSamplesAdaptive}, we now define a covariance estimator by
\begin{align} \label{eq:TwoBitEstimatorAdaptive}
\SIGMAADAP_n = \frac{1}{2}\SIGMA'_n + \frac{1}{2}(\SIGMA'_n)^T,
\end{align}
where
\begin{align} \label{eq:AsymmetricEstimatorAdaptive}
\SIGMA'_n = \sum_{k=1}^n \frac{\tilde\lambda_k^2}{n} \sign(\X_k + \Tau_k)\sign(\X_k + \bar{\Tau}_k)^T.
\end{align}
%Since \eqref{eq:TwoBitEstimator} and \eqref{eq:TwoBitEstimatorAdaptive} are conceptually close, by abuse of notation we will not distinguish them. It will be clear from the context to which estimator we refer.

\begin{remark}
    In addition to storing the two-bit samples in \eqref{eq:TwoBitSamplesAdaptive} one also needs to keep track of the sequence $\tilde\lambda_k$ which has to be stored with higher precision, e.g., in standard $32$-bit representation. Hence, the proposed quantization scheme requires $(2p + 32)$ bits per sample vector. Compared with using high precision samples which would require $32$ bits per entry and thus $32p$ bits per vector, this still results in a massive reduction of storage and computing time if the ambient dimension $p$ is of moderate size.
\end{remark}

Our first result bounds the estimation error of $\SIGMAADAP_n$ in the max and Frobenius norms, denoted by $\| \cdot \|_\infty$ and $\| \cdot \|_F$.

\begin{theorem}
\label{thm:FrobeniusDitheredMask}
For any $K>0$, there exist constants $C_1,C_2>0$ depending only on $K$ such that the following holds. Let $\X \in \mathbb R^p$ be a mean-zero, $K$-subgaussian vector with covariance matrix \sjoerd{$\SIGMA \in \R^{p\times p}$}.
Let $\X_0,...,\X_n \overset{\mathrm{i.i.d.}}{\sim} \X$ and let $\M \in [0,1]^{p\times p}$ be a fixed symmetric mask. If the adaptive dithering parameters satisfy $\tilde\lambda_k^2 = C_1 \log(k) \lambda_k^2$, where $\lambda_k$ is defined in \eqref{eq:lambdaAdaptive}, then for any $\theta \ge C_2\max\{\log(p),\log(n)\}$ we have with probability at least $1 - 2e^{-\theta}$   
    \begin{align*}
        \pnorm{\M \odot \SIGMAADAP_n - \M \odot \SIGMA}{\infty}
        \lesssim_{K} \| \M \|_\infty \| \SIGMA \|_\infty\log(p) \log(n)\left(\frac{\theta^{3/2}}{\sqrt{n}} + \frac{\theta^2}{n}\right)
    \end{align*}
    and 
    \begin{align*}
        \pnorm{\M \odot \SIGMAADAP_n - \M \odot \SIGMA}{F}
        \lesssim_{K} \| \M \|_F \| \SIGMA \|_\infty\log(p) \log(n)\left(\frac{\theta^{3/2}}{\sqrt{n}} + \frac{\theta^2}{n}\right).
    \end{align*}
\end{theorem}

%The proof of Theorem \ref{thm:FrobeniusDitheredMask} is presented in Section \ref{sec:ProofOfFrobeniusDitheredMask}. It consists of two main ingredients. First, we bound the bias of $\SIGMADITH_n$ in \eqref{eq:TwoBitEstimatorAdaptive} by controlling the adaptive dithering ranges $\lambda_k$ and use then Azuma's inequality to estimate the deviation of $\SIGMADITH_n$ around its conditional mean.

Our second result, which is the main result of this work, estimates the operator norm error.

\begin{theorem} \label{thm:OperatorDitheredMask}
For any $K > 0$, there exist constants $C_1,C_2>0$ depending only on $K$ such that the following holds. Let $\X \in \R^p$ be a mean-zero, $K$-subgaussian vector with covariance matrix \sjoerd{$\SIGMA \in \R^{p\times p}$}.
Let $\X_0,...,\X_n \overset{\mathrm{i.i.d.}}{\sim} \X$ and let $\M \in [0,1]^{p\times p}$ be a fixed symmetric mask. If $\tilde\lambda_k^2 = C_1 \log(k) \lambda_k^2$, where $\lambda_k$ is defined in \eqref{eq:lambdaAdaptive}, then for any $\theta \ge C_2\max\{\log(p),\log(n)\}$ we have with probability at least $1 - 2e^{-\theta}$   
	$$\pnorm{\M \odot \SIGMAADAP_n - \M \odot \SIGMA}{}\lesssim_{K} 
	\log(p)\log(n)\left(\theta^{5/2}\frac{\|\M\|_{1\to 2}}{\sqrt{n}}\|\SIGMA\|_{\infty}^{1/2}\|\SIGMA\|^{1/2}+\theta^3\frac{\|\M\|}{n} \|\SIGMA\|_{\infty}\right).
	$$
\end{theorem}
Remarkably, up to small losses in terms of logarithmic factors and a slightly worse (but still exponentially decaying) failure probability, the error bounds in Theorems~\ref{thm:FrobeniusDitheredMask} and \ref{thm:OperatorDitheredMask} match the ones that were previously obtained for the estimator $\SIGMADITH_n$ -- which required oracle knowledge of the maximal variance $\|\SIGMA\|_{\infty}$ of the distribution, c.f.\ \cite[Theorem 1]{yang2023plug} and Theorem~\ref{thm:OperatorDitheredMask_Old}, respectively.

\begin{remark}
    We emphasize that in both theorems it is sufficient to pick $\tilde\lambda_k^2 \geq C_1 \log(k) \lambda_k^2$, i.e., in practice the scaling $C_1$ does not need to be precisely known. Indeed, setting $\tilde\lambda_k^2 = c \cdot C_1 \log(k) \lambda_k^2$ for $c \ge 1$ only results in an additional scaling of the error estimates by $c$. The constant $C_1$ in Theorems \ref{thm:FrobeniusDitheredMask} and \ref{thm:OperatorDitheredMask} defines the smallest feasible choice of $\tilde\lambda_k$.
\end{remark}

Let us now outline the proofs of Theorems~\ref{thm:FrobeniusDitheredMask} and \ref{thm:OperatorDitheredMask} which are detailed in Section \ref{sec:Proofs}. 
%In both cases, the argument consists of two parts: controlling the bias of the estimator and proving concentration of the estimator around its expectation. Concretely, l
Let $\tnorm{\cdot}$ be either the max, Frobenius, or operator norm. We consider the filtration defined by
\begin{equation}
\label{eqn:filtrationDef}
\mathcal{F}_k = \sigma(\X_0,\X_1,\ldots,\X_k,\Tau_1,\ldots,\Tau_k,\bar{\Tau}_1, \ldots,\bar{\Tau}_k), \qquad k=0,1,\ldots,n,
\end{equation}
and let $\E_k(\cdot)=\E(\cdot|\mathcal{F}_k)$ be the associated conditional expectations. The first step is to use the triangle inequality to estimate %\eqref{eq:TwoBitEstimatorAdaptive} and \eqref{eq:AsymmetricEstimatorAdaptive}. Clearly,
	$$\tnorm{\M \odot \SIGMAADAP_n - \M \odot \SIGMA} \leq \tnorm{\M \odot \SIGMA'_n - \M \odot \SIGMA}$$
	and make the split
	\begin{align}
	\label{eqn:splitExpDithered}
	& \tnorm{\M \odot \SIGMA'_n - \M \odot \SIGMA}\nonumber\\
	& \qquad \le \tnorm{ \M \odot \Big( \SIGMA'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) } + \tnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)},
	\end{align}
where we abbreviate $\Y_k = \sign(\X_k + \Tau_k)$ and $\bar{\Y}_k = \sign(\X_k + \bar{\Tau}_k)$. We can think of the second term on the right-hand side as a `bias term'. The argument to control it is nearly identical for each of the three norms. We will develop this part first in Section~\ref{sec:biasControl}, resulting in Lemma~\ref{lem:biasControlUnified}. The first term on the right-hand side in \eqref{eqn:splitExpDithered} takes the form of (the norm of) a sum of martingale differences. To control this term, we will make use of suitable Burkholder-Rosenthal inequalities. In the Frobenius and max norm cases, it will turn out to be sufficient to use the classical Burkholder-Rosenthal inequalities for real-valued martingale differences. To prove the operator norm bound in Theorem~\ref{thm:OperatorDitheredMask}, however, we will need a new version of the Burkholder-Rosenthal inequalities for matrix martingale differences. We expect this result to be of independent interest.\par 
To formulate this result, recall that a sequence of random matrices $({\XI}_k)_{k=1}^n$ is called a matrix martingale difference sequence with respect to a filtration $(\mathcal{F}_k)_{k=0}^n$ if $\XI_k$ is $\mathcal{F}_k$-measurable, $\E\|\XI_k\| < \infty$, and $\E_{k-1} \XI_k = \0$ for all $1\leq k\leq n$. Here and below, we write $\E_{k-1}(\cdot):=\E(\cdot|\mathcal{F}_{k-1})$. 
\begin{theorem}
	\label{thm:matrixBRintro} 
	Let $\max\{2,\log n\}\leq q<\infty$ and $\log(p)\geq 2$. There exist $\alpha_{q,p},\beta_{q,p}>0$ depending only on $q$ and $p$ and satisfying $\alpha_{q,p}\lesssim \max\{\sqrt{q},\sqrt{\log(p)}\}$ and $\beta_{q,p}\lesssim q$ if $q\geq \log p$ such that for any matrix martingale difference sequence $({\XI}_k)_{k=1}^n$ in $\R^{p\times p}$ 
	%the following holds. 
% \noteJ{Do you mean here (?): There exist ... such that the following holds for any $q \ge \log(p)$. Or that $\beta$ is bounded by $p$ if $q$ is sufficiently large?}
% If $({\XI}_k)_{k=1}^n$ is a matrix martingale difference sequence in $\R^{p\times p}$, i.e., $({\XI}_k)_{k=1}^n$ is a sequence of random matrices with $\E\|\XI_k\| < \infty$ and 
% $\E_{k-1} \XI_k = \0$, then
	\begin{align}
 \label{eqn:matrixBRintroUpper}
	\Big(\E \Big\|\sum_{k=1}^n {\XI}_k\Big\|^q \Big)^{1/q} & \leq \beta_{q,p}\alpha_{q,p} \max\Big\{\Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k)\Big)^{1/2}\Big\|^q\Big)^{1/q}, \Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T)\Big)^{1/2}\Big\|^q\Big)^{1/q},\nonumber\\
	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ \alpha_{q,p} \max_{1\leq k\leq n} (\E\|{\XI}_k\|^q)^{1/q}\Big\}.
	\end{align}
	Conversely, for any $2\leq q<\infty$, there is a $\gamma_{q,p}>0$ depending only on $q$ and $p$ satisfying $\gamma_{q,p}\lesssim q$ if $q\geq \log(p)$ such that for any matrix martingale difference sequence $({\XI}_k)_{k=1}^n$ in $\R^{p\times p}$ 
		\begin{align}
   \label{eqn:matrixBRintroLower}
	\gamma_{q,p}\Big(\E \Big\|\sum_{k=1}^n {\XI}_k\Big\|^q \Big)^{1/q} & \gtrsim  \max\Big\{\Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k)\Big)^{1/2}\Big\|^q\Big)^{1/q}, \Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T)\Big)^{1/2}\Big\|^q\Big)^{1/q},\nonumber\\
	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ \max_{1\leq k\leq n} (\E\|{\XI}_k\|^q)^{1/q}\Big\}.
	\end{align}
\end{theorem}

The proof of Theorem \ref{thm:matrixBRintro} is presented in Section \ref{sec:Burkholder}.

%\begin{itemize}
%\item Idea of the proof: split into two parts.
%\item Bias term: same for all cases
%\item MD term: can be controlled using the classical Burkholder-Rosenthal inequality
%\item In the operator norm case:L 
%\end{itemize}
%To prove Theorem~\ref{thm:OperatorDitheredMask}, we need to prove concentration for the operator norm of a sum of matrix-valued martingale differences.


%The proof of Theorem \ref{thm:OperatorDitheredMask} is presented in Section \ref{sec:ProofOfOperatorDitheredMask}. Whereas the bias control works analogously to the Frobenius case, the deviation control requires a matrix version of the Burkholder-Rosenthal inequality which we derive in Section \ref{sec:Burkholder}.


\subsection{Related work}
\label{sec:RelatedWork}

Engineers have been examining the influence of coarse quantization on correlation and covariance estimation for decades, e.g., in the context of signal processing \cite{jacovitti1994estimation,roth2015covariance}, direction of arrival (DOA) estimation \cite{bar2002doa}, and massive multiple-input-multiple-output (MIMO) \cite{choi2016near,li2017channel}. While many related works restrict themselves to the undithered one-bit quantizer $Q(\X) = \sign(\X)$ and use the so-called arcsine law \cite{van1966spectrum} (also known as Grothendieck's identity in the mathematical literature \cite{vershynin2018high}) for estimating the correlation matrix, some recent articles examine the use of quantization thresholds in order to estimate the full covariance matrix.

One line of work focuses on the covariance estimation of stationary \cite{eamaz2021modified,eamaz2023covariance} and non-stationary \cite{eamaz2022covariance} Gaussian processes. In order to estimate the full covariance matrix, the authors propose to add biased Gaussian dithering vectors before quantizing. Due to the Gaussian nature of the dithering thresholds, the authors can derive a modified arcsine law which relates the covariance matrix of the quantized samples $\E(\sign(\X+\Tau)\sign(\X+\Tau)^T)$ to an involved integral expression depending on the entries of $\E(\X\X^T)$. To reconstruct $\E(\X\X^T)$ from $\E(\sign(\X+\Tau)\sign(\X+\Tau)^T)$, different numerical methods are proposed. Note, however, that the modified arcsine law is tailored to Gaussian samples and that none of the three works analyzes the estimation error in terms of the number of samples. In fact, the modified arcsine law only applies to the sample covariance matrix in the infinite sample limit $n \to \infty$.

Following the observation that variances can be estimated from one-bit samples with fixed, deterministic quantization thresholds \cite{chapeau2008fisher,fang2010adaptive}, another line of work investigates the use of such fixed thresholds \cite{liu2021one} to estimate covariance matrices from one-bit samples. Assuming an underlying stationary Gaussian process, the authors explicitly calculate the mean and autocorrelation of one-bit samples with fixed dither. Based on these representations they design an algorithm for estimating the covariance matrix entries of the underlying process and empirically evaluate its performance. Again, the theoretical results are tailored to Gaussian samples and do not analyze the approximation error in terms of the number of samples. Moreover, the provided theoretical results are restricted to single entries of the covariance matrix. Since this does not take into account the matrix structure of $\E(\X\X^T)$, the developed tools will not lend themselves to a tight analysis of the approximation error in matrix Schatten norms like the operator norm.

The authors of \cite{xiao2023one} follow up on \cite{liu2021one} and use low-order Taylor expansions to analyze the variance of maximum-likelihood estimators of the quantities computed in \cite{liu2021one}. They conclude that a fixed dither yields suboptimal results if the variances of the process are strongly fluctuating. They then propose to use a dither that is monotonically increasing in time and evaluate its performance in numerical simulations. As with the previous works, the applied techniques are tailored to Gaussian sample distributions and the analysis is restricted to the infinite sample limit $n \to \infty$.

In contrast, the results in \cite{dirksen2021covariance}, which we extend in this manuscript, provided the first non-asymptotic (and near-minimax optimal) guarantees in the literature. The use of uniform random dithering vectors (instead of fixed, deterministically varying, or random Gaussian ones) considerably simplifies the analysis and allows to generalize the results to non-Gaussian samples. In \cite{yang2023plug}, the non-asymptotic results of \cite{dirksen2021covariance} have been generalized to the complex domain and applied to massive MIMO. A recent work \cite{chen2022high} modified the strategy of \cite{dirksen2021covariance} to cover heavy-tailed distributions (by using truncation before quantizing). 

\subsection{Notation}
\label{sec:Notation}

Let us clarify the notation that will be used throughout this work before continuing. We write $[n] = \{ 1,...,n \}$ for $n\in \mathbb{N}$. \sjoerd{We use the notation $a \lesssim_{\alpha} b$ (resp.\ $\gtrsim_{\alpha}$) to abbreviate $a \le C_{\alpha}b$ (resp.\ $\ge$), for a constant $C_{\alpha} > 0$ depending only on $\alpha$. Similarly, we write $a \lesssim b$ if $a \le Cb$ for an absolute constant $C>0$.} \sjoerdgreen{We write $a\simeq b$ if both $a\lesssim b$ and $b\lesssim a$ hold (with possibly different implicit constants)}. The values of absolute constants $c,C > 0$ may vary from line to line. We let scalar-valued functions act entry-wise on vectors and matrices. In particular,
\begin{align*}{}
[\sign(\x)]_i = \begin{cases}
1 & \text{if } x_i\geq 0 \\
-1& \text{if } x_i<0,
\end{cases}
\end{align*}
for all $\x\in \R^p$ and $i\in [p]$. A mean-zero random vector $\X$  \sjoerd{in $\R^p$} is called $K$-subgaussian if $$\|\langle \X,\x\rangle\|_{\psi_2} \leq K \johannes{(\mathbb{E}\langle \X,\x\rangle^2)^{1/2}} \quad \mbox{ for all }  \x \in \sjoerd{\R^p},$$
where the subgaussian ($\psi_2$-)norm of a random variable $X$ is defined by 
\begin{align*}
    \pnorm{X}{\psi_2} = \inf \curly{ t>0 \colon \johannes{\E\round{ \exp\round{\frac{X^2}{t^2}} }} \le 2 }.
\end{align*}
For $1\leq q<\infty$, the $L^q$-norm of a random variable $X$ is denoted by 
\begin{align*}
    \| X \|_{L^q} = ( \E X^q )^{\frac{1}{q}}.
\end{align*}
For $\Z \in \R^{p\times p}$, we denote the operator norm by $\pnorm{\Z}{}~=~\sup_{\u \in \mathbb{S}^{p-1}} \pnorm{\Z\u}{2}$, the entry-wise max-norm by $\pnorm{\Z}{\infty} = \max_{i,j} \abs{Z_{i,j}}{}$, the Frobenius norm by $\| \Z \|_F~=~(\sum_{i,j} |Z_{i,j}|^2)^{1/2}$, and the maximum column norm \sjoerd{by} $\pnorm{\Z}{1\rightarrow 2} = \max_{j \in [p]} \pnorm{\z_j}{2}$, where $\z_j$ denotes the $j$-th column of $\Z$. The Hadamard (i.e., entry-wise) product of two matrices is denoted by $\odot$. We denote the \sjoerd{all ones}-matrix by $\boldsymbol{1} \in \R^{p\times p}$.
%and define
%\begin{align*}
%\Z^{\odot \ell} = \underbrace{\Z \odot \cdots \odot \Z}_{\ell \text{-times}}.
%\end{align*} 
The $\diag$-operator, when applied to \sjoerd{a} matrix, extracts the diagonal as \sjoerd{a} vector; when applied to a vector, it outputs the corresponding diagonal matrix. 
%The only exception is $|\Z| := (\Z^T \Z)^\frac{1}{2}$, for a matrix $\Z \in \R^{p_1 \times p_2}$.
For symmetric \johannes{$\W,\Z\in \R^{p\times p}$ we write $\W \preceq \Z$ if $\Z-\W$} is positive semidefinite. We will use that for any conditional expectation 
$\E_{\mathcal{G}}=\E(\cdot|\mathcal{G})$,
	\begin{align} \label{eq:Kadison}
	\johannes{\mathbb{E}_{\mathcal{G}}\Z^T\mathbb{E}_{\mathcal{G}}\Z \preceq \mathbb{E}_{\mathcal{G}}(\Z^T\Z) \quad \text{for any } \Z\in \R^{p_1\times p_2}}{},
	\end{align}
	which is immediate from 
	$$\0 \preceq \mathbb{E}_{\mathcal{G}}[(\Z-\mathbb{E}_{\mathcal{G}}\Z)^T(\Z-\mathbb{E}_{\mathcal{G}}\Z)].$$ 
%and refer to it as Kadison's inequality (as it is a special case of Kadison's inequality in noncommutative probability theory).}

\begin{comment}
List of papers from engineering literature on quantized covariance estimation:
\begin{enumerate}
    \item Papers on the modification of the arcsine law for one-bit samples with biased Gaussian dither are \cite{eamaz2021modified,eamaz2023covariance} for stationary and \cite{eamaz2022covariance} for non-stationary Gaussian processes. All works empirically evaluate numerical approaches for solving the involved integral expressions of the generalized arcsine law. 
    \item Under assuming a stationary Gaussian process, \cite{liu2021one} explicitly calculates the mean and autocorrelation of one-bit samples with fixed dither. Based on these representations an algorithm for estimating the autocorrelation of the underlying process is proposed and empirically evaluated.
    \item By low-order Taylor expansions, the authors of \cite{xiao2023one} analyze the mean squared error of the estimates in \cite{liu2021one} in the many sample limit. They conclude that a fixed dither yields suboptimal results if the variances of the process are strongly fluctuating, and hence propose to use a dither that is monotonically increasing in time.
    \item In \cite{huang2019one}, the authors argue that for small SNRs one can ignore the influence of the one-bit quantization in subspace estimation tasks since the arcsine-function, which appears in the arcsine law, behaves like identity around zero. 
\end{enumerate}
\end{comment}
%It is important to note that all of the just mentioned works on engineering side base their analysis upon the asymptotic error decay of \eqref{eq:SampleMean}, for $n \to \infty$. 

\section{Proofs}
\label{sec:Proofs}

As outlined above, we first analyze the bias term of \eqref{eqn:splitExpDithered} in Section \ref{sec:biasControl}. This will result in Lemma \ref{lem:biasControlUnified} below. Sections \ref{sec:ProofOfFrobeniusDitheredMask} and \ref{sec:ProofOfOperatorDitheredMask} then present the proofs of Theorems \ref{thm:FrobeniusDitheredMask} and \ref{thm:OperatorDitheredMask}. Finally, Section \ref{sec:Burkholder} provides the proof of Theorem \ref{thm:matrixBRintro}. 

\subsection{Control of the bias}
\label{sec:biasControl}

Let us begin with some basic observations. First note that since $\lambda_k$ in \eqref{eq:lambdaAdaptive} --- and with it $\tilde\lambda_k = c \log(k) \lambda_k$ --- is independent of $\X_k$, we can apply the following result from \cite{dirksen2021covariance} conditionally to estimate the bias term in \eqref{eqn:splitExpDithered}.

\begin{lemma}[{\cite[Lemma 17]{dirksen2021covariance}}]
	\label{lem:linftyBiasEst}
	For any $K > 0$, there exist constants $c_1,c_2>0$ depending only on $K$ such that the following holds. Let $\X \in \R^p$ be a mean-zero, $K$-subgaussian vector with covariance matrix $\johannes{\E\round{ \X\X^T }}{} = \SIGMA \in \R^{p\times p}$. Let \sjoerd{$\lambda>0$ and let} $\Y=\sign(\X + \Tau)$ and $\bar{\Y}=\sign(\X + \bar{\Tau})$, where $\Tau,\bar{\Tau}$ are independent and uniformly distributed in $[-\lambda,\lambda]^p$ \sjoerd{and independent of $\X$}. Then,
	$$\pnorm{\johannes{\E\round{\lambda^2\Y\bar{\Y}^T}}{} - \SIGMA}{\infty}\sjoerd{\leq c_1} (\lambda^2+\|\SIGMA\|_{\infty})e^{-\sjoerd{c_2}\lambda^2/\|\SIGMA\|_{\infty}}$$
	and 
	$$\pnorm{\johannes{\E\round{\lambda^2\Y\Y^T}}{} - (\SIGMA-\diag(\SIGMA)+\lambda^2 \id)}{\infty}\sjoerd{\leq c_1} (\lambda^2+\|\SIGMA\|_{\infty})e^{-\sjoerd{c_2}\lambda^2/\|\SIGMA\|_{\infty}}.$$
\end{lemma}

To make use of the bias estimate in Lemma \ref{lem:linftyBiasEst}, however, we have to control the adaptive dithering ranges~$\tilde\lambda_k$. This is achieved in the following lemma.

\begin{lemma}
\label{lem:linfConcentration}
    There exist absolute constants $c,C>0$ and, for any given $K > 0$, an $L\gtrsim K^{-3}$ such that the following holds. Let $\X \in \R^p$ be a $K$-subgaussian vector with $\E \X = \0$ and $\E(\X\X^\top) = \SIGMA \in \R^{p\times p}$. Let $\X_0,\dots,\X_n \overset{i.i.d.}{\sim} \X$ and let $\lambda_k$ be defined as in \eqref{eq:lambdaAdaptive}. Then, for $t > 0$,
    \begin{align}
    \label{eqn:lambdakSubgaussian}
        \P{\lambda_{k} > (t+1) CK \sqrt{\| \SIGMA \|_\infty \log(2p)}} \le 2 e^{-ckt^2}
    \end{align}
    and 
%    Let $\theta > 0$. If $\X$ satisfies in addition the $L_1$-$L_2$-equivalence $\E |\langle \X,\z \rangle| \ge L (\E |\langle \X,\z \rangle|^2 )^{\frac{1}{2}}$, for some $L >0$ and any $\z \in \R^p$, and if $k \ge \theta \frac{K^2}{cL^2} \log(p)$, then, for $t > 0$,
    \begin{align}
    \label{eqn:lambdakHPLowerBd}          \P{\lambda_{k} < (1-t) L \sqrt{\| \SIGMA \|_\infty} } \le 2 e^{ - \frac{ckL^2t^2}{K^2\log(p) } }.
    \end{align}
%    $k \ge \theta \frac{K^2}{cL^2} \log(p)
%    Moreover, if $k \ge \theta \frac{K^2}{cL^2} \log(p)$, for any $\theta > 0$, then for all $t > 0$
%%    Let $\theta > 0$. If $\X$ satisfies in addition the $L_1$-$L_2$-equivalence $\E |\langle \X,\z \rangle| \ge L (\E |\langle \X,\z \rangle|^2 )^{\frac{1}{2}}$, for some $L >0$ and any $\z \in \R^p$, and if $k \ge \theta \frac{K^2}{cL^2} \log(p)$, then, for $t > 0$,
%    \begin{align*}
%        \P{\lambda_{k+1} < (1-t) L \sqrt{\| \SIGMA \|_\infty} } \le 2e^{-\theta t^2}.
%    \end{align*}
\end{lemma}

In order to prove Lemma \ref{lem:linfConcentration}, we control the $\ell_\infty$-norm of a subgaussian vector as follows.

\begin{lemma}
\label{lem:linfExpectation}
There exists an absolute constant $C>0$ such that the following holds. Let $\X \in \R^p$ be a $K$-subgaussian vector with $\E \X = \0$ and $\E(\X\X^\top) = \SIGMA \in \R^{p\times p}$. Then $\| \X \|_\infty$ is $(CK \sqrt{ \| \SIGMA \|_\infty \log(p)})$-subgaussian and there exists an $L \gtrsim K^{-3}$ such that
    \begin{align*}
      L \sqrt{ \| \SIGMA \|_\infty} \le  \E \| \X \|_\infty \lesssim K \sqrt{\| \SIGMA \|_\infty \log(2p)}.
    \end{align*}
%    If $\X$ satisfies in addition the $L_1$-$L_2$-equivalence $\E |\langle \X,\z \rangle| \ge L (\E |\langle \X,\z \rangle|^2 )^{\frac{1}{2}}$, for some $L >0$ and any $\z \in \R^p$, then
%    \begin{align*}
%        \E \| \X \|_\infty \ge L \sqrt{ \| \SIGMA \|_\infty}.
%    \end{align*}
\end{lemma}

\begin{proof}
    If $\X$ is $K$-subgaussian, all entries $X_i$ of $\X$ satisfy by definition 
    \begin{align}
    \label{eq:psi2-normComponent}
        \| X_i \|_{\psi_2} = \| \langle \X, \e_i \rangle \|_{\psi_2} \le K ( \E \langle \X,\e_i \rangle^2 )^\frac{1}{2} \le K \sqrt{\| \SIGMA \|_\infty}.
    \end{align}
    Thus we get for any $t > 0$ that
    \begin{align*}
        \P{\| \X \|_\infty - C'K\sqrt{\| \SIGMA \|_\infty \log(p)} > t}
        &= \P{ \exists i \in [p] \colon |X_i| > t + C' K\sqrt{\| \SIGMA \|_\infty \log(p)}}
        \\
        &\le \sum_{i=1}^p \P{|X_i| > t + C'K\sqrt{\| \SIGMA \|_\infty \log(p)}} \\
        &\le 2p e^{-\frac{c(t + C' K\sqrt{\| \SIGMA \|_\infty \log(p)})^2}{K^2 \| \SIGMA \|_\infty}}
        \le 2e^{-\frac{ct^2}{K^2 \| \SIGMA \|_\infty}} e^{-\frac{2 t KcC'\sqrt{\| \SIGMA \|_\infty \log(p)}}{K^2 \| \SIGMA \|_\infty}}
        \\
        &\le 2e^{-c\frac{t^2}{K^2 \| \SIGMA \|_\infty}} \\
        &\le 2e^{-c\frac{t^2}{K^2 \| \SIGMA \|_\infty \log(p)}},
    \end{align*}
   provided that $C'$ is a large enough absolute constant. Together with 
   $$\| \X \|_\infty - K\sqrt{\| \SIGMA \|_\infty \log(p)} \ge - K\sqrt{\| \SIGMA \|_\infty \log(p)}$$ 
   this yields 
    \begin{align*}
        \| (\| \X \|_\infty - K\sqrt{\| \SIGMA \|_\infty \log(p)}) \|_{\psi_2} \lesssim K\sqrt{\| \SIGMA \|_\infty \log(p)},
    \end{align*}
    see e.g.\ \cite[Section 2.5]{vershynin2018high}, and thus the first claim via
    \begin{align*}
        \| \| \X \|_\infty \|_{\psi_2} &\le \| (\| \X \|_\infty - K\sqrt{\| \SIGMA \|_\infty \log(p)}) \|_{\psi_2} + \| K\sqrt{\| \SIGMA \|_\infty \log(p)} \|_{\psi_2} \\
        &\lesssim K\sqrt{\| \SIGMA \|_\infty \log(p)}.
    \end{align*}
    
    Since $\E\X = \0$ by assumption, \eqref{eq:psi2-normComponent} further implies $\E \exp(\theta X_i) \le \exp(CK^2 \| \SIGMA \|_\infty \theta^2)$, for some absolute constant $C > 0$ and any $\theta \in \R$, see e.g.\ \cite[Section 2.5]{vershynin2018high}, so that \cite[Proposition 7.29]{foucart2013compressed} yields the upper bound on $\E \|\X\|_\infty$ via
    \begin{align*}
        \E \| \X \|_\infty 
        = \E \left( \max_{i \in [p]} |X_i| \right)
        \lesssim K \sqrt{\| \SIGMA \|_\infty \log(2p)}.
    \end{align*}
    To see the lower bound, let $i_\star \in [p]$ denote an index such that $X_{i_\star}$ has maximal variance, i.e., $\Sigma_{i_\star, i_\star} = \| \SIGMA \|_\infty$. Then, 
    $$\|X_{i_\star}\|_{L^2} = \|\langle \X, \e_{i_\star} \rangle\|_{L^2} =  \sqrt{\| \SIGMA \|_\infty}.$$
    On the other hand, by H\"older's inequality and \eqref{eq:psi2-normComponent},
    $$\|X_{i_\star}\|_{L^2}\leq \|X_{i_\star}\|_{L^1}^{1/4}\|X_{i_\star}\|_{L^3}^{3/4}\lesssim \|X_{i_\star}\|_{L^1}^{1/4} (K \sqrt{\| \SIGMA \|_\infty})^{3/4}$$
    so that
    $$\|X_{i_\star}\|_{L^1} \gtrsim K^{-3}\sqrt{\| \SIGMA \|_\infty}$$
    and hence
    $$\E \| \X \|_\infty \geq \|X_{i_\star}\|_{L^1} \gtrsim K^{-3}\sqrt{\| \SIGMA \|_\infty}.$$
    
%    
%        \begin{align*}
%        \E \| \X \|_\infty 
%        \ge \E |X_{i_\star}|
%        = \E |\langle \X, \e_{i_\star} \rangle|.
%    \end{align*}
%    By using H\"older's inequality and the fact that $X_{i_\star}$ is subgaussian, we find
%    $$\|\langle \X, \e_{i_\star} \rangle\|_{L^2}\leq \|\langle \X, \e_{i_\star} \rangle\|_{L^1}^{1/4}\|\langle \X, \e_{i_\star} \rangle\|_{L^2}$$
%    \begin{align*}
%        \E \| \X \|_\infty 
%        \ge \E |X_{i_\star}|
%        = \E |\langle \X, \e_{i_\star} \rangle| 
%        \ge L (\E |\langle \X,\e_{i_\star} \rangle|^2 )^{\frac{1}{2}}
%        = L \sqrt{\| \SIGMA \|_\infty},
%    \end{align*}
%    where we used the $L_1$-$L_2$-equivalence in the last inequality.
\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:linfConcentration}]
    Recall that 
    \begin{align*}
        \lambda_{k}
        = \frac{1}{k} \sum_{j=0}^{k-1} \| \X_j \|_\infty.
    \end{align*}
    Since $\| \X \|_\infty$ is $(CK \sqrt{ \| \SIGMA \|_\infty \log(p)})$-subgaussian by Lemma \ref{lem:linfExpectation}, Hoeffding's inequality \cite[Theorem 2.6.2]{vershynin2018high} yields
    \begin{align*}
        \P{|\lambda_{k} - \E \| \X \|_\infty | > u}
        = \P{ \Big| \frac{1}{k} \sum_{j = 0}^{k-1} (\| \X_j \|_\infty - \E \| \X \|_\infty) \Big| > u }
        \le 2 e^{ - \frac{cku^2}{K^2 \| \SIGMA \|_\infty \log(p) } }.
    \end{align*}
    The first claim follows since $\E\|\X \|_\infty \lesssim K \sqrt{\| \SIGMA \|_\infty \log(2p)}$ by Lemma \ref{lem:linfExpectation}.

   For the second claim, we use that $\E\|\X\|_\infty \ge L \sqrt{\| \SIGMA \|_\infty}$ by Lemma \ref{lem:linfExpectation}. Combining this with the previous equation then leads to
 %   For the second claim, just use that if $L_1$-$L_2$-equivalence holds, $\E\|\X\|_\infty \ge L \sqrt{\| \SIGMA \|_\infty}$ by Lemma \ref{lem:linfExpectation}. Combining this with the previous equation then leads to
    \begin{align*}
        \P{ \lambda_{k} < (1-t) L \sqrt{\| \SIGMA \|_\infty} }
        \le \P{|\lambda_{k} - \E \| \X \|_\infty | > t L \sqrt{\| \SIGMA \|_\infty} }
        \le 2 e^{ - \frac{ckL^2t^2}{K^2\log(p) } }
        %,
    \end{align*}
 %   which yields the claim for $k \ge \theta \frac{K^2}{cL^2} \log(p)$.
\end{proof}

Relying on Lemmas \ref{lem:linftyBiasEst} and \ref{lem:linfConcentration}, we can now derive the required bias control.

\begin{lemma}
\label{lem:biasControlUnified}
There exists an absolute constant $c$ such that the following holds.
Let $\X \in \mathbb R^p$ be a mean-zero, $K$-subgaussian vector with covariance matrix \sjoerd{$\SIGMA \in \R^{p\times p}$}.
Let $\X_0,...,\X_n \overset{\mathrm{i.i.d.}}{\sim} \X$ and let $\M \in [0,1]^{p\times p}$ be a fixed symmetric mask. Set $\tilde\lambda_k = \sqrt{\frac{4}{L^2 c_2} \log(k^2)} \lambda_k$, where $\lambda_k$ is defined in \eqref{eq:lambdaAdaptive}, and $c_2$ and $L$ are the constants from Lemmas \ref{lem:linftyBiasEst} and \ref{lem:linfConcentration} depending only on $K$.
Let $\tnorm{\cdot}$ be either the maximum, Frobenius, or operator norm. Then, for any $\theta\geq \max\{c, 2\log(n)\}$, we have with probability at least $1-2e^{-\theta}$
$$\tnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}  \lesssim_{K} \theta \; \tnorm{\M} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n},$$
where we abbreviate the quantized samples in \eqref{eq:TwoBitSamplesAdaptive} by $\Y_k = \sign(\X_k + \Tau_k)$ and $\bar{\Y}_k =\sign(\X_k + \bar{\Tau}_k)$.
\end{lemma}

We extract the following technical observation from the proof of Lemma \ref{lem:biasControlUnified} since we are going to re-use it later.

\begin{lemma}
\label{lem:SumBounds}
There is an absolute constant $c>0$ such that the following holds. Let $\X \in \R^p$ be a $K$-subgaussian vector with $\E \X = \0$ and $\E(\X\X^\top) = \SIGMA \in \R^{p\times p}$. Let $\X_1,\dots,\X_n \overset{i.i.d.}{\sim} \X$ and let $\lambda_k$ be defined as in \eqref{eq:lambdaAdaptive}.
Set $\tilde\lambda_k = \sqrt{\frac{4}{L^2 c_2} \log(k^2)} \lambda_k$ for $k\geq 1$, where $c_2$  and $L$ are the constants from Lemmas \ref{lem:linftyBiasEst} and \ref{lem:linfConcentration} depending only on $K$. Then, for $\theta \ge c$ we have with probability at least $1-2e^{-\theta}$
\begin{equation}
\label{eqn:elemLambdaEstimate}
\tilde\lambda_k \lesssim \frac{K}{L} \sqrt{\theta\| \SIGMA \|_\infty \log(p)\log(k)}, \qquad \text{for all $1\leq k\le n$}.
\end{equation}
Moreover, for any $\theta\geq \max\{c, 2\log(n)\}$, we have with probability at least $1-2e^{-\theta}$ that
\begin{equation}
\label{eqn:sumLambdaEstimate1}
\sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}\lesssim \theta\frac{K^2}{L^2}\log(p)\log(n) \|\SIGMA\|_{\infty}
\end{equation}
and
\begin{equation}
\label{eqn:sumLambdaEstimate2}
\left(\sum_{k=1}^n \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}\right)^{1/2} \lesssim \theta\frac{K^2}{L^2}\log(p)\log(n) \|\SIGMA\|_{\infty}.
\end{equation}
\end{lemma}
\begin{proof}
Assuming that $\theta$ is sufficiently large to guarantee $e^{-c\theta} \le \frac{1}{2}$, for $c>0$ being the constant from Lemma~\ref{lem:linfConcentration}, one can combine \eqref{eqn:lambdakSubgaussian} in Lemma \ref{lem:linfConcentration} with a union bound over all $k\geq 1$, to obtain with probability at least $1-2e^{-c\theta}$ 
    \begin{align} \label{eq:LambdaUB}
        \lambda_k \lesssim K \sqrt{\theta\| \SIGMA \|_\infty \log(2p)},
    \end{align}
for all $k \ge 1$.
This proves \eqref{eqn:elemLambdaEstimate}.

Define now $k_\theta = \min\{\lceil \theta \frac{K^2}{c'L^2} \log(p) \rceil,n\}$, where $c'>0$ only depends on the constant $c$ from Lemma \ref{lem:linfConcentration}. If $k_\theta < n$, we find by \eqref{eqn:lambdakHPLowerBd} in Lemma \ref{lem:linfConcentration} and a union bound over all $k_{\theta} < k \leq n$ 
with probability at least 
\begin{align*}
    2 \sum_{k = k_\theta + 1}^n e^{ - \frac{ckL^2}{4 K^2\log(p) } } 
    \le 2n e^{-\theta} 
    \le 2 e^{-\frac{1}{2} \theta}
\end{align*}
that 
\begin{align}
\label{eq:LambdaKLB}
    \lambda_k \ge \frac{1}{2} L \sqrt{\| \SIGMA \|_\infty}, \qquad \text{for any $k_\theta < k \le n$}.
\end{align}
Note that we used in the union bound that $\theta - \log(n) \ge \frac{1}{2} \theta$ by assumption.
\begin{comment}
\sjoerdnewred{
Define now $k_\theta = \min\{\lceil \theta \frac{K^2}{cL^2} \log(p) \rceil,n\}$, where $c>0$ only depends on the constant from Lemma \ref{lem:linfConcentration}. By \eqref{eqn:lambdakHPLowerBd} in Lemma \ref{lem:linfConcentration} and a union bound over all $k_{\theta}\leq k\leq n$, we find with probability at least 
\begin{align*}
    1-2 \sum_{k=k_{\theta}}^{n} e^{ - \frac{ckL^2}{K^2\log(p)} }
    \geq 1 - 2e^{-c'\theta}
\end{align*}
that 
\begin{align}
\label{eq:LambdaKLB}
    \lambda_k \ge \frac{1}{2} L \sqrt{\| \SIGMA \|_\infty},
\end{align}
for all $k > k_\theta$.
}
\noteJ{Questions regarding the above:
\begin{itemize}
    \item I don't understand how you get the above probability. Wouldn't a union bound yield a failure probability of 
    \begin{align*}
        2(n-k_\theta) e^{-ct}
    \end{align*}
    which would require $t = \log(n) + \theta$ (or $\theta > \log(n)$)? \\
    \sjoerdnewred{I used the estimate that was in the proof of the Lemma, I forgot to change the statement of the Lemma. I hope that it makes more sense now.} \\
    I still don't see it... We only know that $\theta \ge \frac{cL^2}{K^2 \log(p)} k_\theta$, right? But how are you then getting a lower bound for $\frac{cL^2}{K^2 \log(p)}$ in terms of $\theta$? (which would be necessary to translate the geometric sum in $e^{-cL^2/K^2 \log(p)}$ into a geometric sum in $e^{-\theta}$)
\end{itemize}
}
\end{comment}
Upon these events we find
\begin{align*}
& \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
& \qquad = \sum_{\substack{1 \le k \le k_\theta: \\ \lambda_k^2\leq L^2\|\SIGMA\|_{\infty}}} (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}
+ \sum_{\substack{1 \le k \le k_\theta: \\ \lambda_k^2 > L^2\|\SIGMA\|_{\infty}}} (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
&\qquad\qquad + \sum_{k=k_{\theta}+1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}},
%\\
%& \qquad \lesssim k_{\theta} \frac{\log(k_{\theta})}{L^2} \|\SIGMA\|_{\infty} + \sum_{k=1}^{\infty} (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}\\
%& \qquad 
\end{align*}
where we recall that $\tilde\lambda_k = \sqrt{\frac{4}{L^2 c_2} \log(k^2)} \lambda_k$.
Observe that 
\begin{align*}
\sum_{\substack{1 \le k \le k_\theta: \\ \lambda_k^2\leq L^2\|\SIGMA\|_{\infty}}} (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} 
\lesssim k_{\theta}\log(k_{\theta}) \|\SIGMA\|_{\infty}
\leq \theta\frac{K^2}{L^2}\log(p)\log(n) \|\SIGMA\|_{\infty}
%& \qquad \lesssim k_{\theta} \frac{\log(k_{\theta})}{L^2} \|\SIGMA\|_{\infty}\leq \theta\frac{K^2}{L^4}\log(p)[\log(\theta)+\log(K/L)+\log\log(p)] \|\SIGMA\|_{\infty}
\end{align*}
and by \eqref{eqn:elemLambdaEstimate}
\begin{align*}
\sum_{\substack{1 \le k \le k_\theta: \\ \lambda_k^2 > L^2\|\SIGMA\|_{\infty}}} (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}
&\lesssim \sum_{k=1}^{k_{\theta}}(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-\log(k^2)} \lesssim \theta \frac{K^2}{L^2} \|\SIGMA\|_{\infty}\log(p) \sum_{k=1}^{k_{\theta}}\frac{\log(k)}{k^2} \\
&\lesssim  \theta \frac{K^2}{L^2} \|\SIGMA\|_{\infty}\log(p).
\end{align*}
Similarly, if $k_{\theta}<n$ then \eqref{eqn:elemLambdaEstimate} and \eqref{eq:LambdaKLB} yield
\begin{align*}
\sum_{k=k_{\theta}+1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} 
&\lesssim  \sum_{k=k_{\theta}+1}^n \left(\theta \frac{ K^2}{L^2} \|\SIGMA\|_{\infty}\log(p)\log(k) + \|\SIGMA\|_{\infty}\right)e^{-\log(k^2)} \\
&\lesssim \theta \frac{K^2}{L^2} \|\SIGMA\|_{\infty}\log(p).
\end{align*}
Collecting these estimates we obtain \eqref{eqn:sumLambdaEstimate1}. The estimate in \eqref{eqn:sumLambdaEstimate2} is proved similarly.  
\begin{comment}
%%%%%%%%% TO BE REMOVED - START %%%%%%%%%%%%%%%%%%%%%
\par
Similarly,
\begin{align*}
& \sum_{k=1}^n \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
& \qquad = \sum_{k=1}^{k_{\theta}} \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}[1_{\{\lambda_k^2\leq L^2\|\SIGMA\|_{\infty}\}} + 1_{\{\lambda_k^2>L^2 \|\SIGMA\|_{\infty}\}}] + \sum_{k=k_{\theta}+1}^n \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}
%\\
%& \qquad \lesssim k_{\theta} \frac{\log(k_{\theta})}{L^2} \|\SIGMA\|_{\infty} + \sum_{k=1}^{\infty} (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}\\
%& \qquad 
\end{align*}
Observe that 
\begin{align*}
& \sum_{k=1}^{k_{\theta}} \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}1_{\{\lambda_k^2\leq L^2\|\SIGMA\|_{\infty}\}} \\
& \qquad \lesssim k_{\theta}\log^2(k_{\theta}) \|\SIGMA\|_{\infty}^2\leq \theta\frac{K^2}{L^2}\log(p)\log^2(n) \|\SIGMA\|_{\infty}^2
%& \qquad \lesssim k_{\theta} \frac{\log(k_{\theta})}{L^2} \|\SIGMA\|_{\infty}\leq \theta\frac{K^2}{L^4}\log(p)[\log(\theta)+\log(K/L)+\log\log(p)] \|\SIGMA\|_{\infty}
\end{align*}
and 
\begin{align*}
& \sum_{k=1}^{k_{\theta}} \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}1_{\{\lambda_k^2>L^2\|\SIGMA\|_{\infty}\}}\\
& \qquad \lesssim \sum_{k=1}^{k_{\theta}}\tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-\log(k^2)} \lesssim \frac{\theta^2 K^4\|\SIGMA\|_{\infty}^2\log^2(p)}{L^4} \sum_{k=1}^{k_{\theta}}\frac{\log^2(k)}{k^2}\lesssim  \frac{\theta^2 K^4\|\SIGMA\|_{\infty}^2\log^2(p)}{L^4}.
\end{align*}
Similarly, if $k_{\theta}<n$ then 
\begin{align*}
& \sum_{k=k_{\theta}+1}^n \tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}\\
& \qquad \lesssim  \sum_{k=k_{\theta}+1}^n \frac{\theta K^2\|\SIGMA\|_{\infty}\log(p)\log(k)}{L^2}\left(\frac{\theta K^2\|\SIGMA\|_{\infty}\log(p)\log(k)}{L^2}+\|\SIGMA\|_{\infty}\right)e^{-\log(k^2)} \lesssim \frac{\theta^2 K^4\|\SIGMA\|_{\infty}^2\log^2(p)}{L^4}.
\end{align*}
%%%%%%%%% TO BE REMOVED - END %%%%%%%%%%%%%%%%%%%%%
\end{comment}
\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:biasControlUnified}]
Let us first observe that 
\begin{equation}
\label{eqn:takeMout}
\tnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}\leq \tnorm{\M} \pnorm{\sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}.
\end{equation}
This is clear for the maximum and Frobenius norms. For the remaining case $\tnorm{\cdot} = \| \cdot \|$, observe that $\M$ has only \sjoerd{nonnegative} entries and hence
	\begin{align}
	\label{eqn:normEstPos}
	\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{} \nonumber
	&=  \sup_{\v,\w \in \R^p \ : \ \|\v\|_2, \|\w\|_2 \leq 1} \abs{\sum_{i,j=1}^p M_{i,j} \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)_{i,j} v_i w_j }{} \nonumber\\
	&
	\le  \sup_{\v,\w \in \R^p \ : \ \|\v\|_2, \|\w\|_2 \leq 1} \sum_{i,j=1}^p M_{i,j} \abs{ \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)_{i,j} }{} \abs{v_i w_j}{}  \nonumber\\
	&\le \pnorm{\M}{} \pnorm{\sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}.
\end{align}
To complete the proof, we apply Lemma~\ref{lem:linftyBiasEst} conditionally and use Lemma~\ref{lem:SumBounds} to obtain with probability at least $1-2e^{-c\theta}$
\begin{align*}
\pnorm{\sum_{k=1}^n \E_{k-1}( \tilde\lambda_k^2 \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
	& \leq \sum_{k=1}^n \pnorm{ \E_{k-1}( \tilde\lambda_k^2 \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
	\nonumber \\
	&
	\sjoerd{\lesssim_K} \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
	& \lesssim_{K} \theta\log(p) \log(n)\| \SIGMA \|_\infty,
\end{align*}
where in the final step we applied \eqref{eqn:sumLambdaEstimate1} and used that $L$ in Lemma \ref{lem:SumBounds} depends solely on $K$. 
%
%    Recall the definitions of $\SIGMADITH_n$ and $\tilde{\SIGMA}'_n$ in \eqref{eq:TwoBitEstimatorAdaptive} and \eqref{eq:AsymmetricEstimatorAdaptive}. Clearly,
%	$$\pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{} \leq \pnorm{\M \odot \tilde{\SIGMA}'_n - \M \odot \SIGMA}{}$$
%	and
%	\begin{align}
%	\label{eqn:splitExpDithered}
%	\pnorm{\M \odot \tilde{\SIGMA}'_n - \M \odot \SIGMA}{}
%	\le \pnorm{ \M \odot \Big( \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) }{} + \pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{},
%	\end{align}
%    where 
%Recall that $\E_{k}$ abbreviates the conditional expectation with respect to the first $k$ samples $\X^1,\dots,\X^k$ and dithers $\Tau^1,\bar{\Tau}^1,\dots,\Tau^k,\bar{\Tau}^k$, and we abbreviate $\Y_k = \sign(\X^k + \Tau^k)$ and $\bar{\Y}_k = \sign(\X^k + \bar{\Tau}^k)$. By noting that $\M$ has only \sjoerd{nonnegative} entries and applying Lemma~\ref{lem:linftyBiasEst}, we obtain with probability at least $1-e^{-c\theta}$
%	\begin{align}
%	\label{eqn:normEstPos}
%	\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{} \nonumber
%	&=  \sup_{\v,\w \in \R^p \ : \ \|\v\|_2, \|\w\|_2 \leq 1} \abs{\sum_{i,j=1}^p M_{i,j} \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)_{i,j} v_i w_j }{} \nonumber\\
%	&
%	\le  \sup_{\v,\w \in \R^p \ : \ \|\v\|_2, \|\w\|_2 \leq 1} \sum_{i,j=1}^p M_{i,j} \abs{ \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)_{i,j} }{} \abs{v_i w_j}{}  \nonumber\\
%	&\le \pnorm{\M}{} \pnorm{\sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
%	\le \frac{\pnorm{\M}{}}{n} \sum_{k=1}^n \pnorm{ \E_{k-1}( \tilde\lambda_k^2 \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
%	\nonumber \\
%	&
%	\sjoerd{\lesssim_K} \frac{\pnorm{\M}{}}{n} \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
%	& \lesssim_{K,L} \theta^2\pnorm{\M}{} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n},
%	\end{align}
%where in the final step we applied \eqref{eqn:sumLambdaEstimate1}. 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:FrobeniusDitheredMask}}
\label{sec:ProofOfFrobeniusDitheredMask}

%We have now everything at hand to show Theorem \ref{thm:FrobeniusDitheredMask}. For convenience, we control the deviation of \eqref{eq:AsymmetricEstimatorAdaptive} by the following separate lemma.

With Lemma~\ref{lem:biasControlUnified} at hand, it remains to estimate the martingale difference term in \eqref{eqn:splitExpDithered}, i.e., the first term on the right hand side of that equation. The martingale differences in question are not uniformly bounded, so that we cannot use the Azuma-Hoeffding inequality to estimate this term. Instead, we will use the following classical extension of Rosenthal's inequality \cite{Ros70}, due to Burkholder \cite{Bur73}. The stated version with the constants $\sqrt{q}$ and $q$ can be obtained by combining \cite{NaP78} (who proved this result for independent summands) with a decoupling argument \cite{Hit94}, see \cite[Remark 3.6]{DiY19} for a more detailed discussion.

\begin{theorem}
\label{thm:BRreal}
There exists a constant $c>0$ such that for any real-valued martingale difference sequence $(\xi_k)_{k=1}^n$ 
%be a , i.e., a series of random variables with $\E |\xi_k| < \infty$ and $\E_{k-1} \xi_k~:=~\E[\xi_k | \xi_1,\dots, \xi_{k-1}]~=~0$, for any $k \ge 2$. Then, for 
and any $2\leq q<\infty$,
\begin{equation}
\label{eqn:BRreal}
\left(\E\left|\sum_{k=1}^n \xi_k\right|^q\right)^{\frac{1}{q}} \leq c\sqrt{q}\left(\E\left(\sum_{k=1}^n \E_{k-1}|\xi_k|^2\right)^{\frac{q}{2}}\right)^{\frac{1}{q}}+ cq\left(\sum_{k=1}^n\E |\xi_k|^q\right)^{\frac{1}{q}}.
\end{equation}
\end{theorem}
To prove Theorem \ref{thm:FrobeniusDitheredMask}, we will use Theorem~\ref{thm:BRreal} in combination with the following well-known observation, which formalizes that one can pass between tail bounds and $L^q$-bounds at the expense of additional (absolute) constant factors. We provide a proof to keep our exposition self-contained.
\begin{lemma}
\label{lem:LqtoTailBound}
    Assume $0\leq q_0<q_1\leq \infty$ and let $\xi$ be a random variable. 
    \begin{enumerate}
        \item[(i)] Let $J \in \mathbb N$, $p_j>0$ and $a_j\geq 0$ for $j=1,\ldots,J$, and suppose that
        \begin{align*}
            (\mathbb{E}|\xi|^q)^{1/q}\leq \sum_{j=1}^J a_j q^{p_j},
        \end{align*}
        for all $q_0\leq q\leq q_1$. Then, 
        \begin{align*}
            \P{|\xi|\geq e \cdot \sum_{j=1}^J a_j t^{p_j}} \leq e^{-t}
        \end{align*}
        for all $q_0\leq t\leq q_1$. 
        \item[(ii)] If
        \begin{align*}
            \P{|\xi|\geq  a t^{p}} \leq e^{-t},
        \end{align*}
        for fixed $a>0$, $p\geq \tfrac{1}{2}$, and any $t \ge q_0$, then
        \begin{align*}
            (\mathbb{E}|\xi|^q)^{1/q} \lesssim aq_0^p + p^{p} a q^p,
        \end{align*}
        for any $q \ge 1$.
    \end{enumerate}
\end{lemma}

\begin{proof}
		To show $(i)$, 
  %let $s>0$ be such that 
%		$$
%            J \max_{1\le j \le J} \{a_j q_0^{p_j}\} \leq s
%            \leq J \max_{1\le j \le J} \{a_j q_1^{p_j}\}.
%        $$
%		Then
%		\begin{align*}
%		    q:=\min_{1\le j \le J} \left\{\frac{s^{\frac{1}{p_j}}}{J^{\frac{1}{p_j}} a_j^{\frac{1}{p_j}}} \right\}
%		\end{align*}
%		satisfies $q_0\leq q\leq q_1$. By 
  we use Markov's inequality to find
		\begin{align*}
		    \johannes{\P{|\xi|\geq e\sum_{j=1}^J a_j t^{p_j}}}{}
    		\leq \frac{\mathbb{E}|\xi|^t}{(e\sum_{j=1}^J a_j t^{p_j})^t}\leq e^{-t}.
		\end{align*}

        To see $(ii)$, note that by assumption $\P{|\xi|\geq  u \}} \leq e^{-\frac{u^{1/p}}{a^{1/p}}}$ for all $u\geq a q_0^p$. Hence, for any $q \ge 1$
        \begin{align*}
            \mathbb{E}|\xi|^q 
            &= \int_0^{aq_0^p} q u^{q-1} \P{|\xi|\geq  u } du
            + \int_{aq_0^p}^\infty q u^{q-1} \P{|\xi|\geq  u } du 
            \le a^q q_0^{qp} + q \cdot \int_{aq_0^p}^\infty u^{q-1} \cdot e^{-\frac{u^{1/p}}{a^{1/p}}} du \\
            &\leq a^q q_0^{qp} + q \cdot \int_{0}^\infty (a t^p)^{q-1} \cdot e^{-t} \cdot (apt^{p-1}) dt 
            = a^q q_0^{qp} + pq \cdot a^q \cdot \Gamma(pq) \\
            &\le a^q q_0^{qp} + \sqrt{2\pi} a^q \cdot (pq)^{pq+\frac{1}{2}} \cdot e^{-pq},
        \end{align*}
        where we used integration by parts and Stirling's approximation of the Gamma function. Taking the $q$-th root on both sides of the inequality now yields $(ii)$.
\end{proof}
%\begin{lemma}
%\label{lem:LqtoTailBound}
%    Assume $0\leq q_0<q_1\leq \infty$ and let $\xi$ be a random variable. 
%    \begin{enumerate}
%        \item[(i)] Let $J \in \mathbb N$, $p_j>0$ and $a_j\geq 0$ for $j=1,\ldots,J$, and suppose that
%        \begin{align*}
%            (\mathbb{E}|\xi|^q)^{1/q}\leq \sum_{j=1}^J a_j q^{p_j},
%        \end{align*}
%        for all $q_0\leq q\leq q_1$. Then, 
%        \begin{align*}
%            \P{|\xi|\geq J\cdot e \cdot \max_{1\le j \le J} \{a_j t^{p_j}\}} \leq e^{-t}
%        \end{align*}
%        for all $q_0\leq t\leq q_1$. 
%        \item[(ii)] If
%        \begin{align*}
%            \P{|\xi|\geq  a t^{p} \}} \leq e^{-t},
%        \end{align*}
%        for fixed $a,p > 0$ and any $t \ge q_0$, then
%        \begin{align*}
%            (\mathbb{E}|\xi|^q)^{1/q} \lesssim p^{p+2} \cdot a q^p + q_0,
%        \end{align*}
%        for any $q \ge 1$.
%    \end{enumerate}
%\end{lemma}
%
%\begin{proof}
%		To see $(i)$, let $s>0$ be such that 
%		$$
%            J \max_{1\le j \le J} \{a_j q_0^{p_j}\} \leq s
%            \leq J \max_{1\le j \le J} \{a_j q_1^{p_j}\}.
%        $$
%		Then
%		\begin{align*}
%		    q:=\min_{1\le j \le J} \left\{\frac{s^{\frac{1}{p_j}}}{J^{\frac{1}{p_j}} a_j^{\frac{1}{p_j}}} \right\}
%		\end{align*}
%		satisfies $q_0\leq q\leq q_1$. By Markov's inequality,
%		\begin{align*}
%		    \johannes{\P{|\xi|\geq es}}{}
%    		\leq \frac{\mathbb{E}|\xi|^q}{(es)^q}\leq \left(\frac{ \sum_{j=1}^J a_j q^{p_j} }{es}\right)^q
%    		\johannes{\le \round{\frac{1}{e}}^{q} = }
%    		\exp\left(-\min_{1 \le j \le J} \left\{\frac{s^{\frac{1}{p_j}}}{J^{\frac{1}{p_j}} a_j^{\frac{1}{p_j}}}\right\}\right).
%		\end{align*}
%		Setting $s=J \max_{1 \le j \le J} \{a_j t^{p_j} \}$ yields $(i)$.
%
%        To see $(ii)$, note that by assumption $\P{|\xi|\geq  t \}} \leq e^{-\frac{t^{1/p}}{a^{1/p}}}$ such that for any $q \ge 1$
%        \begin{align*}
%            \mathbb{E}|\xi|^q 
%            &= \int_0^{q_0} q t^{q-1} \P{|\xi|\geq  t } dt
%            + \int_{q_0}^\infty q t^{q-1} \P{|\xi|\geq  t } dt 
%            \le q_0^q + q \cdot \int_{q_0}^\infty t^{q-1} \cdot e^{-\frac{t^{1/p}}{a^{1/p}}} dt \\
%            &= q_0^q + q \cdot \int_{q_0}^\infty (a \tilde t^p)^{q-1} \cdot e^{-\tilde t} \cdot (ap \tilde t^{p-1}) d\tilde t 
 %           \le q_0^q + pq \cdot a^q \cdot \Gamma(pq) \\
 %           &\le q_0^q + \sqrt{4\pi} a^q \cdot (pq)^{pq + \frac{3}{2}} \cdot e^{-pq} \\
 %           &\le q_0^q + \sqrt{4\pi} p^{(p+2)q} a^q \cdot q^{pq + \frac{3}{2}},
 %       \end{align*}
 %       where we used integration by parts and Stirling's approximation of the Gamma function. Taking the $q$-th root on both sides of the inequality now yields $(ii)$.
%\end{proof}

Finally, we have all the tools we need to prove Theorem \ref{thm:FrobeniusDitheredMask}. 
%For the reader's convenience, we restate the result before doing so.
%
%\begingroup
%\renewcommand\thetheorem{\ref{thm:FrobeniusDitheredMask}}
%\begin{theorem}
%    For any $K>0$, there exist constants $C_1,C_2>0$ depending only on $K$ such that the following holds. Let $\X \in \mathbb R^p$ be a mean-zero, $K$-subgaussian vector with covariance matrix \sjoerd{$\SIGMA \in \R^{p\times p}$}.
%    Let $\X^1,...,\X^n \overset{\mathrm{i.i.d.}}{\sim} \X$ and let $\M \in [0,1]^{p\times p}$ be a fixed symmetric mask. If the adaptive dithering parameters satisfy $\tilde\lambda_k^2 = C_1 \log(k) \lambda_k^2$, where $\lambda_k$ is defined in \eqref{eq:lambdaAdaptive}, then for any $\theta \ge C_2\max\{\log(p),\log(n)\}$ we have with probability at least $1 - 2e^{-\theta}$   
%    \begin{align*}
%        \pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{\infty}
%        \lesssim_{K} \| \M \|_\infty \| \SIGMA \|_\infty\log(p) \log(n)\left(\frac{\theta^{3/2}}{\sqrt{n}} + \frac{\theta^2}{n}\right)
%    \end{align*}
%    and 
%    \begin{align*}
%        \pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{F}
%        \lesssim_{K} \| \M \|_F \| \SIGMA \|_\infty\log(p) \log(n)\left(\frac{\theta^{3/2}}{\sqrt{n}} + \frac{\theta^2}{n}\right).
%    \end{align*}
%\end{theorem}
%\endgroup

\begin{proof}[Proof of Theorem~\ref{thm:FrobeniusDitheredMask}]
We only need to show the maximum norm bound. The Frobenius norm bound then follows from the fact that $\| \M \odot \A \|_F \le \| \M \|_F \| \A  \|_\infty$, for any $\A \in \R^{p\times p}$.

Recall \eqref{eqn:splitExpDithered} and abbreviate $\Y_k = \sign(\X_k + \Tau_k)$ and $\bar{\Y}_k = \sign(\X_k + \bar{\Tau}_k)$. Set $\tilde\lambda_k = \sqrt{\frac{4}{L^2 c_2} \log(k^2)} \lambda_k$ for $k\geq 1$, where $c_2$ and $L$ are the constants from Lemmas \ref{lem:linftyBiasEst} and \ref{lem:linfConcentration}, respectively. Applying Lemma~\ref{lem:biasControlUnified} to the second term on the right hand side of \eqref{eqn:splitExpDithered} yields
\begin{align}
\label{eq:SecondTermBoundLinfty}
    \left\| \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big) \right\|_\infty  \lesssim_{K} \theta \| \M \|_\infty \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n}.
\end{align} 
To complete the proof, we estimate the first term on the right hand side of \eqref{eqn:splitExpDithered}.
%The proof is similar to the one of Theorem \ref{thm:OperatorDitheredMask}.
First note that for $q \geq \log(m)$ and random variables $Z_1,\dots,Z_m$
\begin{align}
\label{eq:Lq_maxBound}
    \left( \E \max_{i \in [m]} |Z_i|^q \right)^{\frac{1}{q}}
    \le \left( \sum_{i=1}^m \E |Z_i|^q \right)^{\frac{1}{q}}
    \le m^{\frac{1}{\log(m)}} \max_{i \in [m]} \left( \E |Z_i|^q \right)^{\frac{1}{q}} 
    \lesssim \max_{i \in [m]} \left( \E |Z_i|^q \right)^{\frac{1}{q}}
\end{align}
so that for any $q \ge \log(p^2)$
\begin{align}
\label{eq:FirstTermBoundLinfty}
    \left(\E\left\|\SIGMA'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T)\right\|_{\infty}^q\right)^{1/q} 
    & \lesssim \frac{1}{n} \max_{1\leq i,j\leq p}\left(\E\left(\sum_{k=1}^n \tilde\lambda_k^2 \Big( \Y_k\bar{\Y}_k^T - \E_{k-1}( \Y_k\bar{\Y}_k^T) \Big)_{ij}\right)^q\right)^{1/q}.        
\end{align}
We aim to apply Theorem~\ref{thm:BRreal} to the right-hand side of \eqref{eq:FirstTermBoundLinfty}.
Fix $i,j \in [p]$ and define the sequence
$$\xi_k := \tilde\lambda_k^2 \Big( (Y_k)_i (\bar{Y}_k)_j - \E_{k-1}( (Y_k)_i (\bar{Y}_k)_j) \Big).$$
We need to check the assumptions of Theorem \ref{thm:BRreal} and to estimate the quantities appearing on the right-hand side of \eqref{eqn:BRreal}.
%Clearly, $\E |\xi_k| < \infty$ and $\E_{k-1} \xi_k~:=~\E[\xi_k | \xi_1,\dots, \xi_{k-1}]~=~0$, for any $k\ge 2$, such that $\xi_k$ forms a real-valued martingale difference. \\
Clearly, $(\xi_k)$ forms a real-valued martingale difference sequence with respect to the filtration defined by \eqref{eqn:filtrationDef}, i.e.,
$$\mathcal{F}_k = \sigma(\X_0,\X_1,\ldots,\X_k,\Tau_1,\ldots,\Tau_k,\bar{\Tau}_1, \ldots,\bar{\Tau}_k), \qquad k=0,1,\ldots,n.$$
Further observe that $\tilde\lambda_k$ is $\mathcal{F}_{k-1}$-measurable and hence
\begin{align*}
    \E_{k-1}|\xi_k|^2 = \tilde\lambda_k^4 \E_{k-1}\Big[\Big( (Y_k)_i (\bar{Y}_k)_j - \E_{k-1}( (Y_k)_i (\bar{Y}_k)_j) \Big)^2\Big]\leq 4 \tilde\lambda_k^4
\end{align*}
since $(Y_k)_i,(Y_k)_j \in \{-1,1\}$. Hence, by combining \eqref{eqn:elemLambdaEstimate} in Lemma \ref{lem:SumBounds} with Lemma \ref{lem:LqtoTailBound} we obtain for any $q \ge 1$ that
\begin{align*}
    \left\|\left(\sum_{k=1}^n \E_{k-1}|\xi_k|^2\right)^{\frac{1}{2}}\right\|_{L^q} 
    &\leq 2 \left(\sum_{k=1}^n \|\tilde\lambda_k^4\|_{L^{\frac{q}{2}}}\right)^{1/2} 
    = 2\left(\sum_{k=1}^n \|\tilde\lambda_k\|_{L^{2q}}^4\right)^{1/2} \\
    &\lesssim_{K}  \left(\sum_{k=1}^n q^2\log^2(p)\log^2(k)\|\SIGMA\|_{\infty}^2\right)^{1/2} \\
    &\leq \sqrt{n} \cdot q\log(p)\log(n)\|\SIGMA\|_{\infty}.
\end{align*} 
Along similar lines, we get for any $q \ge \log(n)$ that
\begin{align*}
    \left(\sum_{k=1}^n \|\xi_k\|_{L^q}^q \right)^{\frac{1}{q}}
    &\leq 2\left(\sum_{k=1}^n\|\tilde\lambda_k^2\|_{L^q}^q\right)^{\frac{1}{q}} \lesssim_K n^{1/q} \cdot q\log(p)\log(n)\|\SIGMA\|_{\infty} \\
    &\lesssim q\log(p)\log(n)\|\SIGMA\|_{\infty}.
\end{align*}
Applying Theorem~\ref{thm:BRreal} now to the right-hand side of \eqref{eq:FirstTermBoundLinfty} yields 
 \begin{align*}
        \left(\E\left\|\SIGMA'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T)\right\|_{\infty}^q\right)^{1/q} \lesssim_K \frac{q^{3/2}\log(p)\log(n)\|\SIGMA\|_{\infty}}{\sqrt{n}} + \frac{q^2\log(p)\log(n)\|\SIGMA\|_{\infty}}{n},
\end{align*}
for any $q \ge \max\{ 2\log(q),\log(n) \}$. Translating this into a tail bound via Lemma \ref{lem:LqtoTailBound} and inserting it together with \eqref{eq:SecondTermBoundLinfty} into the right-hand side of \eqref{eqn:splitExpDithered} concludes the proof. 
\end{proof}

%The following result completes the proof of Theorem~\ref{thm:FrobeniusDitheredMask}. 
%\begin{lemma}
%\label{lem:LinftyConcentration}
%    There exist constants $c,c' > 0$ such that the following holds. Let $\X$ be a mean-zero, $K$-subgaussian vector with covariance matrix \sjoerd{$\SIGMA$} and let $\X^1,...,\X^n \overset{\mathrm{i.i.d.}}{\sim} \X$. Then, we have for $\tilde{\SIGMA}'_n$ as defined in \eqref{eq:AsymmetricEstimatorAdaptive} that, for any $t > 0$,
%    \begin{align*}
%        \P{ \pnorm{ \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) }{\infty} > \sqrt{\tilde\lambda_k^4 \Big( c' \frac{\log(p)}{n} + t \Big)} } \le 2e^{-cnt},
%    \end{align*}
%    where $\E_{k}$ abbreviates the conditional expectation which is conditioned to the first $k$ samples $\X^1,\dots,\X^k$ and dithers $\Tau^1,\bar{\Tau}^1,\dots,\Tau^k,\bar{\Tau}^k$, and we abbreviate $\Y_k = \sign(\X^k + \Tau^k)$ and $\bar{\Y}_k = \sign(\X^k + \bar{\Tau}^k)$.
%\end{lemma}
%
%\begin{proof}
%    First note that 
%    \begin{align*}
%        \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) 
%        = \frac{1}{n} \sum_{k=1}^n \tilde\lambda_k^2 \Big( \Y_k\bar{\Y}_k^T - \E_{k-1}( \Y_k\bar{\Y}_k^T) \Big).
%    \end{align*}
%    Let us fix $i,j \in [p]$ and define $Z^k := \tilde\lambda_k^2 \Big( (Y_k)_i (\bar{Y}_k)_j - \E_{k-1}( (Y_k)_i (\bar{Y}_k)_j) \Big)$. Since $\E_{k-1} Z^k = 0$, $Z^k$ is a martingale difference sequence with associated martingale $M^n := \sum_{k=1}^n Z^k$ (where we define $M^0 = 0$). Clearly, $|M^n - M^{n-1}| \le 2 \tilde\lambda_k^2$ such that Azuma's inequality \cite[Thm.\ 7.2.1]{alon2016probabilistic} yields
%    \begin{align*}
%        \P{ \left| \frac{1}{n} \sum_{k=1}^n Z^k \right| \ge u }
%        = \P{ \left| M^n \right| \ge nu }
%        \le 2e^{-\frac{n^2u^2}{2 n \tilde\lambda_k^4}} 
%        = 2e^{-cn \frac{u^2}{\tilde\lambda_k^4}}.
%    \end{align*}
%    By setting $u = \sqrt{\tilde\lambda_k^4 \Big( c' \frac{\log(p)}{n} + t \Big)}$, for $t > 0$ and appropriately chosen $c' > 0$, and applying a union bound over all $p^2$ entries, the claim follows.
%\end{proof}
%
%\begin{proof}[Proof of Theorem \ref{thm:FrobeniusDitheredMask}]
%    We only need to show the $\ell_\infty$-norm bound. The Frobenius-norm bound follows then from $\| \M \odot \A \|_F \le \| \M \|_F \| \A  \|_\infty$, for any $\A \in \R^{p\times p}$. The proof is similar to the one of Theorem \ref{thm:OperatorDitheredMask}.
%
%    Recall the definitions of $\SIGMADITH_n$ and $\tilde{\SIGMA}'_n$ in \eqref{eq:TwoBitEstimatorAdaptive} and \eqref{eq:AsymmetricEstimatorAdaptive}. Clearly,
%	$$\pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{\infty} \leq \pnorm{\M \odot \tilde{\SIGMA}'_n - \M \odot \SIGMA}{\infty}$$
%	and
%	\begin{align}
%	\label{eqn:splitExpDithered2}
%	\pnorm{\M \odot \tilde{\SIGMA}'_n - \M \odot \SIGMA}{\infty}
%	\le \pnorm{ \M \odot \Big( \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) }{\infty} + \pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{\infty},
%	\end{align}
%    where $\E_{k}$ abbreviates the conditional expectation which is conditioned to the first $k$ samples $\X^1,\dots,\X^k$ and dithers $\Tau^1,\bar{\Tau}^1,\dots,\Tau^k,\bar{\Tau}^k$, and we abbreviate $\Y_k = \sign(\X^k + \Tau^k)$ and $\bar{\Y}_k = \sign(\X^k + \bar{\Tau}^k)$. 
%    
%    Lemma \ref{lem:LinftyConcentration} yields that 
%    \begin{align}
%    \label{eq:LinftyBoundI}
%        \P{ \pnorm{ \M \odot \Big( \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) }{\infty} > \| \M \|_\infty \sqrt{\tilde\lambda_k^4 \Big( c' \frac{\log(p)}{n} + t \Big)} } \le 2e^{-cnt},
%    \end{align}
%    
%    All that remains is to bound the second term in \eqref{eqn:splitExpDithered2}. By applying Lemma~\ref{lem:linftyBiasEst}, we obtain 
%	\begin{align}
%	\label{eqn:normEstPos2}
%	\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{\infty} \nonumber 
%	&\le \pnorm{\M}{\infty} \pnorm{\sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty} \nonumber \\
%	&\le \frac{\pnorm{\M}{\infty}}{n} \sum_{k=1}^n \pnorm{ \E_{k-1}( \tilde\lambda_k^2 \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
%	\nonumber \\
%	&
%	\sjoerd{\lesssim_K} \frac{\pnorm{\M}{\infty}}{n} \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}.
%	\end{align}
%	Set $\tilde\lambda_k = \sqrt{\frac{8}{L^2 c_1} \log(k^2)} \lambda_k$ for $k\geq 1$ and set $\theta>1$. By \eqref{eqn:sumLambdaEstimate1}, with probability at least $1-e^{-c\theta}$,
%%
%%    We set $\tilde\lambda_k = \sqrt{\frac{8}{L^2 c_1} \log(k)} \lambda_k$ and $k_\theta = \lceil \theta \frac{K^2}{cL^2} \log(p) \rceil$, for $\theta \ge 1$. By applying Lemma \ref{lem:linfConcentration}, we obtain for any $k \ge 2$ with probability at least $1-2e^{-c\theta^2}$ that
%%    \begin{align} 
%%        \lambda_k \le \theta K \sqrt{\| \SIGMA \|_\infty \log(2p)}
%%    \end{align}
%%    and, for any $k > k_\theta$, with probability at least $1-2e^{-\frac{1}{4}\theta}$ that
%%    \begin{align} 
%%        \lambda_k \ge \frac{1}{2} L \sqrt{\| \SIGMA \|_\infty}.
%%    \end{align}
%%%    Applying a union bound, \eqref{eqn:normEstPos2} thus becomes with probability at least $1-2n e^{-c\theta}$
%%    \begin{align} \label{eq:SumSplit2}
%%    \begin{split}
%%        &
%        $$\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{\infty} 
%    	\lesssim_K \frac{\pnorm{\M}{\infty}}{n} \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}}
%	\lesssim_K \theta\frac{\pnorm{\M}{\infty}}{n}\log(p)\log(n) \|\SIGMA\|_{\infty}$$
%%	
%%        &\lesssim_{K,L} \frac{\pnorm{\M}{\infty}}{n} \left( \sum_{k=1}^{k_\theta} \Big( \log(k)\lambda_k^2+\|\SIGMA\|_{\infty} \Big) + \sum_{k=k_\theta + 1}^n \Big(\log(k)\lambda_k^2+\|\SIGMA\|_{\infty} \Big) e^{- \frac{8\log(k)}{L^2} \lambda_k^2/\|\SIGMA\|_{\infty}} \right) \\
%%        &\lesssim_{K,L} \frac{\pnorm{\M}{\infty}}{n} 
%%         \left( k_\theta \cdot (\theta^2 \log(2p) \log(k_\theta) + 1) \| \SIGMA \|_\infty +  
%%         \sum_{k=k_\theta+1}^n \frac{(\theta^2 \log(2p) \log(n) + 1) \| \SIGMA \|_\infty}{k^2}
%%         \right) \\
%%    	&\lesssim_{K,L} \theta^2\pnorm{\M}{\infty} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n}.
%%    \end{split}
%%	\end{align}
%    Consequently, we get that the second term in \eqref{eqn:splitExpDithered2} can be bounded with probability at least $1-e^{c'\theta}$
%    %$1 - 2e^{-(\theta - 1) \log(n)}$ as
%    \begin{align}
%    \label{eq:LinftyBoundII}
%    	\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{\infty} 
%    	\lesssim_{K,L} \theta^2\pnorm{\M}{\infty} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)^3}{n}.
%	\end{align}
%    The claim now follows from combining \eqref{eq:LinftyBoundI} and \eqref{eq:LinftyBoundII} with a union bound in \eqref{eqn:splitExpDithered2}.
%\end{proof}

\subsection{Proof of Theorem \ref{thm:OperatorDitheredMask}}
\label{sec:ProofOfOperatorDitheredMask}

The proof follows the lines of the proof of Theorem~\ref{thm:FrobeniusDitheredMask}. We again use Lemmas~\ref{lem:biasControlUnified} and \ref{lem:SumBounds}. The main difference is that we use the Burkholder-Rosenthal for matrix martingales (Theorem \ref{thm:matrixBRintro}) instead of Theorem \ref{thm:BRreal}. We will additionally need the following observation, which is a simple consequence of Schur's product theorem \cite[Eq. (3.7.11)]{johnson1990matrix}.
% to prove Theorem \ref{thm:OperatorDitheredMask}. 
%The first controls the operator norm of Hadamard products.

\begin{lemma}
%[{{dirksen2021covariance}}]
\label{lem:HadamardOperatorNorm}
	Let $\A,\B \in \R^{p\times p}$, let $\A$ be symmetric and define $\C = (\A^T\A)^\frac{1}{2}$. Then
	\begin{align*}
	\pnorm{\A \odot \B}{} \le \round{\max_{i \in [p]} C_{ii}} \pnorm{\B}{} = \pnorm{\A}{1\rightarrow 2} \pnorm{\B}{}.
	\end{align*}{}
	If $\A$ is in addition positive semidefinite, then
	\begin{align*}
	\pnorm{\A \odot \B}{} \le \round{\max_{i \in [p]} A_{ii} } \pnorm{\B}{}.
	\end{align*}
\end{lemma}{}

%As with Theorem \ref{thm:FrobeniusDitheredMask} above, we restate the result for the reader's convenience. 
%
%\begingroup
%\renewcommand\thetheorem{\ref{thm:OperatorDitheredMask}}
%\begin{theorem} 
%For any $K > 0$, there exist constants $C_1,C_2>0$ depending only on $K$ such that the following holds. Let $\X \in \R^p$ be a mean-zero, $K$-subgaussian vector with covariance matrix \sjoerd{$\SIGMA \in \R^{p\times p}$}.
%Let $\X^1,...,\X^n \overset{\mathrm{i.i.d.}}{\sim} \X$ and let $\M \in [0,1]^{p\times p}$ be a fixed symmetric mask. If the adaptive dithering parameters satisfy $\tilde\lambda_k^2 = C_1 \log(k) \lambda_k^2$, where $\lambda_k$ is defined in \eqref{eq:lambdaAdaptive}, then for any $\theta \ge C_2\max\{\log(p),\log(n)\}$ we have with probability at least $1 - 2e^{-\theta}$   
%	$$\pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{}\lesssim_{K} 
%	\log(p)\log(n)\Big(\theta^{5/2}\frac{\|\M\|_{1\to 2}}{\sqrt{n}}\|\SIGMA\|_{\infty}^{1/2}\|\SIGMA\|^{1/2}+\theta^3\frac{\|\M\|}{n} \|\SIGMA\|_{\infty}\Big).
%	$$
%\end{theorem}
%\endgroup


\begin{proof}[Proof of Theorem~\ref{thm:OperatorDitheredMask}]
%    Recall the definitions of $\SIGMADITH_n$ and $\tilde{\SIGMA}'_n$ in \eqref{eq:TwoBitEstimatorAdaptive} and \eqref{eq:AsymmetricEstimatorAdaptive}. Clearly,
%	$$\pnorm{\M \odot \SIGMADITH_n - \M \odot \SIGMA}{} \leq \pnorm{\M \odot \tilde{\SIGMA}'_n - \M \odot \SIGMA}{}$$
%	and
%	\begin{align}
%	\label{eqn:splitExpDithered}
%	\pnorm{\M \odot \tilde{\SIGMA}'_n - \M \odot \SIGMA}{}
%	\le \pnorm{ \M \odot \Big( \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) }{} + \pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{},
%	\end{align}
%    where $\E_{k}$ abbreviates the conditional expectation which is conditioned to the first $k$ samples $\X^1,\dots,\X^k$ and dithers $\Tau^1,\bar{\Tau}^1,\dots,\Tau^k,\bar{\Tau}^k$, and we abbreviate $\Y_k = \sign(\X^k + \Tau^k)$ and $\bar{\Y}_k = \sign(\X^k + \bar{\Tau}^k)$. By noting that $\M$ has only \sjoerd{nonnegative} entries and applying Lemma~\ref{lem:linftyBiasEst}, we obtain with probability at least $1-e^{-c\theta}$
%	\begin{align}
%	\label{eqn:normEstPos}
%	\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{} \nonumber
%	&=  \sup_{\v,\w \in \R^p \ : \ \|\v\|_2, \|\w\|_2 \leq 1} \abs{\sum_{i,j=1}^p M_{i,j} \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)_{i,j} v_i w_j }{} \nonumber\\
%	&
%	\le  \sup_{\v,\w \in \R^p \ : \ \|\v\|_2, \|\w\|_2 \leq 1} \sum_{i,j=1}^p M_{i,j} \abs{ \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)_{i,j} }{} \abs{v_i w_j}{}  \nonumber\\
%	&\le \pnorm{\M}{} \pnorm{\sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
%	\le \frac{\pnorm{\M}{}}{n} \sum_{k=1}^n \pnorm{ \E_{k-1}( \tilde\lambda_k^2 \Y_k\bar{\Y}_k^T) - \SIGMA}{\infty}
%	\nonumber \\
%	&
%	\sjoerd{\lesssim_K} \frac{\pnorm{\M}{}}{n} \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
%	& \lesssim_{K,L} \theta^2\pnorm{\M}{} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n},
%	\end{align}
%where in the final step we applied \eqref{eqn:sumLambdaEstimate1}. 
%
%    We set $\tilde\lambda_k = \sqrt{\frac{8}{L^2 c_1} \log(k)} \lambda_k$ and $k_\theta = \lceil \theta \frac{K^2}{cL^2} \log(p) \rceil$, for $\theta \ge 1$. By applying Lemma \ref{lem:linfConcentration}, we obtain for any $k \ge 2$ with probability at least $1-2e^{-c\theta^2}$ that
%    \begin{align} \label{eq:LambdaUB}
%        \lambda_k \le \theta K \sqrt{\| \SIGMA \|_\infty \log(2p)}
%    \end{align}
%    and, for any $k > k_\theta$, with probability at least $1-2e^{-\frac{1}{4}\theta}$ that
%    \begin{align} \label{eq:LambdaLB}
%        \lambda_k \ge \frac{1}{2} L \sqrt{\| \SIGMA \|_\infty}.
%    \end{align}
%    Applying a union bound, \eqref{eqn:normEstPos} thus becomes with probability at least $1-2n e^{-c\theta}$
%    \begin{align} \label{eq:SumSplit}
%    \begin{split}
%        &\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{} 
%    	\lesssim_K \frac{\pnorm{\M}{}}{n} \sum_{k=1}^n (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \\
%        &\lesssim_{K,L} \frac{\pnorm{\M}{}}{n} \left( \sum_{k=1}^{k_\theta} \Big( \log(k)\lambda_k^2+\|\SIGMA\|_{\infty} \Big) + \sum_{k=k_\theta + 1}^n \Big(\log(k)\lambda_k^2+\|\SIGMA\|_{\infty} \Big) e^{- \frac{8\log(k)}{L^2} \lambda_k^2/\|\SIGMA\|_{\infty}} \right) \\
%        &\lesssim_{K,L} \frac{\pnorm{\M}{}}{n} 
%         \left( k_\theta \cdot (\theta^2 \log(2p) \log(k_\theta) + 1) \| \SIGMA \|_\infty +  
%         \sum_{k=k_\theta+1}^n \frac{(\theta^2 \log(2p) \log(n) + 1) \| \SIGMA \|_\infty}{k^2}
%         \right) \\
%    	&\lesssim_{K,L} \theta^2\pnorm{\M}{} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n}.
%    \end{split}
%	\end{align}
%    Consequently, we get that the second term in \eqref{eqn:splitExpDithered} can be bounded with probability at least $1 - 2e^{-(\theta - 1) \log(n)}$ as
%    \begin{align}
%    \label{eq:BiasBound}
%    	\pnorm{ \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big)}{} 
%    	\lesssim_{K,L} \theta^2\pnorm{\M}{} \| \SIGMA \|_\infty \frac{\log(2p) \log(n)^3}{n}.
%	\end{align}
    

    Recall that $\Y_k = \sign(\X_k + \Tau_k)$, $\bar{\Y}_k = \sign(\X_k + \bar{\Tau}_k)$, and set $\tilde\lambda_k = \sqrt{\frac{4}{L^2 c_2} \log(k^2)} \lambda_k$ for $k\geq 1$, where $c_2$ and $L$ are the constants from Lemmas~\ref{lem:linftyBiasEst} and \ref{lem:linfConcentration}, respectively. 
%    Consider the filtration given by 
%$$\mathcal{F}_k = \sigma(\X_0,\X_1,\ldots,\X_k,\Tau_1,\ldots,\Tau_k,\bar{\Tau}_1, \ldots,\bar{\Tau}_k), \qquad k=0,1,\ldots,n,$$
%and let $\mathbb{E}
We again use the split in \eqref{eqn:splitExpDithered}. Applying Lemma~\ref{lem:biasControlUnified} to the second term on the right hand side of \eqref{eqn:splitExpDithered} yields
    \begin{align}
    \label{eq:SecondTermBoundLoperator}
        \left\| \M \odot \Big( \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) - \SIGMA \Big) \right\|  \lesssim_{K} \theta \| \M \| \| \SIGMA \|_\infty \frac{\log(2p) \log(n)}{n}.
    \end{align} 
    To complete the proof, we will estimate the first term on the right hand side of \eqref{eqn:splitExpDithered} using Theorem~\ref{thm:matrixBRintro}. To this end, define
    \begin{align*}
        {\XI}_k 
        = \frac{\tilde\lambda_k^2}{n} \M\odot \round{ \Y_k\bar{\Y}_k^T - \E_{k-1}\round{\Y_k\bar{\Y}_k^T} }{}
        \quad \text{ \sjoerd{so} that } \quad
        \M \odot \Big( \SIGMA'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) = \sum_{k=1}^n \XI_k.
    \end{align*}
Since $\E\|\XI_k\| < \infty$ and $\E_{k-1} \XI_k = \0$ by construction, the sequence $\XI_1,\dots,\XI_n$ forms a matrix martingale difference sequence with respect to the filtration \eqref{eqn:filtrationDef} and we can hence apply Theorem~\ref{thm:matrixBRintro}. Let us now estimate the quantities appearing in the bounds of Theorem \ref{thm:matrixBRintro}. For any $1\leq k\leq n$,
    \begin{equation*}
        %	\label{eqn:XikMaxNormEstimate}
        \|{\XI}_k\| = \frac{\tilde\lambda_k^2}{n} \| \M \odot \Y_k\bar{\Y}_k^T - \E_{k-1}\round{\M \odot \Y_k\bar{\Y}_k^T} \| 
        \leq \frac{2\tilde\lambda_k^2 \|\M\|}{n}
    \end{equation*}
    since $\M \odot \Y_k \bar{\Y}_k^T = \diag(\Y_k) \; \M  \; \diag(\bar{\Y}_k)$ and $\pnorm{\diag(\Y_k)}{} = 1$. Hence,
    %Combining the previous bound with our assumption
    %$q\geq \log(n)$, we find by an argument similar to \eqref{eq:Lq_maxBound} that
    \begin{equation}
    \label{eqn:XikMaxNormEstimateLq}
        \max_{1\leq k\leq n} (\E\|{\XI}_k\|^q)^{1/q} \lesssim \frac{\|\M\|}{n}\max_{1\leq k\leq n} \| \tilde\lambda_k^2 \|_{L^q}\lesssim_K \frac{\|\M\|}{n}q\|\SIGMA\|_{\infty}\log(p)\log(n)
    \end{equation} 
    where we used in the last inequality a moment bound for $\tilde\lambda_k^2$ derived from \eqref{eqn:elemLambdaEstimate} via Lemma \ref{lem:LqtoTailBound}. 
    Next, using that $(\Y_k,\bar{\Y}_k)$ and $(\bar{\Y}_k,\Y_k)$ are identically distributed, we get %(recall that $|\Z| = (\Z^T \Z)^\frac{1}{2}$)
    \begin{align*}
    & \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k) \Big)^{1/2}\Big\| \\
    &\qquad = \pnorm{ \sum_{k=1}^n \frac{\tilde\lambda_k^4}{n^2} \left( \mathbb{E}_{k-1}[(\M \odot \Y_k\bar{\Y}_k^T)^T(\M \odot \Y_k\bar{\Y}_k^T)] - \Big(\M \odot \johannes{\E_{k-1}\round{ \Y_k\bar{\Y}_k^T }}{} \Big)^T \Big(\M \odot \johannes{\E_{k-1}\round{ \Y_k\bar{\Y}_k^T }}{} \Big) \right) }{}^{1/2}\\
    &\qquad= \pnorm{ \sum_{k=1}^n \frac{\tilde\lambda_k^4}{n^2} \left( \M^2\odot\johannes{\E_{k-1}(\bar{\Y}_k\bar{\Y}_k^T)}{} - \Big( \M \odot \johannes{\E_{k-1}\round{ \Y_k\bar{\Y}_k^T }}{} \Big)^T \Big( \M \odot \johannes{\E_{k-1}\round{ \Y_k\bar{\Y}_k^T }}{} \Big) \right) }{}^{1/2}\\
    &\qquad= \pnorm{ \sum_{k=1}^n \frac{\tilde\lambda_k^4}{n^2} \left( \M^2\odot \johannes{\E_{k-1}(\Y_k\Y_k^T)}{} - \Big( \M \odot \johannes{\E_{k-1}\round{ \bar{\Y}_k\Y_k^T }}{} \Big)^T \Big( \M \odot \johannes{\E_{k-1}\round{ \bar{\Y}_k\Y_k^T }}{} \Big) \right) }{}^{1/2},
    \end{align*}
    where we used in the second equality that by $\diag(\Y_k)^2 = \id$
    \begin{align}
    \label{eq:Msquared}
        (\M\odot \Y_k\bar{\Y}_k^T)^T (\M\odot \Y_k\bar{\Y}_k^T)
       = \diag(\bar{\Y}_k)\;\M\; \diag(\Y_k)\;\diag(\Y_k)\;\M\;\diag(\bar{\Y}_k)
       =\M^2 \odot \bar{\Y}_k\bar{\Y}_k^T.
    \end{align}
    Interchanging the roles of $\Y_k$ and $\bar{\Y}_k$ yields
    \begin{align*}
    & \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T) \Big)^{1/2}\Big\| \\
    & \qquad = \pnorm{ \sum_{k=1}^n \frac{\tilde\lambda_k^4}{n^2} \left( \M^2\odot \johannes{\E_{k-1}(\Y_k\Y_k^T)}{} - \Big( \M \odot \johannes{\E_{k-1}\round{ \bar{\Y}_k\Y_k^T }}{} \Big)^T \Big( \M \odot \johannes{\E_{k-1}\round{ \bar{\Y}_k\Y_k^T }}{} \Big) \right) }{}^{1/2}.
    \end{align*}
    Since by \eqref{eq:Kadison}
    $$\Big( \M \odot \johannes{\E_{k-1}\round{ \bar{\Y}_k\Y_k^T }}{} \Big)^T \Big( \M \odot \johannes{\E_{k-1}\round{ \bar{\Y}_k\Y_k^T }}{} \Big) 
    \preceq \E_{k-1} \Big( (\M \odot \bar{\Y}_k\Y_k^T )^T ( \M \odot \bar{\Y}_k\Y_k^T ) \Big) $$ 
    we find
    \begin{align*}
    & \max\left\{ \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k) \Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T) \Big)^{1/2}\Big\| \right\} \\
    &\quad \leq \left( \sum_{k=1}^n \frac{2 \tilde\lambda_k^4}{n^2} \pnorm{ \M^2\odot \E_{k-1}(\Y_k\Y_k^T)}{} \right)^{1/2}\\
    & \quad \leq \left( \sum_{k=1}^n \frac{2 \tilde\lambda_k^2}{n^2} \pnorm{ \M^2\odot \left(\tilde\lambda_k^2\johannes{\E_{k-1}\round{\Y_k\Y_k^T}}{} - (\SIGMA-\diag(\SIGMA)+\tilde\lambda_k^2 \id) \right) }{} \right. \\
    & \quad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \left. \sum_{k=1}^n \frac{2 \tilde\lambda_k^2}{n^2} \pnorm{ \M^2\odot (\SIGMA-\diag(\SIGMA)+\tilde\lambda_k^2 \id) }{} \right)^{1/2},
    \end{align*}
    where we re-used \eqref{eq:Msquared} in the first inequality.
    By the same reasoning as in \eqref{eqn:normEstPos} together with Lemma~\ref{lem:linftyBiasEst} 
    \begin{align*}
    &\pnorm{ \M^2\odot \left(\tilde\lambda_k^2\johannes{\E_{k-1}\round{\Y_k\Y_k^T}}{} - (\SIGMA-\diag(\SIGMA)+\tilde\lambda_k^2 \id) \right) }{} \\
    &\quad \leq \|\M^2\| \pnorm{\tilde\lambda_k^2\johannes{\E_{k-1}\round{\Y_k\Y_k^T}}{} - (\SIGMA-\diag(\SIGMA)+\tilde\lambda_k^2 \id)}{\infty} 
     \sjoerd{\lesssim_K} \|\M\|^2 (\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} 
    \end{align*}
    Moreover, by Lemma~\ref{lem:HadamardOperatorNorm}, we find
	\begin{align*}
	\|\M^2 \odot (\SIGMA-\diag(\SIGMA)+\tilde\lambda_k^2 \id) \| 
	&\leq \|\M^2\odot\SIGMA\|+ \|\M^2\odot \id \odot\SIGMA\| + \tilde\lambda_k^2 \|\M^2\odot \id \| \\
	&\leq \|\M\|_{1\to 2}^2(2\|\SIGMA\|+\tilde\lambda_k^2).
	\end{align*}
    Thus, we obtain
	\begin{align}
	\label{eqn:XikSquareFunEstimate}
	& \max\left\{ \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k) \Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T) \Big)^{1/2}\Big\| \right\} \nonumber\\
		& \quad \lesssim \frac{\|\M\|}{n}\left( \sum_{k=1}^n\tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \right)^{1/2} + \frac{\|\M\|_{1\to 2}}{n} \left( \sum_{k=1}^n \tilde\lambda_k^2(\|\SIGMA\|+\tilde\lambda_k^2) \right)^{1/2}, 	
    \end{align}
    By translating \eqref{eqn:sumLambdaEstimate1} in Lemma \ref{lem:SumBounds} into a moment bound via Lemma \ref{lem:LqtoTailBound}, we find that
    \begin{align*}
        \left\|\left( \sum_{k=1}^n\tilde\lambda_k^2(\tilde\lambda_k^2+\|\SIGMA\|_{\infty})e^{-c_2\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \right)^{1/2}\right\|_{L^q} \lesssim_{K} q\log(p)\log(n) \|\SIGMA\|_{\infty}.
    \end{align*}
    Moreover, by again translating \eqref{eqn:elemLambdaEstimate} in Lemma \ref{lem:SumBounds} into a moment bound via Lemma \ref{lem:LqtoTailBound}, we further get that
    \begin{align*}
        \left\|\left( \sum_{k=1}^n \tilde\lambda_k^2(\|\SIGMA\|+\tilde\lambda_k^2) \right)^{1/2}\right\|_{L^q} & \leq \left(\sum_{k=1}^n \|\tilde\lambda_k^2\|_{L^{\frac{q}{2}}}\|\SIGMA\|+\|\tilde\lambda_k^4\|_{L^{\frac{q}{2}}} \right)^{1/2}\\
        & \lesssim_{K} \left(\sum_{k=1}^n q\log(p)\log(k)\|\SIGMA\|_{\infty}\|\SIGMA\|+q^2\log^2(p)\log^2(k)\|\SIGMA\|_{\infty}^2\right)^{1/2} \\
        & = \sqrt{n}\left(q\log(p)\log(n)\|\SIGMA\|_{\infty}\|\SIGMA\| +q^2\log^2(p)\log^2(n)\|\SIGMA\|_{\infty}^2\right)^{1/2}\\
        & \lesssim \sqrt{n}q\log(p)\log(n)\|\SIGMA\|_{\infty}^{1/2}\|\SIGMA\|^{1/2}
    \end{align*}
    Thus, taking $L^q$-norms in \eqref{eqn:XikSquareFunEstimate} we find
    	\begin{align}
    	\label{eqn:XikSquareFunEstimateLq}
    	& \max\left\{\left(\mathbb{E}\left\|\left(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k) \right)^{1/2}\right\|^q\right)^{1/q}, \left(\mathbb{E}\left\|\left(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T) \right)^{1/2}\right\|^q\right)^{1/q} \right\} \nonumber \\
    	& \quad \lesssim_K q\log(p)\log(n)\left(\frac{\|\M\|}{n} \|\SIGMA\|_{\infty} + \frac{\|\M\|_{1\to 2}}{\sqrt{n}}\|\SIGMA\|_{\infty}^{1/2}\|\SIGMA\|^{1/2}\right).
    	\end{align}
    By using Theorem~\ref{thm:matrixBRintro} and inserting \eqref{eqn:XikMaxNormEstimateLq} and \eqref{eqn:XikSquareFunEstimateLq} we finally get for any $q \ge \max\{\log(p),\log(n)\}$ that 
    \begin{align*}
    & \left(\mathbb{E}\left\|\M \odot \left( \SIGMA'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \right)\right\|^q\right)^{1/q} \\
    & \qquad \lesssim_K q^2\log(p)\log(n)\left(\sqrt{q}\frac{\|\M\|_{1\to 2}}{\sqrt{n}}\|\SIGMA\|_{\infty}^{1/2}\|\SIGMA\|^{1/2}+q\frac{\|\M\|}{n} \|\SIGMA\|_{\infty}\right).
    \end{align*}
    The result now immediately follows by applying Lemma~\ref{lem:LqtoTailBound} to the previous moment bound and inserting the resulting tail estimate with \eqref{eq:SecondTermBoundLoperator} into the right-hand side of \eqref{eqn:splitExpDithered}. 
%
%
%	\begin{align*}
%	&\max\left\{ \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k) \Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T) \Big)^{1/2}\Big\| \right\} \\
%	&\sjoerd{\lesssim_K} \max_{k \in [n]} \left\{ \frac{\tilde\lambda_k}{\sqrt{n}}
%	\|\M^2 \odot (\SIGMA-\diag(\SIGMA)+\tilde\lambda_k^2 \id) \|^{1/2} \right\}  + \frac{\|\M\|}{n} \left( \sum_{k=1}^n \tilde\lambda_k^2 (\tilde\lambda_k^2+\|\SIGMA\|_{\infty} )e^{-c_1\tilde\lambda_k^2/\|\SIGMA\|_{\infty}} \right)^\frac{1}{2}.
%	\end{align*}
%	Recalling \eqref{eq:LambdaUB}, \eqref{eq:LambdaLB}, \eqref{eq:SumSplit}, and \eqref{eq:BiasBound}, we obtain in summary on the same event as in \eqref{eq:BiasBound}, which holds with probability at least $1-2e^{-(\theta-1) \log(n)}$, that
%	\begin{align}
%	\label{eqn:XiksquareNormEstimate}
%	\begin{split}
%	    &\max\left\{ \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k) \Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T) \Big)^{1/2}\Big\| \right\} \\
%        &\qquad\qquad\qquad\qquad\qquad\qquad \lesssim_{K,L} \theta^2 \left( \frac{\|\M\|_{1\to 2}}{\sqrt{n}} + \frac{\pnorm{\M}{}}{n} \right) \log(2p)\log(n)^3 \| \SIGMA \|_\infty.
%	\end{split}
%	\end{align}
% By Theorem~\ref{thm:matrixBR}, \eqref{eqn:XikMaxNormEstimate}, and \eqref{eqn:XiksquareNormEstimate} we find on the above event that for any $q\geq 2\log(p)$
% $$\johannes{\round{ \E\pnorm{ \M \odot \Big( \tilde{\SIGMA}'_n - \sum_{k=1}^n \E_{k-1}( \tfrac{\tilde\lambda_k^2}{n} \Y_k\bar{\Y}_k^T) \Big) }{}^q}^{1/q}}{} \lesssim_{K,L} \theta^2 \left( \sqrt{q} \frac{\|\M\|_{1\to 2}}{\sqrt{n}} + q \frac{\pnorm{\M}{}}{n} \right) \log(2p)\log(n)^3 \| \SIGMA \|_\infty.$$
% 
% \red{JM: What is $\alpha_{q,d}$ in Theorem \ref{thm:matrixBR}? Are the above scalings in $q$ right if one applies Theorem \ref{thm:matrixBR}?}
% 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:matrixBRintro}}
\label{sec:Burkholder}

%We will now derive a version of the Burkholder-Rosenthal inequality for random matrices that is required for the proof of Theorem \ref{thm:OperatorDitheredMask} and might be of independent interest to some of the readers. 
%\footnote{\todo{Add proof of second statement, make $\gamma$ dependence explicit. Add refs. Change $d$ to $p$ to make everything consistent. Define matrix martingale difference sequence}} 
We will follow an argument from \cite{DiY19}, where Burkholder-Rosenthal inequalities were derived for $L^p$-valued martingale difference sequences. Our starting point is the following $L^q$-version of the matrix Bernstein inequality \cite{tropp2012user}, see \cite[Theorem 6.2]{Dir14}.
\begin{theorem}
	\label{thm:sumsRMs} 
	Let $2\leq q<\infty$. If $({\THETA}_k)_{k=1}^n$ is a sequence of independent, mean-zero random matrices in $\R^{p_1\times p_2}$, then
	\begin{align*}
	\johannes{\Big(\E \Big\|\sum_{k=1}^n {\THETA}_k\Big\|^q \Big)^{1/q}} & \leq C_{q,p}^{\operatorname{(MR)}} \max\Big\{\Big\|\Big(\sum_{k=1}^n \mathbb{E}(\THETA_k^T\THETA_k)\Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}(\THETA_k\THETA_k^T)\Big)^{1/2}\Big\|,\\
	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ 2C_{\frac{q}{2},p}^{\operatorname{(MR)}} \johannes{\Big(\E \max_{1\leq k\leq n} \|{\THETA}_k\|^q\Big)^{1/q}\Big\},}
	\end{align*}
	where $p=\max\{p_1,p_2\}$ and
	$C_{q,p}^{\operatorname{(MR)}}\leq  2^{\frac{3}{2}} e (1 + \sqrt{2}) \sqrt{\max\{q,\log p\}}$.  Moreover, the reverse inequality holds with universal constants. 
\end{theorem}

%\begin{theorem}
%	\label{thm:sumsRMs} 
%	Let $2\leq q<\infty$. If $({\XI}_k)_{k=1}^n$ is a sequence of independent, mean-zero random matrices in $\R^{d_1\times d_2}$, then
%	\begin{align*}
%	\johannes{\Big(\E \Big\|\sum_{k=1}^n {\XI}_k\Big\|^q \Big)^{1/q}} & \leq C_{q,d} \max\Big\{\Big\|\Big(\sum_{k=1}^n \mathbb{E}(\XI_k^T\XI_k)\Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}(\XI_k\XI_k^T)\Big)^{1/2}\Big\|,\\
%	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ 2C_{\frac{q}{2},d} \johannes{\Big(\E \max_{1\leq k\leq n} \|{\XI}_k\|^q\Big)^{1/q}\Big\},}
%	\end{align*}
%	where $d=\max\{d_1,d_2\}$ and
%	$C_{q,d}\leq  2^{\frac{3}{2}} e (1 + \sqrt{2}) \sqrt{\max\{q,\log d\}}$.  Moreover, the reverse inequality holds with universal constants. 
%\end{theorem}
We will derive Theorem~\ref{thm:matrixBRintro} by combining Theorem~\ref{thm:sumsRMs} with a general decoupling technique from \cite{KwW92}. Let
$(\Om,\cF,\bP)$ be a complete probability space, let
$(\cF_i)_{i\geq 0}$ be a filtration, and let $X$ be a Banach space. We denote the Borel $\sigma$-algebra on $X$ by $\mathcal{B}(X)$.
Two $(\cF_i)_{i\geq 1}$-adapted sequences $(\xi_i)_{i\geq 1}$ and
$(\theta_i)_{i\geq 1}$ of $X$-valued random variables are called
\emph{tangent} if for every $i\geq 1$ and $A \in \mathcal{B}(X)$
\begin{equation}
\label{eqn:tangent}
\bP(\xi_i \in A|\cF_{i-1}) = \bP(\theta_i \in A|\cF_{i-1}).
\end{equation}
An $(\cF_i)_{i\geq 1}$-adapted sequence $(\theta_i)_{i\geq 1}$ of $X$-valued random variables is said to satisfy \emph{condition (CI)} if, firstly, there exists a
sub-$\sigma$-algebra
$$
\cG\subset\cF_{\infty}=\sigma(\cup_{i\geq 0}\cF_i)
$$
such that for every $i\geq 1$ and $A \in \mathcal{B}(X)$,
\begin{equation}
\label{eqn:CI}
\bP(\theta_i \in A|\cF_{i-1}) = \bP(\theta_i \in A|\cG)
\end{equation}
and, secondly, $(\theta_i)_{i\geq 1}$ consists of $\cG$-independent random variables, i.e.\ for all $n\geq 1$ and $A_1,\ldots,A_n \in \mathcal{B}(X)$,
\begin{align}
\label{eq:G_independence}
    \E(\mathbf 1_{\theta_1 \in A_1}\cdot\ldots\cdot \mathbf 1_{\theta_n \in A_n}|\cG) = \E(\mathbf 1_{\theta_1 \in A_1}|\cG)\cdot\ldots\cdot\E(\mathbf 1_{\theta_n \in A_n}|\cG).
\end{align}
It is shown in \cite{KwW92} that for every $(\cF_i)_{i\geq
1}$-adapted sequence $(\xi_i)_{i\geq 1}$ there exists an
$(\cF_i)_{i\geq 1}$-adapted sequence $(\theta_i)_{i\geq 1}$ on a
possibly enlarged probability space which is tangent to
$(\xi_i)_{i\geq 1}$ and satisfies condition (CI). This sequence is
called a \emph{decoupled tangent sequence} for $(\xi_i)_{i\geq 1}$
and is unique in law.\par 
To prove Theorem \ref{thm:matrixBRintro}, we will set $(\xi_i)_{i\ge 1} = (\XI_i)_{i\ge 1}$ and apply
Theorem~\ref{thm:sumsRMs} conditionally to its decoupled
tangent sequence $(\theta_i)_{i\geq 1}$. For this approach to work, we
will need to relate various norms of $(\xi_i)_{i\geq 1}$ and
$(\theta_i)_{i\geq 1}$. One of these estimates can be formulated as a
Banach space property. A
Banach space $X$ satisfies \emph{the $q$-decoupling property}
for some $1\leq q<\infty$ if there is a constant $C_{q,X}$ such that for
any complete probability space $(\Om,\cF,\bP)$, any filtration
$(\cF_i)_{i\geq 0}$, and any $(\cF_{i})_{i\geq 1}$-adapted
sequence $(\xi_i)_{i\geq 1}$ in $L^q(\Om,X)$,
\begin{equation}
\label{eqn:decouple}
\Big(\E\Big\|\sum_{i=1}^n \xi_i\Big\|_X^q\Big)^{1/q} \leq C_{q,X} \Big(\E\Big\|\sum_{i=1}^n \theta_i\Big\|_X^q\Big)^{1/q},
\end{equation}
for all $n\geq 1$, where $(\theta_i)_{i\geq 1}$ is any decoupled
tangent sequence of $(\xi_i)_{i\geq 1}$. We will let $C^{\operatorname{(DEC)}}_{q,X}$ denote the best possible constant and call it the \emph{$q$-decoupling constant} of $X$. If $X$ is a UMD Banach space,
then one can also \emph{recouple}, meaning that for all
$1<q<\infty$ there is a constant $c_{q,X}$ such that for any
martingale difference sequence $(\xi_i)_{i\geq 1}$ and any
associated decoupled tangent sequence $(\theta_i)_{i\geq 1}$,
\begin{equation}
\label{eqn:recouple}
\Big(\E\Big\|\sum_{i=1}^n \theta_i\Big\|_X^q\Big)^{1/q} \leq c_{q,X} \Big(\E\Big\|\sum_{i=1}^n \xi_i\Big\|_X^q\Big)^{1/q}.
\end{equation}
Let us denote the best possible constant in this inequality by $C^{\operatorname{(REC)}}_{q,X}$ and call it the \emph{$q$-recoupling constant}. Conversely, if both (\ref{eqn:decouple}) and (\ref{eqn:recouple}) hold for some $1<q<\infty$, then $X$ must be a UMD space (in this case \eqref{eqn:decouple} and \eqref{eqn:recouple} hold for all choices of $q$). This equivalence is independently due to McConnell \cite{McC89} and Hitczenko \cite{HitUP}. In fact, it follows from their proof that $\max\{C^{\operatorname{(DEC)}}_{q,X},C^{\operatorname{(REC)}}_{q,X}\}\leq C^{\operatorname{(UMD)}}_{q,X}$ where $C^{\operatorname{(UMD)}}_{q,X}$ is the $q$-UMD constant of $X$. Finally, we will use that for any $1<r\leq q$, $C^{\operatorname{(DEC)}}_{q,X}\lesssim \frac{q}{r}C^{\operatorname{(DEC)}}_{r,X}$ (\cite[Theorem 4.1]{CoV11}) and $C^{\operatorname{(UMD)}}_{q,X}\lesssim \frac{q}{r}C^{\operatorname{(UMD)}}_{r,X}$ (\cite[Theorem 4.23]{HNV16}).\par  
With these tools we can now prove the main result of this section.
%\par
\begin{proof}[Proof of Theorem~\ref{thm:matrixBRintro}]
%\begin{theorem}
%	\label{thm:matrixBR} 
%	Let $\max\{2,\log n\}\leq q<\infty$. If $({\DELTA}_k)_{k=1}^n$ is a matrix martingale difference sequence in $\R^{d_1\times d_2}$, then
%	\begin{align*}
%	\Big(\E \Big\|\sum_{k=1}^n {\DELTA}_k\Big\|^q \Big)^{1/q} & \leq \beta_{q,d}\alpha_{q,d} \max\Big\{\Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\DELTA_k^T\DELTA_k)\Big)^{1/2}\Big\|^q\Big)^{1/q}, \Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\DELTA_k\DELTA_k^T)\Big)^{1/2}\Big\|^q\Big)^{1/q},\\
%	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ 2\alpha_{\frac{q}{2},d} \max_{1\leq k\leq n} (\E\|{\DELTA}_k\|^q)^{1/q}\Big\}.
%	\end{align*}
%	Moreover, $\beta_{q,d}\lesssim q$ if $q\geq \log d$.\par
%	Conversely, for any $2\leq q<\infty$, 
%		\begin{align*}
%	\gamma_{q,d} \Big(\E \Big\|\sum_{k=1}^n {\DELTA}_k\Big\|^q \Big)^{1/q} & \gtrsim \max\Big\{\Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\DELTA_k^T\DELTA_k)\Big)^{1/2}\Big\|^q\Big)^{1/q}, \Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\DELTA_k\DELTA_k^T)\Big)^{1/2}\Big\|^q\Big)^{1/q},\\
%	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ \max_{1\leq k\leq n} (\E\|{\DELTA}_k\|^q)^{1/q}\Big\}.
%	\end{align*}
%\end{theorem}
%\begin{proof}
Let $C^{\operatorname{(DEC)}}_{q,p}$ and $C^{\operatorname{(REC)}}_{q,p}$ denote the $q$-decoupling and $q$-recoupling constants of the Banach space of $(\R^{p\times p},\|\cdot\|)$, which is finite dimensional and hence a UMD space. Moreover, we write $\E_{\cG} = \E(\cdot|\cG)$ for brevity. Let $({\THETA}_k)$ be the decoupled tangent sequence for the martingale difference sequence $(\XI_k)$. By the $q$-decoupling inequality,
\begin{align*}
\Big(\E\Big\|\sum_k \XI_k\Big\|^q\Big)^{1/q} & \leq C^{\operatorname{(DEC)}}_{q,p} \Big(\E\Big\|\sum_k {\THETA}_k\Big\|^q\Big)^{1/q}.
\end{align*}
Since the summands ${\THETA}_k$ are $\cG$-conditionally independent by \eqref{eq:G_independence} and $\cG$-mean zero by \eqref{eqn:tangent}, \eqref{eqn:CI}, and the fact that $(\XI_k)$ forms a martingale difference sequence, we can apply Theorem~\ref{thm:sumsRMs} conditionally to find, a.s.,
	\begin{align}
	\label{est:decoupledLqnorm}
	\Big(\E_{\cG}\Big\|\sum_{k=1}^n {\THETA}_k\Big\|^q \Big)^{1/q} & \leq C_{q,p}^{\operatorname{(MR)}} \max\Big\{\Big\|\Big(\sum_{k=1}^n \E_{\cG}(\THETA_k^T\THETA_k)\Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \E_{\cG}(\THETA_k\THETA_k^T)\Big)^{1/2}\Big\|,\nonumber\\
	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\qquad \ \ \ \ \ 2C_{\frac{q}{2},p}^{\operatorname{(MR)}}\Big(\E_{\cG} \max_{1\leq k\leq n} \|{\THETA}_k\|^q\Big)^{1/q}\Big\}\nonumber\\
	& \leq C_{q,p}^{\operatorname{(MR)}} \max\Big\{\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k)\Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T)\Big)^{1/2}\Big\|,\nonumber\\
	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad \qquad \ \ \ \ \ 2C_{\frac{q}{2},p}^{\operatorname{(MR)}} \Big( \max_{1\leq k\leq n} \E_{k-1}\|{\XI}_k\|^q\Big)^{1/q}\Big\},
	\end{align}
where in the last step we used \eqref{eq:Lq_maxBound} (together with the assumption $q\geq \log(n)$) and moreover used that by the properties \eqref{eqn:CI} and \eqref{eqn:tangent} of a decoupled tangent sequence, 
$$\E_{\cG} (\THETA_k^T\THETA_k)  = \E_{k-1} (\XI_k^T\XI_k)$$
and 
$$\E_{\cG}\|{\THETA}_k\|^q = \E_{k-1}\|{\XI}_k\|^q.$$
%\noteJ{In the last equation max and expectation swap position. Shouln't there appear a $\log(n)$ dependence? }
%	\begin{align*}
%	\Big(\E_{\cG}\Big\|\sum_{k=1}^n {\XI}_k\Big\|^q \Big)^{1/q} & \leq C_{q,d} \max\Big\{\Big\|\Big(\sum_{k=1}^n \E_{\cG}(\XI_k^T\XI_k)\Big)^{1/2}\Big\|, \Big\|\Big(\sum_{k=1}^n \E_{\cG}(\XI_k\XI_k^T)\Big)^{1/2}\Big\|,\\
%	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ 2C_{\frac{q}{2},d}\Big(\E_{\cG} \max_{1\leq k\leq n} \|{\XI}_k\|^q\Big)^{1/q}\Big\}\\
%	& \leq \alpha_{q,d} \max\Big\{\Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\DELTA_k^T\DELTA_k)\Big)^{1/2}\Big\|^q\Big)^{1/q}, \Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\DELTA_k\DELTA_k^T)\Big)^{1/2}\Big\|^q\Big)^{1/q},\\
%	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ 2\alpha_{\frac{q}{2},d} \Big( \max_{1\leq k\leq n} \E_{k-1}\|{\DELTA}_k\|^q\Big)^{1/q}\Big\}.
%	\end{align*}
%where we have used that by the properties \eqref{eqn:CI} and \eqref{eqn:tangent} of a decoupled tangent sequence, 
%= \E_{k-1} \johannesnew{(\XI_k^T\XI_k)}$$
%$$\E_{\cG}|{\XI}_k|^2 = \E_{k-1}|{\XI}_k|^2 = \E_{k-1}|{\DELTA}_k|^2$$
Now take $L^q$-norms on both sides of \eqref{est:decoupledLqnorm}, apply the triangle inequality, and again use \eqref{eq:Lq_maxBound} to obtain the claimed estimate.\par
To estimate $C^{\operatorname{(DEC)}}_{q,p}$, recall that the operator norm on $\R^{p\times p}$ is equivalent to the Schatten $r$-norm for $r:=\log p$ up to absolute constants. Since $q\geq \log(p)\geq 1$, 
$$C^{\operatorname{(DEC)}}_{q,p}\simeq C^{\operatorname{(DEC)}}_{q,S^{\log(p)}} \lesssim \frac{q}{\log(p)}  C^{\operatorname{(DEC)}}_{\log(p),S^{\log(p)}} \leq \frac{q}{\log(p)} C^{\operatorname{(UMD)}}_{\log(p),S^{\log(p)}}$$
Since $C^{\operatorname{(UMD)}}_{r,S^r}\lesssim r$ for any $2\leq r<\infty$ (see \cite[Corollary 4.5]{Ran02}), 
%\red{JM: Reference missing}
we find $C^{\operatorname{(DEC)}}_{q,p}\lesssim q$ whenever $q\geq \log(p)$. This proves \eqref{eqn:matrixBRintroUpper}.\par
%\footnote{\todo{It may be suboptimal to pass through the UMD property - for instance, it is known that the $q$-decoupling constant of $\R$ is $q$-independent whereas the $q$-UMD constant is equal to $q$ for $q\geq 2$.}}\par
Proceeding analogously using the lower bound in Theorem~\ref{thm:sumsRMs}, we find 
		\begin{align*}
	C^{\operatorname{(REC)}}_{q,p} \Big(\E \Big\|\sum_{k=1}^n {\XI}_k\Big\|^q \Big)^{1/q} & \gtrsim \max\Big\{\Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k^T\XI_k)\Big)^{1/2}\Big\|^q\Big)^{1/q}, \Big(\E\Big\|\Big(\sum_{k=1}^n \mathbb{E}_{k-1}(\XI_k\XI_k^T)\Big)^{1/2}\Big\|^q\Big)^{1/q},\\
	& \qquad \qquad\qquad\qquad\qquad\qquad \qquad\ \ \ \ \ \max_{1\leq k\leq n} (\E\|{\XI}_k\|^q)^{1/q}\Big\}.
	\end{align*}
%\noteJ{Don't we need to state a respective lower bound in Theorem \ref{thm:sumsRMs} to obtain the above?}
Since 
$$C^{\operatorname{(REC)}}_{q,p}\simeq C^{\operatorname{(REC)}}_{q,S^{\log(p)}}\leq  C^{\operatorname{(UMD)}}_{q,S^{\log(p)}} \lesssim \frac{q}{\log(p)} C^{\operatorname{(UMD)}}_{\log (p),S^{\log(p)}} \lesssim q,$$
our proof is complete.    
%Since $\gamma_{q,d}\leq C^{\operatorname{(UMD)}}_{p,S^{\log d}}$  \cite{HNV16}[Theorem 4.23]
\end{proof}

%\section{Conclusion}
%\label{sec:Conclusion}
%
%In the present work, we proposed and analyzed a data adaptive method for estimating covariance matrices from coarsely quantized samples. Surprisingly the derived error guarantees match optimal results obtained for a related method that relies on hyperparameter tuning under oracle knowledge. In particular, the present results only suffer a slightly decreased probability of success when compared to the previous work.

\section*{Acknowledgements}

The authors were supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) through the project CoCoMIMO funded within the priority program SPP 1798 Compressed Sensing in Information Processing (COSIP).

\begin{comment}
We first estimate the bias of $\tilde{\SIGMA}'_n$, see  Lemma~\ref{lem:linftyBiasEst}. Its proof relies on Lemmas~\ref{lem:Expectation} and \ref{lem:biasSignProd}. The remainder of the proof is a relatively straightforward application of the matrix Bernstein inequality:
\begin{lemma} \label{lem:Expectation}
	Let $a,b\in \R$, let $\lambda>\max\{|a|,|b|\}$ and let $\sigma,\sigma'$ be independent and uniformly distributed in $[-\lambda,\lambda]$. Then, 
	$$\johannes{\E (\sign(a+\sigma)\sign(b+\sigma'))}{} = \frac{ab}{\lambda^2}.$$
\end{lemma}{}
In the proof of the following lemma we use that for any subgaussian random variable $W$ and any $\mu>0$,
\begin{align}
\label{eqn:truncEstSubg}
\johannes{\E\round{|W|1_{\{|W|>\mu\}}}}{} & = \int_0^{\infty} \P{|W|1_{\{|W|>\mu\}}>t} \ dt 
= \mu \P{|W|>\mu} + \int_{\mu}^{\infty} \P{|W|>t} \ dt \nonumber \\
& \lesssim \mu e^{-c\mu^2/\|W\|_{\psi_2}^2} + \frac{\|W\|_{\psi_2}^2}{\mu}e^{-c\mu^2/\|W\|_{\psi_2}^2},
\end{align}
\sjoerd{where the final inequality follows from the well-known estimate
	$$\int_u^{\infty} e^{-t^2/2} \ dt \leq \frac{1}{u}e^{-u^2/2}, \qquad \sjoerdgreen{u>0}.$$
	Similarly,} for any subexponential random variable $Z$, 
\begin{align}
\label{eqn:truncEstSubexp}
\johannes{\E\round{ |Z|1_{\{|Z|>\mu\}} } }{}
\lesssim \mu e^{-c\mu/\|Z\|_{\psi_1}} + \|Z\|_{\psi_1}e^{-c\mu/\|Z\|_{\psi_1}}.
\end{align}
\begin{lemma} \label{lem:biasSignProd}
	Let $U,V$ be subgaussian random variables, let $\lambda>0$, and \sjoerd{let} $\sigma,\sigma'$ be independent, uniformly distributed in $[-\lambda,\lambda]$ and independent of $U$ and $V$. Then, 
	$$\left| \johannes{\E\round{\lambda^2\sign(U + \sigma) \sign(V + \sigma')} - \E\round{UV}}{} \right|
	\lesssim
	(\lambda^2+\theta_{U,V}^2)e^{-c\lambda^2/\theta_{U,V}^2},$$
	where $\theta_{U,V} = \max\{\|U\|_{\psi_2},\|V\|_{\psi_2}\}$.
\end{lemma}
\begin{proof}
	Since $U$ and $V$ are independent of $\sigma$ and $\sigma'$, Lemma~\ref{lem:Expectation} yields
	$$\johannes{\E_{\sigma,\sigma'}\round{\lambda^2\sign(U + \sigma) \sign(V + \sigma')}}{} 1_{\{|U|\leq \lambda, |V|\leq \lambda\}} = UV 1_{\{|U|\leq \lambda, |V|\leq \lambda\}}.$$
	Hence,
	\begin{align} \label{eq:Claim}
	&\johannes{\E\round{\lambda^2\sign(U + \sigma) \sign(V + \sigma')} - \E(UV)}{} \\
	&\quad = \johannes{\E\round{(\lambda^2\sign(U + \sigma)\sign(V + \sigma')-UV)1_{\{|U|>\lambda\}\cup\{|V|>\lambda\}}}}{}. \notag
	\end{align}
	Since $U,V$ are subgaussian, 
	\begin{align} \label{eq:Part1}
	\left| \johannes{\E\round{\lambda^2\sign(U + \sigma)\sign(V + \sigma')1_{\{|U|>\lambda\}\cup\{|V|>\lambda\}}}}{} \right| 
	&\leq 
	\lambda^2(\mathbb{P}[|U|>\lambda]+\mathbb{P}[|V|>\lambda] ) \\
	&\leq 2\lambda^2 e^{-c\lambda^2/\theta_{U,V}^2}. \notag
	\end{align}
	Moreover, 
	\begin{align}
	\left| \johannes{\E\round{UV1_{\{|U|>\lambda\}\cup\{|V|>\lambda\}}}}{} \right|
	\leq 
	\johannes{\E\round{|UV| 1_{\{|U|>\lambda\}}} + \E\round{|UV| 1_{\{|V|>\lambda\}}}}{}.
	\end{align}
	By \eqref{eqn:truncEstSubg} and \eqref{eqn:truncEstSubexp}
	\begin{align} 
	& \johannes{\E\round{|UV|1_{\{|U|>\lambda\}}} }{}\notag\\
	&\qquad = \johannes{\E\round{|UV| (1_{\{|U|>\lambda, |V|>\lambda\}} + 1_{\{|U|>\lambda, |V|\leq\lambda\}})}}{} \notag \\
	&\qquad \leq \johannes{\E\round{|UV|1_{\{|UV|>\lambda^2\}}} + \lambda \E\round{|U|1_{\{|U|>\lambda\}}}}{}\notag\\
	&\qquad \leq (\lambda^2+\|UV\|_{\psi_1})e^{-c\lambda^2/\|UV\|_{\psi_1}} + \lambda\Big(\lambda e^{-c\lambda^2/\|U\|_{\psi_2}^2} + \frac{\|U\|_{\psi_2}^2}{\lambda}e^{-c\lambda^2/\|U\|_{\psi_2}^2}\Big) \notag\\
	&\qquad \leq 2(\lambda^2+\theta_{U,V}^2)e^{-c\lambda^2/\theta_{U,V}^2},\label{eq:Part2}
	\end{align}
	where we have used $\|UV\|_{\psi_1}\leq \|U\|_{\psi_2}\|V\|_{\psi_2}$. The claim follows \sjoerd{by} using \eqref{eq:Part1}-\eqref{eq:Part2} in \eqref{eq:Claim}.
\end{proof}
\begin{proof}
	Since $\X$ is $K$-subgaussian, for any $\ell\in[p]$,
	$$\|X_{\ell}\|_{\psi_2} = \|\langle \X,\mathbf{e}_{\ell}\rangle\|_{\psi_2} \leq K \johannes{(\mathbb{E}\langle \X,\mathbf{e}_{\ell}\rangle^2)^{1/2}}{} = K\SIGMA_{\ell\ell}^{1/2}\leq K\|\SIGMA\|_{\infty}^{1/2}.$$
	Lemma~\ref{lem:biasSignProd} applied for $U=X_i$ and $V=X_j$ yields 
	$$\abs{ \johannes{\E\round{ \lambda^2 \sign(X_i + \tau_i) \sign(X_j + \bar\tau_j)}}{} - \Sigma_{i,j} }{} \lesssim (\lambda^2+\sjoerdgreen{K^2}\|\SIGMA\|_{\infty})e^{-c_1\lambda^2/\sjoerdgreen{K^2}\|\SIGMA\|_{\infty}}$$ 
	for all $i,j\in [p]$ and  
	$$\abs{ \johannes{\E\round{ \lambda^2 \sign(X_i + \tau_i) \sign(X_j + \tau_j)}}{} - \Sigma_{i,j} }{} \lesssim (\lambda^2+\sjoerdgreen{K^2}\|\SIGMA\|_{\infty})e^{-c_1\lambda^2/\sjoerdgreen{K^2}\|\SIGMA\|_{\infty}}$$
	whenever $i\neq j$. These two observations immediately imply the two statements. 
\end{proof}
\end{comment}



\bibliography{mybib}{}
\bibliographystyle{plain}

\end{document}
