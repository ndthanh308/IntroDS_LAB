\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{brown2020gpt}
Brown, T.~B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.;
  Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.;
  Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
  D.~M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
  S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
  I.; and Amodei, D. 2020.
\newblock Language Models are Few-Shot Learners.
\newblock arXiv:2005.14165.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.;
  Zhuang, S.; Zhuang, Y.; Gonzalez, J.~E.; Stoica, I.; and Xing, E.~P. 2023.
\newblock Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT
  Quality.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 4171--4186. Minneapolis,
  Minnesota: Association for Computational Linguistics.

\bibitem[{Dolan and Brockett(2005)}]{dolan-brockett-2005-MRPC}
Dolan, W.~B.; and Brockett, C. 2005.
\newblock Automatically Constructing a Corpus of Sentential Paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing ({IWP}2005)}.

\bibitem[{Du et~al.(2022)Du, Qian, Liu, Ding, Qiu, Yang, and Tang}]{du2022glm}
Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2022.
\newblock GLM: General Language Model Pretraining with Autoregressive Blank
  Infilling.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 320--335.

\bibitem[{Goyal, Li, and Durrett(2023)}]{goyal2023gpt-sum}
Goyal, T.; Li, J.~J.; and Durrett, G. 2023.
\newblock News Summarization and Evaluation in the Era of GPT-3.
\newblock arXiv:2209.12356.

\bibitem[{Gu et~al.(2023)Gu, Zhu, Ye, Zhang, Wang, Jiang, Xiong, Li, He, Xu,
  Huang, Wang, Wang, Zheng, Feng, and Xiao}]{gu2023xiezhi}
Gu, Z.; Zhu, X.; Ye, H.; Zhang, L.; Wang, J.; Jiang, S.; Xiong, Z.; Li, Z.; He,
  Q.; Xu, R.; Huang, W.; Wang, Z.; Wang, S.; Zheng, W.; Feng, H.; and Xiao, Y.
  2023.
\newblock Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge
  Evaluation.
\newblock arXiv:2306.05783.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt}]{Dan2020mmlu}
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and
  Steinhardt, J. 2020.
\newblock Measuring Massive Multitask Language Understanding.
\newblock \emph{CoRR}, abs/2009.03300.

\bibitem[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang,
  Lei, Fu, Sun, and He}]{huang2023ceval}
Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.;
  Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He, J. 2023.
\newblock C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for
  Foundation Models.
\newblock \emph{arXiv preprint arXiv:2305.08322}.

\bibitem[{Kasneci et~al.(2023)Kasneci, Seßler, Küchemann, Bannert,
  Dementieva, Fischer, Gasser, Groh, Günnemann, Hüllermeier, and
  et~al.}]{kasneci2023education_survey}
Kasneci, E.; Seßler, K.; Küchemann, S.; Bannert, M.; Dementieva, D.; Fischer,
  F.; Gasser, U.; Groh, G.; Günnemann, S.; Hüllermeier, E.; and et~al. 2023.
\newblock ChatGPT for Good? On Opportunities and Challenges of Large Language
  Models for Education.

\bibitem[{Lee~Rodgers and Nicewander(1988)}]{lee1988pearson}
Lee~Rodgers, J.; and Nicewander, W.~A. 1988.
\newblock Thirteen ways to look at the correlation coefficient.
\newblock \emph{The American Statistician}, 42(1): 59--66.

\bibitem[{Lewkowycz et~al.(2022)Lewkowycz, Slone, Andreassen, Freeman, Dyer,
  Mishra, Gur-Ari, Lee, Sohl-dickstein, Chiafullo, Fedus, Fiedel, Liu, Misra,
  and Ramasesh}]{Srivastava2022BIGbench}
Lewkowycz, A.; Slone, A.; Andreassen, A.; Freeman, D.; Dyer, E.~S.; Mishra, G.;
  Gur-Ari, G.; Lee, J.; Sohl-dickstein, J.; Chiafullo, K.; Fedus, L.~B.;
  Fiedel, N.; Liu, R.; Misra, V.; and Ramasesh, V.~V. 2022.
\newblock Beyond the Imitation Game: Quantifying and extrapolating the
  capabilities of language models.
\newblock Technical report.

\bibitem[{Li et~al.(2023)Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang,
  and Hashimoto}]{alpaca_eval}
Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin, C.; Liang,
  P.; and Hashimoto, T.~B. 2023.
\newblock AlpacaEval: An Automatic Evaluator of Instruction-following Models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}.

\bibitem[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, Newman, Yuan, Yan, Zhang, Cosgrove, Manning,
  Ré, Acosta-Navas, Hudson, Zelikman, Durmus, Ladhak, Rong, Ren, Yao, Wang,
  Santhanam, Orr, Zheng, Yuksekgonul, Suzgun, Kim, Guha, Chatterji, Khattab,
  Henderson, Huang, Chi, Xie, Santurkar, Ganguli, Hashimoto, Icard, Zhang,
  Chaudhary, Wang, Li, Mai, Zhang, and Koreeda}]{liang2022helm}
Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang,
  Y.; Narayanan, D.; Wu, Y.; Kumar, A.; Newman, B.; Yuan, B.; Yan, B.; Zhang,
  C.; Cosgrove, C.; Manning, C.~D.; Ré, C.; Acosta-Navas, D.; Hudson, D.~A.;
  Zelikman, E.; Durmus, E.; Ladhak, F.; Rong, F.; Ren, H.; Yao, H.; Wang, J.;
  Santhanam, K.; Orr, L.; Zheng, L.; Yuksekgonul, M.; Suzgun, M.; Kim, N.;
  Guha, N.; Chatterji, N.; Khattab, O.; Henderson, P.; Huang, Q.; Chi, R.; Xie,
  S.~M.; Santurkar, S.; Ganguli, S.; Hashimoto, T.; Icard, T.; Zhang, T.;
  Chaudhary, V.; Wang, W.; Li, X.; Mai, Y.; Zhang, Y.; and Koreeda, Y. 2022.
\newblock Holistic Evaluation of Language Models.
\newblock arXiv:2211.09110.

\bibitem[{Nov, Singh, and Mann(2023)}]{nov2023chatgpt_medical}
Nov, O.; Singh, N.; and Mann, D. 2023.
\newblock Putting ChatGPT's Medical Advice to the (Turing) Test.
\newblock arXiv:2301.10035.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock GPT-4 Technical Report.
\newblock arXiv:2303.08774.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{ouyang2022RLHF}
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.~L.; Mishkin, P.;
  Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton,
  F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P.; Leike,
  J.; and Lowe, R. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock arXiv:2203.02155.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar2016squad}
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}.

\bibitem[{Sallam(2023)}]{sallam2023chatgpt_in_healthcare}
Sallam, M. 2023.
\newblock ChatGPT utility in healthcare education, research, and practice:
  systematic review on the promising perspectives and valid concerns.
\newblock In \emph{Healthcare}, volume~11, 887. MDPI.

\bibitem[{Sarlin et~al.(2019)Sarlin, DeTone, Malisiewicz, and
  Rabinovich}]{paul2019superglue}
Sarlin, P.; DeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2019.
\newblock SuperGlue: Learning Feature Matching with Graph Neural Networks.
\newblock \emph{CoRR}, abs/1911.11763.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts}]{socher2013SST}
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C.~D.; Ng, A.~Y.; and
  Potts, C. 2013.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, 1631--1642.

\bibitem[{Spearman(1987)}]{spearman1987spearman}
Spearman, C. 1987.
\newblock The Proof and Measurement of Association between Two Things.
\newblock \emph{The American Journal of Psychology}, 100(3/4): 441--471.

\bibitem[{Sun et~al.(2023)Sun, Zhang, He, Li, Cheng, Yan, Liu, Shao, Tang,
  Zhao, Chen, Zheng, Zhou, Li, Zhan, Zhou, Li, Yang, Wu, Yin, Huang, and
  Qiu}]{sun2023moss}
Sun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu, X.; Shao, Y.;
  Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y.; Zhou, Z.; Li, R.; Zhan, J.; Zhou,
  Y.; Li, L.; Yang, X.; Wu, L.; Yin, Z.; Huang, X.; and Qiu, X. 2023.
\newblock MOSS: Training Conversational Language Models from Synthetic Data.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample}]{touvron2023llama}
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix,
  T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin,
  A.; Grave, E.; and Lample, G. 2023.
\newblock LLaMA: Open and Efficient Foundation Language Models.
\newblock arXiv:2302.13971.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang-etal-2018-glue}
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018.
\newblock {GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, 353--355. Brussels,
  Belgium: Association for Computational Linguistics.

\bibitem[{Wang et~al.(2022)Wang, Zhang, Zhang, Yang, Gao, Wu, Dong, He, Zhuo,
  Yang, Huang, Li, Wu, Lu, Zhu, Chen, Han, Pan, Wang, Wang, Wu, Zeng, Chen,
  Gan, and Zhang}]{fengshenbang}
Wang, J.; Zhang, Y.; Zhang, L.; Yang, P.; Gao, X.; Wu, Z.; Dong, X.; He, J.;
  Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.; Wu, Y.; Lu, J.; Zhu, X.; Chen, W.;
  Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.; Zeng, Z.; Chen, C.; Gan, R.;
  and Zhang, J. 2022.
\newblock Fengshenbang 1.0: Being the Foundation of Chinese Cognitive
  Intelligence.
\newblock \emph{CoRR}, abs/2209.02970.

\bibitem[{Williams, Nangia, and Bowman(2018)}]{Williams2018MNLI}
Williams, A.; Nangia, N.; and Bowman, S. 2018.
\newblock A Broad-Coverage Challenge Corpus for Sentence Understanding through
  Inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, 1112--1122. Association for
  Computational Linguistics.

\bibitem[{Xu et~al.(2020)Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian,
  Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou,
  Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and Lan}]{xu-etal-2020-clue}
Xu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y.; Xu, Y.; Sun, K.; Yu, D.;
  Yu, C.; Tian, Y.; Dong, Q.; Liu, W.; Shi, B.; Cui, Y.; Li, J.; Zeng, J.;
  Wang, R.; Xie, W.; Li, Y.; Patterson, Y.; Tian, Z.; Zhang, Y.; Zhou, H.; Liu,
  S.; Zhao, Z.; Zhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and
  Lan, Z. 2020.
\newblock {CLUE}: A {C}hinese Language Understanding Evaluation Benchmark.
\newblock In \emph{Proceedings of the 28th International Conference on
  Computational Linguistics}, 4762--4772. Barcelona, Spain (Online):
  International Committee on Computational Linguistics.

\bibitem[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia
  et~al.}]{zeng2022glm}
Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.;
  Zheng, W.; Xia, X.; et~al. 2022.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}.

\bibitem[{Zeng(2023)}]{zeng2023mmcu}
Zeng, H. 2023.
\newblock Measuring Massive Multitask Chinese Understanding.
\newblock arXiv:2304.12986.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer}]{zhang2022opt}
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.;
  Diab, M.; Li, X.; Lin, X.~V.; Mihaylov, T.; Ott, M.; Shleifer, S.; Shuster,
  K.; Simig, D.; Koura, P.~S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L. 2022.
\newblock OPT: Open Pre-trained Transformer Language Models.
\newblock arXiv:2205.01068.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, Zhang, Gonzalez, and Stoica}]{zheng2023llm-judge}
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.;
  Li, Z.; Li, D.; Xing, E.~P.; Zhang, H.; Gonzalez, J.~E.; and Stoica, I. 2023.
\newblock Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.
\newblock arXiv:2306.05685.

\bibitem[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and
  Duan}]{Zhong2023AGIEval}
Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.; Saied, A. S.~S.;
  Chen, W.; and Duan, N. 2023.
\newblock AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.
\newblock \emph{ArXiv}, abs/2304.06364.

\bibitem[{Zhuang et~al.(2021)Zhuang, Wayne, Ya, and
  Jun}]{zhuang-etal-2021-roberta}
Zhuang, L.; Wayne, L.; Ya, S.; and Jun, Z. 2021.
\newblock A Robustly Optimized {BERT} Pre-training Approach with Post-training.
\newblock In \emph{Proceedings of the 20th Chinese National Conference on
  Computational Linguistics}, 1218--1227. Huhhot, China: Chinese Information
  Processing Society of China.

\end{thebibliography}
