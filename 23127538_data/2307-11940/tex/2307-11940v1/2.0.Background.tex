\section{Background}
\label{sec:back}


\subsection{Safety-Critical System Development Process}

Domain specific safety standards describe the development process to be followed for safety-critical systems. Examples of such standards include ISO26262~\cite{ISO26262} for the automotive domain, EN5012x~\cite{EN50126,EN50128,EN50129} for the railway domain, and IEC61508~\cite{IEC61508} for industrial systems. Those development processes follow a ``V'' model (see Figure~\ref{fig:devproc}). First, the safety goals of the system need being specified. Safety requirements are obtained out of those safety goals. Then, the architecture of the system is devised ensuring that all safety requirements are mapped to specific items that will have to fulfill them. 

Safety-critical systems are designed to be correct by construction, hence meaning that both, hardware and software, are error free. This is achieved by following specific rules and processes when architecting the system and implementing it. However, random hardware faults due to radiation, sporadic deadline violations, voltage fluctuations, and the like cannot be fully avoided. Hence, safety measures must be included in the system design to manage errors emanating from those faults (e.g., tolerating them, or detecting them and reaching a safe state timely). Once the system has been architected, its different components are implemented adhering to specific constraints dictated by the standards (e.g., avoiding unobvious control flow in the software).

Verification activities for the system architecture and implementation must be included in the process as a way to assess that the design adheres to its requirements and specifications. Controllability means are generally needed along with the design and verification activities to avoid problematic behavior during operation and to enforce specific behavior for verification purposes.

Validation activities (the right part of the ``V'') include testing activities for individual components, as well as for the subsequent integrations with the goal of spotting design errors and, in their absence, gain confidence on the safety of the system. Those testing activities must be as little intrusive as possible to observe the system in operation conditions. Using observability means allow provision of system information without altering the behavior of a system.

Overall, safety-critical systems need to include safety measures, as well as controllability and observability channels to ease their development in accordance with safety standards.

% Figure environment removed



\subsection{Relevant Concepts}

In this paper we build on a number of concepts with broad application across domains, but different names. For the sake of consistency, we resort to automotive naming only (i.e. from ISO26262). 

\textbf{Integrity levels}. There are multiple integrity levels that describe different levels of acceptable risk. They are referred to as Automotive Safety Integrity Levels (ASIL) in the context of automotive, with ASIL-D being the most stringent ASIL, and ASIL-A the least stringent yet with some safety requirements. Finally, an additional level named Quality Managed (QM) is used to refer to items with no safety requirement at all.

\textbf{Dual (DMR) and Triple Modular Redundancy (TMR)}. Systems with the highest integrity levels are often realized building on DMR or TMR for error detection and/or correction. In the particular case of automotive systems, ASIL-D components are generally deployed on Dual-Core Lockstep (DCLS) CPUs. DCLS is an efficient implementation of time and space redundancy for computing cores where the same software runs on two identical cores, but with some staggering (time shift) among them so that a fault affecting both cores simultaneously would cause different errors -- if any -- due to their different state. Hence, errors are detectable by means of simple comparison since redundancy is \emph{diverse}. {\color{black}The potential sources for faults affecting redundant cores, as well as their impact, have been carefully analyzed~\cite{staggeringCauses}, and caveats to deal with the impact of such faults provided~\cite{staggeringCaveats}.}


\textbf{Safe state}. Many systems have a safe state, i.e. a state that, whenever reached, guarantees system safety. For instance, the safe state could consist of a successful transfer of the control to the driver in the case of partially autonomous cars, or stopping the car in a safe location. Reaching such safe state may imply that the corresponding safety system is no longer working. Hence, the safe state may affect the availability of the faulty component or the overall system. {\color{black}Systems lacking a safe state are often referred to as fail-operational systems, whereas those with a safe state are referred to as fail-safe systems.
Note that fault tolerance (for fail-operational systems) and safe states (for fail-safe systems) are intended to be achieved at a given system level, but failures may be allowed at lower levels. For instance, a job of the task controlling the braking system may fail due to a soft error, so that we have a failure at the scope of such microcontroller. However, at a higher level we may have multiple redundant units and a voting system to achieve fault tolerance, or mechanisms to detect the error and potentially enforce a lower driving speed (i.e. a potential safe state) if such fault occurs too often and challenges the timeliness of the braking system.}

\textbf{Fault Tolerant Time Interval (FTTI)}. From the time a fault occurs until it is properly controlled, there is a maximum time affordable in which no hazard can occur. That time interval is referred to as FTTI in automotive, and determines the time needed to recover normal operation, or to reach a safe state where, despite availability may be harmed, safety is preserved.



\subsection{Related Work}

This paper presents a safety island explicitly conceived to provide advanced safety services to an HPC device. However, other works such as that by Siemens~\cite{SiemensSI} already envision their own safety island, and some HPC platforms targeting mainly the automotive domain, such as the Intel Go platform~\cite{IntelGo} and the NVIDIA DRIVE AGX Orin~\cite{NVIDIAdrive,NVIDIAdrive2} already include an automotive ASIL-D compliant microcontroller as a form of ``safety island''. 
In the case of Siemens, their safety island~\cite{SiemensSI} aims at providing a safe enclave for execution and post-mortem test capabilities for the HPC device but, to our knowledge, it lacks advanced observability and controllability features, as well as safety measures, intended to preserve safe operation in the HPC device despite errors, as opposed to our concept.
In the case of the HPC platforms with ASIL-D compliant microcontrollers, such automotive microcontroller is an Infineon AURIX processor in both cases, which is primarily intended to operate as a standalone microcontroller. However, if configured properly, it can provide some of the services provided by the safety island described in this paper, potentially with lower efficiency due to the lack of explicit hardware support for some features, such as multicore interference monitoring and diverse redundancy support for the cores in the HPC device.

For the realm of IoT, the safety island on Intel's Atom\textsuperscript{\textregistered} x6000FE Series~\cite{ElkhartLake} enables functional safety (FuSa) capabilities that detect and attenuate a system fault before it causes or exacerbates further errors (if using the fault $\rightarrow$ error $\rightarrow$ failure sequence terms as described in\cite{avizienis2004basic}). Functionality includes fault monitoring and reporting, on-demand diagnostics measurements, watchdog timers, temperature monitoring, self-diagnostics of the safety island itself and encoding/decoding of communication protocols between the safety island and other external elements.

In the area of security, some Arm processors include the Arm TrustZone~\cite{TrustZone}, which is a form of highly-coupled security island providing a security enclave in Arm processors. Hence, despite with different purposes (security instead of safety), and with a particular degree of coupling (only for a highly-coupled implementation), Arm's TrustZone has some commonalities with the safety island.

