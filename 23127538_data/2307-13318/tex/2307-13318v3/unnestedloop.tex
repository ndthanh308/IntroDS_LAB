\section{Algorithmic Techniques for Affine Disjunctive Invariant Generation}
\label{sec:alg}
In this section, we formally present our method for generating affine disjunctive invariants over non-nested loops. Our method includes a control flow transformation that transforms a non-nested loop into an affine transition system, an invariant propagation technique that optimizes the invariant generation process, and the tackling of the infeasible implication in the application of Farkas' Lemma to optimize the accuracy. 

\subsection{Control Flow Transformation}\label{sec:cft}

Throughout the section, we fix the set of program variables in a loop
as $X=\{x_1,\dots,x_n\}$ and identify it as the set of variables in the \LTS{} to be derived from the loop. We consider the canonical form of a non-nested affine while loop as in Figure~\ref{fig:CanonicalForm}, where we have: 
\begin{itemize}
\item The column vector $\mathbf{x}=(x_1,\dots,x_n)^{\mathrm{T}}$ represents the vector of program variables. 
\item $G$ is the loop condition (or loop guard) of the while loop that in general can be a PAP. 
\item The \textbf{case} keyword divides the loop into conditional branches so that if the current values of the program variables satisfy the condition $\phi_i$, then the assignment at the $i$th conditional branch (i.e., $\mathbf{x}:=\mathbf{F}_i(\mathbf{x})$) is executed. Note that we do not require the branch conditions $\phi_1,\dots,\phi_m$ need not to be logically pairwise disjoint.  
%(i.e., there can be some valuation $\tsEval$ that satisfies both $\phi_i,\phi_j$ ($i\ne j$)), 
%so that our setting covers nondeterminism in imperative programs. 
\item  Each $\mathbf{F}_i$ ($1\le i\le m$) is an affine function, i.e.,  $\mathbf{F}_i(\mathbf{x})=\mathbf{A} \mathbf{x}+\mathbf{b}$ where $\mathbf{A}$ (resp. $\mathbf{b}$) is an $n\times n$ square matrices (resp. $n$-dimensional column vector) that specifies the affine update under the function $\mathbf{F}_i$ in the conditional branch $\phi_i$. 
The assignment $\mathbf{x}:=\mathbf{F}_i(\mathbf{x})$ is considered simultaneously for the variables in $\mathbf{x}$ so that in one execution step, the current valuation $\tsEval$ is updated to $\mathbf{F}_i(\tsEval)$. 
\item The statements $\delta_1,\dots,\delta_m$ specify whether the loop continues after the affine update of the conditional branches $\phi_1,\dots,\phi_m$. Each statement $\delta_i$ is either the \textbf{skip} statement that does nothing (which means that the loop continues after the affine update of $\mathbf{F}_i$) or the \textbf{break} statement (which means that the loop exits after the affine update). 
\end{itemize}

\lstset{language=program}
\lstset{tabsize=3}
\newsavebox{\unnested}
\begin{lrbox}{\unnested}
\begin{lstlisting}[mathescape]
while $(G)$ {
  case $\phi_1$: $\mathbf{x}:=\mathbf{F}_1(\mathbf{x})$;$\delta_1$;
      $\vdots$ 
  case $\phi_m$: $\mathbf{x}:=\mathbf{F}_m(\mathbf{x})$;$\delta_m$;
}
\end{lstlisting}
\end{lrbox}

% Figure environment removed

Any non-nested affine while loop with break statement can be transformed into the canonical form in Figure~\ref{fig:CanonicalForm} by recursively examining the substructures of the loop body of the loop. 
%at the cost of a possible exponential blow-up in the number of conditional branches in the loop body. 
A detailed recursive transformation process is provided in 
%recursive algorithm that transforms an affine program $P$ with break statement and without nested loops into a program $\mathsf{C}_P$ 
%%(in our case, the loop body of an unnested affine while  loop) 
%that fits the loop body of the canonical form in Figure~\ref{fig:unnested}, is attached to the 
Appendix~\ref{appendix:transform}.
Note that although the transformation into our canonical form may cause exponential blow up in the number of conditional branches in the loop body, in practice a loop typically has a small number of conditional branches and further improvement can be carried out by removing invalid branches (i.e., those whose branch condition is unsatisfiable, such as $\tau_2$ in Figure ~\ref{fig:ExampleCodeAndATS}). Moreover, such a canonical form is often necessary to capture precise disjunctive information in a while loop.

Below we demonstrate our control flow transformation that transforms the canonical form  into an ATS. The intuition is that we treat each branch condition $\phi_i$ in the figure as a standalone location and the overall disjunctive invariant is the disjunction of the generated invariants at these locations. Moreover, the transitions are determined by the jumps between branch locations on whether the loop will enter a branch condition in the next loop iteration given the branch condition the loop currently resides in.
The detailed transformation is given as follows. 




%The canonical form in Figure~\ref{fig:unnested} coincides with the top-level branches~\cite{DBLP:conf/sigsoft/XieCLLL16} in the sense that each branch condition $\phi_i$ encodes the truth values of all the conditional branches in the original loop body, so that each $\phi_i$ corresponds to an execution of the whole loop body. 
%Note that we do not consider the \textbf{goto} statement since they would otherwise lead to infinite executions within the loop body, and thus \hongming{break} the loop structure.

 

%Below we illustrate our algorithm to generate disjunctive affine invariants on unnested affine while loops. Informally, our algorithm follows Farkas' Lemma for affine invariant generation as in \citet{DBLP:conf/cav/ColonSS03,DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable}, and further proposes the improvement of invariant generation that is closely related to the structure of the canonical form and has not been considered in the existing approaches~\cite{DBLP:conf/cav/ColonSS03,DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable}. Here we first consider an affine while loop $W$. The workflow of our algorithm is demonstrated as follows (\textbf{Step 1} -- \textbf{Step 3}). 

%\smallskip
%\noindent\textbf{Step 1.} We first abstract the loop $W$ into a canonical form $\mathsf{C}_W$ w.r.t Figure~\ref{fig:unnestedPQandRecursive} as stated previously. 

% Taking Figure~\ref{fig:ExampleCodeAndATS} as a running example, the canonical form of the example is given in Example~\ref{eg:transformedcode}. 

%\smallskip
%\noindent\textbf{Step 2.} Then we transform the loop $\mathsf{C}_W$ into a \LTS{}. The transformation is in a straightforward fashion that each branch (i.e., $\phi_i$ in Figure~\ref{fig:unnestedPQandRecursive}) corresponds to a stand-alone location, and the guard of a transition is determined by the loop condition (i.e., $G$) as well as the branch conditions of the source and target locations of the transition. 

Formally, the \LTS{} $\Gamma_W$ for a loop $W$ in our canonical form is constructed as follows:
\begin{itemize}
\item The set of locations is $\{\tsLoc_1,\dots, \tsLoc_m, \tsLoc_{e}\}$, 
%\times \{0,\dots, \kappa-1\}^n\cup\{\tsLoc_{e}\}$ 
where each $\tsLoc_i$ ($1\le i\le m$) corresponds to the branch condition $\phi_i$ and $\tsLoc_{e}$ is the termination location of the loop. 
\item For each $1\le i\le m$ such that $\delta_i=\mathbf{break}$, we have the transition (we denote $\mathbf{x}':=(x'_1,\dots,x'_n)^\mathrm{T}$)
$$
\tau_{i}=(\tsLoc_i, \tsLoc_e, G \wedge \phi_i \wedge  \mathbf{x}'=\mathbf{F}_i(\mathbf{x}))
$$
that specifies the one-step jump from the branch condition $\phi_i$ to the termination location $\tsLoc_e$, where the guard condition is a conjunction of the loop guard G (for staying in the loop at the current loop iteration), the branch condition $\phi_i$ (that the current execution of the loop body resides in $\phi_i$) and $\mathbf{x}'=\mathbf{F}_i(\mathbf{x})$ (for the affine update). 
%and (ii) we also consider the value of the variables modulo a natural number $\kappa\ge 1$. (The choice of $\kappa=1$ means the ignorance of modulus information.)    
\item For each $1\le i,j\le m$ such that $\delta_i\neq \mathbf{break}$, we have the transition 
$$
\tau_{ij}=(\tsLoc_i, \tsLoc_j, G \wedge \phi_i \wedge G[\mathbf{x}'/\mathbf{x}] \wedge \phi_j[\mathbf{x}'/\mathbf{x}] \wedge \mathbf{x}'=\mathbf{F}_i(\mathbf{x}))
$$ 
that specifies the jump from the branch condition $\phi_i$ in the current loop iteration to the branch condition $\phi_j$ in the next loop iteration, for which the guard condition is $G \wedge \phi_i \wedge G[\mathbf{x}'/\mathbf{x}] \wedge \phi_j[\mathbf{x}'/\mathbf{x}]\wedge \mathbf{x}'=\mathbf{F}_i(\mathbf{x})$ since the transition needs to pass the loop guard $G$, satisfy the branch condition $\phi_i$ when staying in the location $\tsLoc_i$, have the affine update specified by $\mathbf{F}_i$ and fulfill the loop guard $G[\mathbf{x}'/\mathbf{x}]$ and the branch condition $\phi_j$ upon entering the location $\tsLoc_j$ in the next loop iteration. 
\item For each $1\le i\le m$ such that $\delta_i\neq \mathbf{break}$, we have the transition 
$$
\tau'_{i}=(\tsLoc_i, \tsLoc_{e}, G\wedge \phi_i\wedge (\neg G)[\mathbf{x}'/\mathbf{x}]\wedge \mathbf{x}'=\mathbf{F}_i(\mathbf{x}))
$$ 
%that specifies 
for the jump from the branch condition $\phi_i$ to the termination location $\tsLoc_e$ for which the guard condition is a conjunction of the loop guard $G$, the branch condition  $\phi_i$, the affine update $\mathbf{x}'=\mathbf{F}_i(\mathbf{x})$ and the negation of the loop guard (for jumping out of the loop). 
\end{itemize} 

After the transformation, we remove transitions with unsatisfiable guard condition to reduce the size of the derived \LTS{}. The transformation for our running example has been given in Example~\ref{eg:runningtransitions}. 
%\smallskip
%\noindent\textbf{Step 3.} 
With the transformed \LTS{} $\Gamma_W$, we follow existing approaches~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable} to  generate affine conjunctive invariants at the locations of the ATS $\Gamma_W$. In particular, we apply the recent approach~\cite{oopsla22/scalable} that has the most scalability. 
Finally, recall that the overall disjunctive invariant for the ATS $\Gamma_W$ is the disjunction of the invariants at all the locations. 

%(see Example ~\ref{eg:farkasapplication} for the running example). A slight difference is that we do not encode the constraints for the termination program location $\tsLoc_{e}$. This is because the invariant at $\tsLoc_{e}$ can be derived %by the invariant propagation technique that propagates the invariants 
%from non-termination locations to the termination location. In the following, we further propose an invariant propagation technique that takes advantage of a common feature in the directed graph of transformed \LTS{} to improve time efficiency. 

\subsection{Invariant Propagation}
% In our invariant propagation, we explore a special structure in the derived \LTS{} that often arises in the top-level branches, and propose a technique that applies to the special structure and allows one to generate invariants at only one location and obtain the invariants at other locations through a propagation process. To illustrate the invariant propagation, we first identify the special structure of non-crossing affine transition systems. 

In the computation of invariants, previous approaches ~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable} require to generate the invariants at all the locations of an \LTS{}. As invariant computation is usually expensive, it is important to explore optimizations that avoid to generate invariants at every location. In this section, we propose a novel invariant propagation technique that is
%\ke{changes here.}
%In the process of generating affine invariants using the aforementioned method, it's important to note that projecting the calculated PAP $\Phi$ onto the template parameters corresponding to each Location (as shown in Example ~\ref{eg:farkasapplication}) and computing the corresponding generators leads to the core computational expense of the algorithm. To optimize this step, we propose an invariant propagation technique, 
applicable to any directed graph of an \LTS{} and achieves maximal efficiency in specific graph structures such as directed cycles. Below we  demonstrate the invariant propagation via its pseudocode. 

The pseudocode is given in Algorithm~\ref{alg:invprop}, where the variable \textit{SCCs} denotes a list of sub-\LTS{}'s, each element $\Gamma_i$ corresponding to the \LTS{} of a strongly connected component in the \(DG(\Gamma)\) graph, and the variable \textit{SCCNo} denotes the index of the SCC, to which $\tsLoc$ belongs, in the list \textit{SCCs}. Moreover, \textbf{FindSCC}($\tsLoc,\textit{SCCs}$) is used to find the index of the SCC containing the location $\tsLoc$ in the \textit{SCCs}. \textbf{Merge}($\eta,\eta_s$) is employed to merge two inductive assertion maps by disjunction. \textbf{Project}($\Gamma,\tsLoc$) is used to remove the relevant location and transitions of $\tsLoc$ in \LTS{} $\Gamma$ to obtain the sub-\LTS{} $\Gamma_{sub}$.



\begin{algorithm}
    \caption{Invariant Propagation \textbf{InvProp($\Gamma, DG(\Gamma), \tsLoc^*$)}}
    \label{alg:InvarintPropagation}
    \begin{algorithmic}[1]
        \REQUIRE $\Gamma$ --- \LTS{}, $DG(\Gamma)$ --- directed graph of $\Gamma$, $\tsLoc^*$ --- initial location of $\Gamma$.
        \ENSURE $\mathbf{\eta}$ --- an inductive assertion map for $\Gamma$.
        \STATE $\textit{SCCs} = \textbf{Tarjan}(DG(\Gamma),\Gamma)$
        \IF{$\textbf{Len}(\textit{SCCs}) \neq 1$}
            \STATE $\textit{SCCNo}=\textbf{FindSCC}(\tsLoc^*,\textit{SCCs})$
            \STATE $\textit{stack}.push(\textit{SCCNo},\tsLoc^*)$
            \WHILE{$\neg$ stack.isEmpty()}
                \STATE $\textit{curNo},\tsLoc_s=\textit{stack}\text{.pop()}$
                \STATE $\Gamma_s=\textit{SCCs}[\textit{curNo}]$
                \STATE $\mathbf{\eta_s}=\textbf{InvProp}(\Gamma_s,DG(\Gamma_s),\tsLoc_s)$
                \FOR{\textbf{each} transition $\tau$ directed from $\tsLoc_s$ to $\tsLoc_t$}
                    \STATE \textit{nextNo}$=$\textbf{FindSCC}$(\tsLoc_t,\textit{SCCs})$
                    \STATE $\textit{stack}.push(\textit{nextNo},\tsLoc_t)$
                \ENDFOR
                \STATE $\mathbf{\eta}=\textbf{Merge}(\mathbf{\eta},\mathbf{\eta_s})$
            \ENDWHILE
            \RETURN $\mathbf{\eta}$
        \ENDIF
        \STATE $\mathbf{\eta}=\textbf{InitInv}(\Gamma,\tsLoc^*)$
        \STATE $\Gamma_s=\textbf{Project}(\Gamma,\tsLoc^*)$
        \FOR{\textbf{each} transition $\tau$ directed from $\tsLoc^*$ to $\tsLoc_t$}
            \STATE $\mathbf{\eta_s}=\textbf{InvProp}(\Gamma_s,DG(\Gamma_s),\tsLoc_t)$
            \STATE $\mathbf{\eta}=\textbf{Merge}(\mathbf{\eta},\mathbf{\eta_s})$
        \ENDFOR
        \RETURN $\mathbf{\eta}$
    \end{algorithmic}
    \label{alg:invprop}
\end{algorithm}

First, at Line 1 of Algorithm \ref{alg:InvarintPropagation}, the $\textbf{Tarjan}()$ function, which refers to the classic Tarjan's algorithm to compute the strongly connected components (SCC) of a directed graph~\cite{tarjan1972depth}, computes the list of strongly connected components $\text{SCCs}=\Gamma_1, \ldots, \Gamma_n$ of the input \LTS{} $\Gamma$. 
Subsequently, we decompose the entire directed graph $DG(\Gamma)$ into the clusters of its strongly connected components and cases that cannot be further divided. 
In the following text, we use "SCC" to refer to the strongly connected components in the directed graph, 
as well as to both the \LTS{} \(\Gamma\) projected onto the corresponding parts and the sub-\LTS{} \(\Gamma_s\).

For the divisible cases that the whole graph $DG(\Gamma)$ can be divided into different SCCs, as encountered at Line 2, we traverse the SCCs starting from the SCC of the initial location \(\tsLoc^*\) in a depth-first search order. In each traversal, we call the \textbf{InvProp}() function on the current SCC to solve its inductive AAM $\eta$. We then consider transitions that connect the current SCC to another SCC (at Line 9 of the pseudocode, with some details omitted such as the fact that these edges definitely connect the current SCC with another distinct SCC). We conjunctively propagate the invariants with the transition guard conditions in a single step to another SCC, setting it as the initial condition \(\theta\). Afterwards, we add this to the stack, specifying the relevant starting location.

For the indivisible cases, we adopt the projection method as in ~\citet{oopsla22/scalable} to compute the invariant \(\eta({\tsLoc^*})\) at the starting location \(\tsLoc^*\) only, which is implemented by \textbf{InitInv}() in Line 17. Similarly, we project the SCC onto the sub-SCC \(\Gamma_{sub}\) (Line 18) after removing the initial location \(\tsLoc^*\). We then consider the initial conditions \(\theta_s\) and location \(\tsLoc_s\) for \(\Gamma_s\) obtained along different transitions (Line 19) originating from the initial location. The \textbf{InvProp}() function is recursively called to obtain the complete inductive AAM \(\eta\) for the input \(\Gamma\).

\begin{example}
Recall the example in Section~\ref{overview}, specifically Example~\ref{fig:ExampleofInvPropagation}. Here, \(\Gamma\) is an indivisible SCC. After computing the invariant $\eta(\tsLoc_2)$ of the ATS $\Gamma$ at the initial location $\tsLoc_2$, we consider all transitions $\{\tau_4\}$ starting from the initial location $\tsLoc_2$, as depicted in the figure. Then, we propagate the invariant through the transition $\tau_4$ to \(\tsLoc_1\). After project to obtain the remaining sub-\LTS{} $\Gamma_{sub}$, composed of \(\tsLoc_1\) and its self-loop transition, we recursively compute this indivisible SCC to obtain the complete inductive assertion map. \qed
\end{example}

Our invariant propagation technique applies to all 
%lies not only in its wide applicability, applicable to all 
\LTS{}. The main advantage to incorporate this technique is that it allows the generation of invariants only at the initial locations of (sub-)SCCs, thus avoiding the generation of the ivnariants at all locations as adopted in ~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable}. In the case that the directed graph of the input \LTS{} is a cycle, our invariant propagation reaches the highest efficiency that generates the invariant only at the initial location of the cycle and derives invariants at other locations of the cycle by propagation, since the cycle has an explicit topological order after the removal of the initial location. This advantage becomes more prominent in loops with a non-neglectable amount of conditional branches.


%but also in its efficiency compared to previous methods. Earlier approaches generated invariants at all locations either by solving wholly~\cite{DBLP:conf/sas/SankaranarayananSM04} or solving separately~\cite{oopsla22/scalable} through the generators computation of polyhedral cones. Invariant propagation reduces this problem to computing invariants for a single location and its self-loop transitions (which may not exist), as well as computing an invariant for the initial location of a single SCC. This approach addresses the relatively high overhead and potential exponential explosion, significantly improving time efficiency. 

Finally, we present the soundness of the invariant propagation in the following theorem. 

\begin{theorem}
    The assertion maps generated by the invariant propagation algorithm is inductive.
\end{theorem}

%will present a proof of the correctness of the invariant propagation algorithm in generating the inductive affine assertion map.
\begin{proof}
We prove by an induction on the number $k$ of locations in the input \LTS{} \(\Gamma\)  that the assertion map obtained by our invariant propagation algorithm for the \LTS{} \(\Gamma\) 
%with \(k\) locations 
is inductive. 

We firstly consider the base case, i.e., \(k = 1\). In this case, \(DG(\Gamma)\) has only one location, which is obviously indivisible. Here, the function \textbf{InitInv}(), previously mentioned as applying Farkas' Lemma for conjunctive invariant computation, is called. Therefore, the resulting assertion map is inductive, and its correctness is guaranteed by the prior results.

Assuming that the case when the size of $\Gamma$ equals \(k\) holds, we prove that it holds for $\Gamma$ of size \(k + 1\). 
For an \LTS{} \(\Gamma\) with \(k + 1\) locations, if it is divisible, it can be decomposed into several sub-SCCs \(\Gamma_{sub}\) with sizes less than or equal to \(k\). After the call to function \(\textbf{InvProp}()\) at Line 8, we obtain an inductive assertion mapping by the inductive condition. The \(\textbf{Merge}\)() function does not affect the inductive condition of the combined mapping. On the other hand, if it is indivisible, then our approach computes the invariant at its initial location and, after projecting away the initial location \(\tsLoc^*\), obtains a sub-\LTS{} \(\Gamma_{sub}\) of size \(k\). Similarly, the recursive call to invariant propagation at Line 20 and merging the returned results always yields an inductive assertion map by the inductive condition.
\end{proof}

\subsection{Other Optimizations}\label{sec:otheropt}

In this section, we consider further optimizations. The first is the tackling of infeasible implications (i.e., "$-1\ge 0$") in the application of Farkas' Lemma. 
This situation has not been handled in the previous approaches in Farkas' Lemma. The infeasible implication is important in the generation of disjunctive invariants since ignoring them would break the internal disjunctive feature of the loop, thus leading to the failure of the generation of the desired disjunctive invariant. A key difficulty to tackle the infeasible implication is that we obtain general polyhedra rather than polyhedral cones in establishing the constraints for invariant generation, and thus cannot directly apply the generator computation over polyhedral cones. To address this difficulty, we show that it suffices to fix the nonlinear parameter $\mu$ multiplied to the template 
in the Farkas tabular (Figure~\ref{tab:farkas}) to $1$ and include the generators of both the polytope and the polyhedral cone of the Minkowski decomposition of the polyhedron resulting from the constraint solving of the PAP after the application of Farkas' Lemma. As its correctness proof is somewhat technical, we relegate them to Appendix~\ref{sec:appendix_mu1} and Appendix~\ref{sec:appendix_minkowski}.

The second is the extension of our approach to nested loop. The main difficulty here is how to handle the inner loops in a nested loop. Recall that in the previous section, we transform a non-nested loop into a canonical form and further transforms it into an affine transition system. This cannot be applied to nested loops since the inner loops does not obey this canonical form. To address this difficulty, we consider to use the standard technique of loop summary to abstract the input-output relationship of the inner loops, and use the loop summaries of inner loops to construct the overall affine transition system for the outer loops. 

Given a nested affine while loop $W$, our approach works by first recursively computing the loop summary $\ProcSmry_{W'}$ for each inner while loop $W'$ in $W$ (from the innermost to the outermost), 
%that is a while loop which is in the loop body of $W$ and does not lie in other inner loops), 
and then %construct the affine transition system for 
tackling the main loop body via the control flow of the loop body and the loop summaries $\ProcSmry_{W'}$ of the inner loops. Below we fix a nested affine while loop $W$ with variable set $\tsVars=\{\tsVar_1,\dots,\tsVar_n\}$ and present the technical details. 

The most involved part in our approach is the transformation of the main loop $W$ into its corresponding \LTS{}.  %and the loop summaries of its inner loops. 
%Unlike the situation of unnested while loops, a direct recursive algorithm that transforms the loop $W$ into a canonical form in Figure~\ref{fig:unnestedPQandRecursive} as in the unnested case is not possible, since one needs to tackle the loop summaries from the inner while loops in $W$.
%since to apply the state-transition pattern here one needs to specify the conditions for the output of the inner while loops in a total conditionl branch (i.e., a conditional branch that appears at the top level like some $\varphi_i$ in Figure~\ref{?}). 
To address the inner loops, our algorithm works with the \emph{control flow graph} (CFG) $H$ of the loop body of the loop $W$ and considers the \emph{execution paths} in this CFG. The CFG $H$ is a directed graph whose vertices are the program counters of the loop body and whose edges describe the one-step jumps between these program counters. Except for the standard semantics of the jumps emitting from assignment statements and conditional branches, for a program counter that represents the entry point of an inner while loop that is not nested in other inner loops, we have the special treatment that the jump at the program counter is directed to the termination program counter of this inner loop in the loop body of $W$ (i.e., skipping the execution of this inner loop). An \emph{execution path} in the CFG $H$ is a directed path of program counters that ends in (i) either the termination program counter of the loop body of $W$ without visiting a  program counter that represents the \textbf{break} statement or (ii) a first \textbf{break} statement without visiting prior \textbf{break} statements. 
An example is as follows.

% Notably, our back-end includes two additional features. The first one is the functionality to remove invalid transitions with unsatisfiable guard condition $\tsGuardcond$. 
% The second one is the treatment of the situation of the unsatisfiability in the application of Farkas' Lemma 
% (see $-1\ge 0$ at the bottom of Figure~\ref{tab:farkasinit} and Figure~\ref{tab:farkascons}), which is however missing in the original tool StInG~\cite{Sting}. The former can simplify the \LTS{} to improve time efficiency and the later can increase accuracy. 
% A key difficulty in the second one is that we obtain polyhedra rather than polyhedral cones, and thus cannot directly apply the generator computation. To address this difficulty, we show that it suffices to consider $\mu=1$ in Figure~\ref{tab:farkascons} and include the generators of both the polytope and the polyhedral cone of the Minkowski decomposition of the polyhedron. As its correctness proof is somewhat technical, we relegate them to Appendix~\ref{sec:appendix_mu1} and Appendix~\ref{sec:appendix_minkowski}.

\begin{example}\label{eg:janne_cfg}
Consider the janne\_complex program from ~\citet{DBLP:conf/vmcai/BoutonnetH19} in Figure~\ref{fig:jannecomplex}.  
The CFG of the program is given in Figure~\ref{fig:janne_cfg} where the nodes correspond to the program counters, the directed edges with guards specifies the jumps and their conditions, and the affine assignments are given in the program counters $A_1,A_2,A_3$. 

\newsavebox{\jannecomplex}
\begin{lrbox}{\jannecomplex}
\begin{lstlisting}[mathescape]
while($x<30$){
    while($y<x$){
        if ($y>5$) $y=y*3$;
        else $y=y+2$;
        if (y>=10 && y<=12) $x=x+10$;
        else $x=x+1$;
    }
    $x=x+2$; $y=y-10$;
}
\end{lstlisting}
\end{lrbox}

% Figure environment removed

% Figure environment removed

We denote by $W$ the outer loop with entry point $E_{\mathrm{Outer}}$, and by $W'$ the inner loop with entry point $E_{\mathrm{inner}}$.    
%the CFG of a specific example with outermost loop $W$ and the inner loop $W'$ within $W$ from \emph{janne\_complex}~\cite{DBLP:conf/vmcai/BoutonnetH19} in Figure~\ref{fig:janne_cfg}. 
The execution path starts at the \emph{Initial Condition} $\left[x, y \right]$, jumps to the next vertices along the edge whose condition is satisfied (e.g., \emph{True} is tautology, \emph{$x < 30$} is satisfied when variable $x$ value is less than $30$, etc.), and terminates in the \emph{Exit} statement. 
The only execution path for the loop body of $W$ is %$E_{Outer} \rightarrow A_{IS} \rightarrow A_{1}$, 
$A_{IS} \rightarrow A_{1}$,
%\rightarrow E_{Outer}$, 
for which we abstract the whole inner loop by $A_{IS}$. 
%The \emph{$E_{Outer}$} means the outermost loop entry of $W$ and \emph{$E_{Inner}$} means the inner-loop entry of $W'$ in $W$. 
%The \emph{$A_1$, $A_2$, $A_3$} represents the assignment statements in the program and \emph{$A_{IS}$} is a special assignment statement for the inner-loop $W'$ which could be obtained by computing loop summary for $W'$. 
\qed 
\end{example}


Based on the CFG $H$ and the execution paths, our approach constructs the \LTS{} for the outer loop $W$ as follows. Since the output of an inner while loop $W'$ in $W$ cannot be exactly determined from the input to the loop $W'$, we first have fresh output variables 
$\overline{x}_{W',1},\dots,\overline{x}_{W',n'}$ to represent the output values of the variables $\overline{x}_{W',1},\dots,\overline{x}_{W',n'}$ after the execution of the inner loop $W'$. These output variables are used to express the loop summaries of these inner loops. 

Then, to get the numerical information from execution paths,  we symbolically compute the values of the program variables at each program counter in an execution path. In detail, given an execution path $\omega=\iota_1,\dots,\iota_k$ where each $\iota_i$ is a program counter of the loop body of the loop $W$, our approach computes the affine expressions $\alpha_{\tsVar,i}$ and PAPs $\beta_i$ (for $\tsVar\in \tsVars$ and $1\le i\le k$) over the program variables in $\tsVars$ (for which they represent their initial values at the start of the loop body of $W$ here) and the fresh output variables. 
%(for the output of the inner loops). 
The intuition is that (i) each affine expression $\alpha_{\tsVar,i}$ represents the value of the variable $\tsVar$ at the program counter $\iota_i$ along the execution path $\omega$ and (ii) each PAP $\beta_i$ specifies the condition that the program counter $\iota_i$ is reached along the execution path $\omega$. The computation is recursive on $i$ as follows.

Denote the vectors $\alpha_i:=(\alpha_{\tsVar_1,i},\dots, \alpha_{\tsVar_n,i})$ and  $\overline{x}_{W'}=(\overline{x}_{W',1},\dots,\overline{x}_{W',n'})$. For the base case when $i=1$, we have $\alpha_1=(\tsVar_1,\dots,\tsVar_n)$ and $\beta_1=\mathbf{true}$ that specifies the initial setting at the start program counter $\iota_1$ of the loop body of the original loop $W$. For the recursive case, suppose that our approach has computed the affine expressions in $\alpha_{i}$ and the PAP $\beta_i$. We classify four cases below:

\begin{itemize}
\item \emph{Case 1:} The program counter $\iota_{i}$ is an affine assignment statement $\mathbf{x}:=\mathbf{F}(\mathbf{x})$. Then we have that $\alpha_{i+1}= \alpha_i[\mathbf{F}(\mathbf{x})/\mathbf{x}]$ and $\beta_{i+1}:=\beta_i$.  
\item \emph{Case 2:} The program counter $\iota_{i}$ is a conditional branch with branch condition $b$ and the next program counter $\iota_{i+1}$ follows its \textbf{then}-branch. Then the vector $\alpha_{i+1}$ is the same as $\alpha_i$, and the PAP $\beta_{i+1}$ is obtained as $\beta_{i+1}=\beta_i\wedge b$.   
\item \emph{Case 3:} The program counter $\iota_{i}$ is a conditional branch with branch condition $b$ and the next program counter $\iota_{i+1}$ follows its \textbf{else}-branch. The only difference between this case and the previous case is that $\beta_{i+1}$ is obtained as $\beta_{i+1}:=\beta_i\wedge \neg b$.   
\item \emph{Case 4:} The program counter $\iota_{i}$ is the entry point of an inner while loop $W'$ of $W$ and $\iota_{i+1}$ is the successor program counter outside $W'$ in the loop body of $W$. Then $\alpha_{i+1}:=\overline{x}_{W'}$ and $\beta_{i+1}:=\ProcSmry_{W'}(\alpha_i, \overline{x}_{W'})$. Here we use the ouput variables to express the loop summary. Note that the loop summary $\ProcSmry_{W'}$ (see Page~\pageref{pg:loopsmry} for the definition of $\ProcSmry$) is recursively computed. 
\end{itemize}  

\begin{example}\label{eg:evolution}
Continue with the execution path in Example~\ref{eg:janne_cfg}. 
% Figure environment removed
The evolution of $\alpha_{i}$ and $\beta_{i}$ with the initial setting $\alpha_{1}=[x,y],\beta_{1}=\mathbf{true}$ is given in Figure~\ref{fig:outer_inner}.  %where $W$ denotes the current loop. 
\qed 
\end{example}


After the $\alpha_i,\beta_i$'s are obtained for an execution path $\omega=\iota_1,\dots,\iota_k$ from the recursive computation above, we let the PAP $\Psi_\omega:=\bigwedge_{i\in I} \beta_i$ where the index set $I$ is the set of all $1\le i\le k$ such that the program counter $\iota_i$ corresponds to either a conditional branch or the entry point of an inner while loop, and the vector of affine expression $\alpha_{\omega}:=\alpha_{k+1}$. Note that the PAP $\Psi_\omega$ is the condition that the execution of the loop body follows the execution path $\omega$, and the affine expressions in the vector $\alpha_\omega$ represent the values of the program variables after the execution path $\omega$ of the loop body of $W$ in terms of the initial values of the program variables and the fresh variables for the output of the inner while loops in $W$. 


Finally, our approach constructs the \LTS{} for the loop $W$ and we only present the main points: %as follows:

\begin{itemize}
\item First, for each execution path $\omega$ of the loop body of $W$, we have a standalone location $\tsLoc_\omega$ for  this execution path. Recall that we abstract the inner loops, so that the execution paths can be finitely enumerated. 
\item Second, for all locations $\tsLoc_\omega,\tsLoc_{\omega'}$ (from the execution paths $\omega,\omega'$), we have the transition $\tau_{\omega,\omega'}:=(\tsLoc_{\omega}, \tsLoc_{\omega'}, \Psi_{\omega} \wedge \Psi'_{\omega'}\wedge \mathbf{x}'=\alpha_\omega)$ which means that if the execution path in the current iteration of the loop $W$ is $\omega$, then in the next iteration the execution path can be $\omega'$ with the guard condition $\Psi_{\omega} \wedge \Psi'_{\omega'}\wedge \mathbf{x}'=\alpha_\omega$ that comprises the conditions for the execution paths $\omega,\omega'$ and the condition $\mathbf{x}'=\alpha_\omega$ for the next values of the program variables.
\item Third, we enumerate all possible initial locations $l_\omega$, along with their corresponding initial conditions $\tsInitcond= G \wedge \Psi_{\omega}$. To derive loop summary, we follow the standard technique (see e.g.~\citet{DBLP:conf/vmcai/BoutonnetH19}) to include the input variables $\tsVars_{\mathsf{in}}$ and conjunct the affine assertion $\bigwedge_{\tsVar\in\tsVars} \tsVar=\tsVar_{\mathsf{in}}$ into each disjunctive clause of the initial condition $\tsInitcond$. 

Manually specified initial conditions can also be conjuncted into 
$\tsInitcond$. 
\end{itemize}

A detailed process that handles $\mathbf{break}$ statement is similar to the unnested situation. Again, we can remove invalid transitions by checking whether their guard condition is satisfiable or not. 

At the end of the illustration of our algorithms, we discuss possible extensions as follows. 

\begin{remark}[Extensions]\label{rmk:extension}
Our approach can be extended in the following ways. To obtain a more precise branch condition representation, one extension is by (i) distinguishing the remainders (e.g., even/odd) modulo a fixed positive integer (e.g. 2) when handling modular arithmetics and (ii) detecting hidden termination phases via the approach in \cite{DBLP:conf/cav/Ben-AmramG17}. To handle machine integers, another extension is by having a piecewise disjunctive treatment for the cases of overflow and non-overflow. 
%To handle polynomial invariants, a further extension is by applying Handelman's theorem (see e.g.~\cite{DBLP:conf/cav/ChatterjeeFG17}) that yields affine constraints similar to the application of Farkas' Lemma. 
Finally, our approach could be extended to floating point numbers by considering piecewise affine approximations~\cite{DBLP:conf/esop/Mine04, DBLP:conf/vmcai/Mine06}. 
\end{remark}




