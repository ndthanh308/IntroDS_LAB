
\section{Algorithmic Details in Our Approach}
\label{sec:alg}
Below we present our approach for generating affine disjunctive invariants over affine programs. We first illustrate our control flow transformation for non-nested loops, then our invariant propagation to reduce invariant computation, and finally the resolution of the infeasible implication and the extension to nested loops.


\subsection{Control Flow Transformation}\label{sec:cft}
We fix the set of program variables in a loop
as $X=\{x_1,\dots,x_n\}$ and identify it as the set of variables in the \LTS{} to be derived from the loop. We consider the canonical form of a non-nested affine while loop as in Figure~\ref{fig:CanonicalForm} similar to \cite{DBLP:conf/cav/JiFFC22}, where we have: 
\begin{itemize}
\item The column vector $\mathbf{x}=(x_1,\dots,x_n)^{\mathrm{T}}$ represents the vector of program variables, and $G$ is a disjunction of affine assertions that serves as the loop condition. 
\item  Each $\mathbf{F}_i$ ($1\le i\le m$) is an affine function, i.e.,  $\mathbf{F}_i(\mathbf{x})=\mathbf{A} \mathbf{x}+\mathbf{b}$ where $\mathbf{A}$ (resp. $\mathbf{b}$) is an $n\times n$ square matrix (resp. $n$-dimensional column vector) that specifies the affine update under the function $\mathbf{F}_i$ in the conditional branch $\phi_i$. 
The assignment $\mathbf{x}:=\mathbf{F}_i(\mathbf{x})$ is considered simultaneously for the variables in $\mathbf{x}$ so that in one execution step, the current valuation $\tsEval$ is updated to $\mathbf{F}_i(\tsEval)$. 
\item The statements $\delta_1,\dots,\delta_m$ specify whether the loop continues after the affine update of the conditional branches $\phi_1,\dots,\phi_m$. Each statement $\delta_i$ is either the \textbf{skip} statement that has no effect or the \textbf{break} statement that exits. 
\end{itemize}

\lstset{language=program}
\lstset{tabsize=3}
\newsavebox{\unnested}
\begin{lrbox}{\unnested}
\begin{lstlisting}[mathescape]
while $(G)$ {
  case $\phi_1$: $\mathbf{x}:=\mathbf{F}_1(\mathbf{x})$;$\delta_1$;
      $\vdots$ 
  case $\phi_m$: $\mathbf{x}:=\mathbf{F}_m(\mathbf{x})$;$\delta_m$;
}
\end{lstlisting}
\end{lrbox}

% Figure environment removed

Any non-nested affine while loop with a break statement can be transformed into the canonical form in Figure~\ref{fig:CanonicalForm} by recursively examining the substructures of the loop body. 
A detailed recursive transformation process is provided in Appendix~\ref{appendix:transform}.
Note that although the transformation into our canonical form may cause exponential growth in the number of conditional branches in the loop body, in practice a loop typically has a small number of conditional branches and further improvement can be carried out by removing invalid branches (i.e., those whose branch condition is unsatisfiable, such as $\tau_2$ in Figure~\ref{fig:ExampleCodeAndATS}). 

Moreover, such a canonical form is often necessary to capture precise disjunctive information in a while loop. Each case corresponds not merely to an individual loop path but also encapsulates the set of states at the entry of a loop. By finely partitioning the incoming program states according to branching conditions and formulating constraints among these states, we endeavor to precisely characterize the dynamics of internal state transitions within loops exhibiting multi-phase behavior.
Below we demonstrate our control flow transformation that transforms the canonical form into an ATS. 

Formally, the \LTS{} $\Gamma_W$ for a loop $W$ in our canonical form is 
%constructed 
given as follows:
\begin{itemize}
\item The set of locations is $\{\tsLoc_1,\dots, \tsLoc_m, \tsLoc_{e}\}$, 
where each $\tsLoc_i$ ($1\le i\le m$) corresponds to \(i\)-th case in the canonical form and $\tsLoc_{e}$ is the termination location of the loop. 
\item For each $1\le i\le m$ such that $\delta_i=\mathbf{break}$, we have the transition (we denote $\mathbf{x}':=(x'_1,\dots,x'_n)^\mathrm{T}$)
$$
\tau_{i}=(\tsLoc_i, \tsLoc_e, G \wedge \phi_i \wedge  \mathbf{x}'=\mathbf{F}_i(\mathbf{x}))
$$
that specifies the one-step jump from \(\tsLoc_i\) to the termination location $\tsLoc_e$.
\item For each $1\le i,j\le m$ such that $\delta_i\neq \mathbf{break}$, we have the transition 
$$
\tau_{ij}=(\tsLoc_i, \tsLoc_j, G \wedge \phi_i \wedge G[\mathbf{x}'/\mathbf{x}] \wedge \phi_j[\mathbf{x}'/\mathbf{x}] \wedge \mathbf{x}'=\mathbf{F}_i(\mathbf{x}))
$$ 
that specifies the jump from \(\tsLoc_i\) in the current loop iteration to \(\tsLoc_j\)  in the next loop iteration.
\item For each $1\le i\le m$ such that $\delta_i\neq \mathbf{break}$, we have the transition 
$$
\tau'_{i}=(\tsLoc_i, \tsLoc_{e}, G\wedge \phi_i\wedge (\neg G)[\mathbf{x}'/\mathbf{x}]\wedge \mathbf{x}'=\mathbf{F}_i(\mathbf{x}))
$$ 
for the jump from \(\tsLoc_i\) to the termination location $\tsLoc_e$.
\end{itemize} 

After the transformation, we remove transitions with an unsatisfiable guard condition to reduce the size of the derived \LTS{}. The transformation for our running example has been given in Figure~\ref{fig:ExampleCodeAndATS}. 
The transformed \LTS{} $\Gamma_W$ enables us to apply existing approaches~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable} to generate invariants at the locations of the ATS $\Gamma_W$. 
Finally, recall that the overall disjunctive invariant for the ATS $\Gamma_W$ is the disjunction of the invariants at all the locations. 
    
As for the previous approaches~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable} that only set locations at the initial location of the loop, our control flow transformation has different locations corresponding to different paths of the original loop, thereby achieving fine-grained piecewise invariants. 
Moreover, contrary to the granular program translations employed in traditional software model checking, which is similar to the control flow transformation, our motivation is mainly to integrate this transformation with Farkas' Lemma. By analyzing the transition patterns among internal loop paths, we aim to more effectively capture the phase-specific characteristics of multi-stage programs.

As demonstrated by our experiments, considering transitions between any pair of paths \((\tsLoc_i, \tsLoc_j)\), rather than partitioning the loop into more complex path regular expressions (using regular expressions to abstract loop behaviors over multiple iterations) allows us to maintain a balance between the precision of invariant generated and the efficiency of constraint solving. To further improve the efficiency, we have designed the invariant propagation algorithm shown below for these directed graphs constructed from paths.

\subsection{Invariant Propagation}
In the computation of invariants, previous approaches ~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable} require to generate the invariants at all the locations of an \LTS{}. As invariant computation is usually expensive, it is important to explore optimizations that avoid redundant computations. In this section, we propose a novel invariant propagation technique that is applicable to any directed graph of an \LTS{} and achieves maximal efficiency in specific graph structures such as directed cycles. Below we  demonstrate the procedure of invariant propagation via Algorithm~\ref{alg:invprop}. 


\begin{algorithm}[t]
    \caption{\textit{InvProp}($\Gamma$,\(DG(\Gamma)\),\(\tsLoc^*\))}
    \begin{algorithmic}[1]
        \REQUIRE $\Gamma$ --- \LTS{}, $DG(\Gamma)$ --- directed graph of $\Gamma$, $\tsLoc^*$ --- initial location of $\Gamma$.
        \ENSURE $\mathbf{\eta}$ --- an inductive assertion map for $\Gamma$.
        \STATE \textit{Init assertion map} \(\eta\) for \(\Gamma\) .  \label{line:1}
        \STATE $\text{SCCs} \gets \textit{Tarjan}(DG(\Gamma),\Gamma)$ \Comment{Find all SCCs in the directed graph} \label{line:2}
        \IF{$\textit{Len}(\text{SCCs}) \neq 1$} \label{line:3}
            \STATE $\text{id} \gets \textit{FindSCC}(\tsLoc^*,\text{SCCs})$ \Comment{Find the SCC containing $\tsLoc^*$} 
            \STATE $\text{stack}.\textit{push}(\text{id},\tsLoc^*)$ 
            \WHILE{$\neg \text{stack}.\textit{isEmpty}()$}
                \STATE $(\text{cur}, \tsLoc_s )\gets \text{stack}.\textit{pop}()$ \Comment{\(\tsLoc_s\) is the initial location of current SCC}
                \STATE $\Gamma_s \gets \text{SCCs}[\text{cur}]$ \Comment{\emph{cur} is the index of current traversed SCC}
                \STATE $\mathbf{\eta_s} \gets \textit{InvProp}(\Gamma_s,DG(\Gamma_s),\tsLoc_s)$ 
                \Comment{Process single SCC}
                \FOR{\textbf{each} transition $\tau$ directed from $\tsLoc_s$ to $\tsLoc_t$} \label{line:for1}
                    \STATE $\text{next} \gets \textit{FindSCC}(\tsLoc_t,\text{SCCs})$
                    \IF{\(\text{next} \neq \text{cur} \)}
                       \STATE $\text{stack}.\textit{push}(\text{next},\tsLoc_t)$ \Comment{Traverse SCCs in BFS order}
                    \ENDIF
                \ENDFOR
                \STATE $\mathbf{\eta} \gets \textit{Merge}(\mathbf{\eta},\mathbf{\eta_s})$ \label{line:merge}
                \Comment{Combine assertion maps disjunctively}
            \ENDWHILE
            \RETURN $\mathbf{\eta}$ 
        \ENDIF \label{line:19}
        \STATE $\mathbf{\eta} \gets \textit{InitInv}(\Gamma,\tsLoc^*)$ \label{line:init_inv}
        \Comment{Compute invariant only in initial location}
        \STATE $\Gamma_s \gets \textit{Project}(\Gamma,\tsLoc^*)$ \label{line:project}
        \Comment{Derive sub-\LTS{} \(\Gamma_s\) by removing \(l^*\)}
        \FOR{\textbf{each} transition $\tau$ directed from $\tsLoc^*$ to $\tsLoc_t$} \label{line:for2}
        \STATE $\mathbf{\eta_s} \gets \textit{InvProp}(\Gamma_s,DG(\Gamma_s),\tsLoc_t)$
        \STATE $\mathbf{\eta} \gets \textit{Merge}(\mathbf{\eta},\mathbf{\eta_s})$ \label{line:merge2}
        \ENDFOR
        \RETURN $\mathbf{\eta}$ \label{line:return_res_single}
    \end{algorithmic}
    \label{alg:invprop}
\end{algorithm}

The algorithm consists of the following steps: First, we initialize the assertion map and use the classical Tarjan's algorithm~\cite{tarjan1972depth} to compute a list of strongly connected components (SCCs) in the directed graph \(DG(\Gamma)\) (lines~\ref{line:1}-\ref{line:2}). Depending on whether the graph is decomposable, i.e., whether the size of the SCC list is more than one, we consider two cases: 

(\roman{counter1}) For a directed graph that can be decomposed into multiple SCCs, we start from the entry SCC and traverse the list of SCCs in breadth-first order, computing invariants for each SCC recursively and integrating them into the final assertion map \(\eta\) (lines~\ref{line:3}-\ref{line:19}).

(\roman{counter2}) For a directed graph that is a single SCC, we compute the initial invariants at the starting location using the previous method~\cite{oopsla22/scalable}, then eliminate the start location \(\tsLoc^*\), traverse each edge originating from \( \tsLoc^*\) , propagate the invariants to the remaining sub-graph, and disjunctively merge the returned inductive assertion mappings to produce the final disjunctive invariant (lines~\ref{line:init_inv}-\ref{line:return_res_single}).

We formally define the specific functions involved in the algorithm as follows:

\begin{enumerate}
    \item \textbf{Merge}$(\eta_1, \eta_2)$ (line~\ref{line:merge} and line~\ref{line:merge2}). We extend $\eta$ to a mapping from the set of locations $\tsLocs$ to disjunction of affine assertions, specifically representing affine inequalities in DNF. The Merge function is thereby defined as a new mapping such that, for any location $\tsLoc \in \tsLocs$:
    \[
    \text{Merge}(\eta_1, \eta_2)(\tsLoc) = \eta_1(\tsLoc) \lor \eta_2(\tsLoc)
    \]
    
    \item \textbf{Project}$(\Gamma, \tsLoc^*)$ (line~\ref{line:project}). Considering the directed graph $\text{DG}(\Gamma)$ corresponding to the \LTS{} $\Gamma$, we remove all edges associated with the node $\tsLoc^*$, as well as the node itself. The derived \LTS{} corresponding to the resulting sub-graph is denoted by $\text{Project}(\Gamma, \tsLoc^*)$.
\end{enumerate}

Note that in Algorithm~\ref{alg:invprop}, we omitted the initial condition \(\tsInitcond\) and the propagation effect along the transitions. 
At each point of our algorithm (line~\ref{line:for1} and line~\ref{line:for2}) that tackles a transition \(\langle \tsLoc_s, \tsLoc_t,\tsGuardcond \rangle\), the propagation effect is computed as the post image of the conjunction of the invariant on \(\tsLoc_s\) and the guard \(\tsGuardcond\) via polyhedral projection onto the primed variables $\tsVars'$ and serves as the initial condition \(\tsInitcond\) of the new \LTS{} including \(\tsLoc_t\).

\begin{example}
Recall the example in Section~\ref{sec:overview}, specifically Figure~\ref{fig:ExampleofInvPropagation}. Here, \(\Gamma\) is an indivisible SCC. After computing the invariant $\eta(\tsLoc_2)$ of the ATS $\Gamma$ at the initial location $\tsLoc_2$, we consider all transitions (i.e. $\{\tau_4\}$) starting from the initial location $\tsLoc_2$, as depicted in the figure. Then, we propagate the invariant through the transition $\tau_4$ to \(\tsLoc_1\). After project to obtain the remaining sub-\LTS{} $\Gamma_{sub}$, composed of \(\tsLoc_1\) and its self-loop transition, we recursively compute this indivisible SCC to obtain the complete inductive assertion map. \qed
\end{example}

Our invariant propagation technique applies to all 
\LTS{}. The main advantage to incorporate this technique is that it allows the generation of invariants only at the initial locations of (sub-)SCCs, thus avoiding the generation of the invariants at all locations as adopted in~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable}. In the case that the directed graph of the input \LTS{} is a cycle, our invariant propagation reaches the highest efficiency that generates the invariant only at the initial location of the cycle and derives invariants at other locations of the cycle by propagation, since the cycle has an explicit topological order after the removal of the initial location. This advantage becomes more prominent in loops with a non-neglectable amount of conditional branches.
The soundness of the invariant propagation is given in the following theorem.

\begin{theorem}
    \label{thm:propagation}
    The assertion maps generated by the invariant propagation algorithm are inductive.
\end{theorem}

\begin{proof}
We prove by induction on the number $k$ of locations in the input \LTS{} \(\Gamma\)  that the assertion map obtained by our invariant propagation algorithm for the \LTS{} \(\Gamma\) is inductive. 

We first consider the base case, i.e., \(k = 1\). In this case, \(DG(\Gamma)\) has only one location, which is obviously indivisible. Here, the function \textbf{InitInv}(), previously mentioned as applying Farkas' Lemma for conjunctive invariant computation, is called. Therefore, the resulting assertion map is inductive, and its correctness is guaranteed by the prior results.

Assuming that the case when the size of $\Gamma$ equals \(k\) holds, we prove that it holds for $\Gamma$ of size \(k + 1\). 
For an \LTS{} \(\Gamma\) with \(k + 1\) locations, if it is divisible, it can be decomposed into several sub-SCCs \(\Gamma_{sub}\) with sizes less than or equal to \(k\). After the call to function \(\textbf{InvProp}()\) at Line 8, we obtain an inductive assertion mapping by the inductive condition. The \(\textbf{Merge}\)() function does not affect the inductive condition of the combined mapping. On the other hand, if it is indivisible, then our approach computes the invariant at its initial location and, after projecting away the initial location \(\tsLoc^*\), obtains a sub-\LTS{} \(\Gamma_{sub}\) of size \(k\). Similarly, the recursive call to invariant propagation at Line 20 and merging the returned results always yields an inductive assertion map by the inductive condition. \qed
\end{proof}

\subsection{Other Optimizations}\label{sec:otheropt}


\smalltitle{Loop Summary.} To address more general control flow, such as nested loops, we use the standard method of loop summary to express the input-output relationship of the inner loops (while adding fresh variables for input values) to handle nested loops. See Appendix~\ref{app:loop_sum} for the technical details.

\smalltitle{Infeasible Implication.} In the previous results~\cite{DBLP:conf/sas/SankaranarayananSM04,oopsla22/scalable}, the infeasible implication is not handled in their prototype. 
Recall the infeasible implication corresponding to $\eta(\tsLoc) \land \rho \models -1 \geq 0$ illustrated in Figure~\ref{tab:farkascons}. To fully address this issue, we can simply set \(\mu=1\) in Figure~\ref{tab:farkascons} so that the nonlinear multiplier \(\mu\) is eliminated. The correctness is given by the following theorem. 

\begin{theorem} \label{thr:infeasible}
Let \(\Gamma\) be an ATS. For any AAM \(\eta\) that fulfills the initial and consecution conditions derived from the ATS \(\Gamma\) with the original constraints for the infeasible implication as in each consecution tabular of Figure~\ref{tab:farkascons} (aimed at \(-1>=0\)) with each $\mu$ in an infeasible implication instantiated as $k$ for some $k>0$, it is equivalent to setting all $\mu$'s to $1$ while preserving the constraints of infeasible implication.
\end{theorem}

We present our proof in Appendix~\ref{app:thm_infeasible} and Appendix~\ref{sec:appendix_minkowski}. The main idea of the proof is that, for the infeasible implication case,  by scaling each \(\lambda_i\) other than \(\mu\), the consecution tabular used to generate polyhedra is transformed into an equivalent tabular with scaled lambda variables \(\lambda'_i\). so that it suffices to choose the multiplier $\mu$ to be $1$. 

\begin{remark}[Extensions]\label{rmk:extension}
Our approach can be extended in the following ways. To obtain a more precise path condition representation, one extension is by (i) distinguishing the remainders (e.g., even/odd) modulo a fixed positive integer (e.g. 2) when handling modular arithmetic and (ii) detecting hidden termination phases via the approach in \cite{DBLP:conf/cav/Ben-AmramG17}. To handle machine integers, another extension is by having a piecewise disjunctive treatment for the cases of overflow and non-overflow. 
By applying standard bottom-up analysis for interprocedural programs, our approach can also handle programs with function calls.
Finally, our approach could be further extended to floating point numbers by considering piecewise affine approximations~\cite{DBLP:conf/esop/Mine04,DBLP:conf/vmcai/Mine06}. 
\end{remark}





