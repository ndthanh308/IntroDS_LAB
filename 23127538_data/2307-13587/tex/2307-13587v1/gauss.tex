
\documentclass[11pt,a4paper, parskip=half]{scrartcl} %option parskip=half
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
	\setlength{\parindent}{15pt}		% indention at beginning of paragraph, default 15pt

\usepackage[runin]{abstract}
    \abslabeldelim{.}
    \renewcommand{\abstractnamefont}{\small\sffamily\bfseries}

\usepackage{todonotes}

\usepackage{amsmath, amsfonts, amssymb, amsthm, amstext}
\usepackage{mathtools}
\usepackage{mathrsfs}
%\usepackage{nicematrix}
\usepackage{mathdots}
\usepackage{cases}
%\usepackage{refcheck}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}

\font\mfett=cmmib10 at11pt
\def\mubf{\hbox{\mfett\char022}} %% fettes \mu
\font\mfetts=cmmib10 at9pt
\def\mubfs{\hbox{\mfetts\char022}} %% fettes \mu
\def\bgamma{\hbox{\mfett\char013}}
\def\gamra{\hbox{\mfett\char013}}
\def\gamras{\hbox{\mfetts\char013}}
\def\berta{\hbox{\mfett\char012}}
\def\balpha{\hbox{\mfett\char011}}
\def\blambda{\hbox{\mfett\char021}}
\def\bslambda{\hbox{\mfetts\char021}}
\def\bphi{\hbox{\mfett\char030}}
\newcommand\mycom[2]{\genfrac{}{}{0pt}{}{#1}{#2}}

\usepackage{enumitem}

\usepackage{hyperref} %, nameref}
\usepackage[capitalise]{cleveref}
	
\newcounter{thm}
\numberwithin{thm}{section}
\numberwithin{equation}{section}

	\newtheoremstyle{myplain}		% name of the style to be used
			{}			% measure of space to leave above the theorem. E.g.: 3pt
			{}			% measure of space to leave below the theorem. E.g.: 3pt
			{\itshape}				% name of font to use in the body of the theorem
			{}				% measure of space to indent
			{\sffamily\bfseries}				% name of head font
			{.}		% punctuation between head and body
			{ }				% space after theorem head; " " = normal interword space
			{\thmname{#1}\thmnumber{ #2}\textnormal{\textsf{\thmnote{ (#3)}}}}			% Manually specify head
    \newtheoremstyle{mybreak}
            {}{}{}{}{\sffamily\bfseries}{.}{\newline}
            {\thmname{#1}\thmnumber{ #2}\textnormal{\textsf{\thmnote{ (#3)}}}}
	\newtheoremstyle{mydef}
			{}{}{}{}{\sffamily\bfseries}{.}{ }
			{\thmname{#1}\thmnumber{ #2}}
	\newtheoremstyle{myrem}
			{}{}{}{}{\sffamily\itshape}{.}{ }
			{\thmname{#1}\thmnumber{ #2}}


\theoremstyle{myplain}
	\newtheorem{theorem}[thm]{Theorem}
	\newtheorem{lemma}[thm]{Lemma}
	\newtheorem{prop}{Proposition}
    \newtheorem{corollary}[thm]{Corollary}
%  	\newtheorem{remark}[thm]{Remark}
\theoremstyle{mybreak}
%	\newtheorem{algorithm}[thm]{Algorithm}
\theoremstyle{mydef}
	\newtheorem{definition}[thm]{Definition}
	\newtheorem{remark}[thm]{Remark}
\theoremstyle{mydef}
% 	\newtheorem{remark}[thm]{Remark}
	\newtheorem{example}[thm]{Example}


	\newcommand{\cc}{\mathbb{C}}
		\newcommand{\zz}{\mathbb{Z}}
		\newcommand{\nn}{\mathbb{N}}
	\newcommand{\rr}{\mathbb{R}}
	\newcommand{\I}{\mathbb{I}}
	\newcommand{\HH}{\mathbf{H}}
	\newcommand{\HHz}{\mathbf{H}_{z}}
	\newcommand{\HHt}{\mathbf{H}_{\triangleright}}
	\newcommand{\PI}{\mathbf{\Pi}}
	
	\newcommand{\x}{\boldsymbol{x}}
	\newcommand{\y}{\boldsymbol{y}}
	\newcommand{\z}{\boldsymbol{z}}
	\newcommand{\vv}{\boldsymbol{v}}
	\newcommand{\uu}{\boldsymbol{u}}

\DeclareMathOperator{\rank}{rank}
%\DeclareMathOperator{\argmax}{argmax}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\allowdisplaybreaks

% primed sum
\def\sumprime_#1^#2{
    \setbox0=\hbox{$\scriptstyle{#1}$}
    \setbox1=\hbox{$\scriptstyle{#2}$}
    \setbox2=\hbox{$\displaystyle{\sum}$}
    \setbox4=\hbox{${}^\prime\mathsurround=0pt$}
    %
    \dimen0=.5\wd0 \advance\dimen0 by-.5\wd2
    \ifdim\dimen0>0pt
        \ifdim\dimen0>\wd4 \kern\wd4
        \else\kern\dimen0
        \ifdim\dimen1>\wd4 \kern\wd4
        \else\kern\dimen1
    \fi\fi\fi
\mathop{{\sum}^\prime}_{\kern-\wd4 #1}^{\kern-\wd4 #2}
}
%

\title{\Large Differential approximation of the Gaussian by short cosine sums with  exponential error decay}
\author{Nadiia Derevianko\footnote{Institute for Numerical and Applied Mathematics, G\"ottingen University, Lotzestr.\ 16-18, 37083 G\"ottingen, Germany, \{n.derevianko,plonka\}@math.uni-goettingen.de} \footnote{Corresponding author} \quad Gerlind Plonka$^{*}$}
\date{\today}

\begin{document}
	\let\oldproofname=\proofname
	\renewcommand{\proofname}{\itshape\sffamily{\oldproofname}}

\maketitle

\begin{abstract}
In this paper we propose a method to approximate the Gaussian function on ${\mathbb R}$ by a short cosine sum.  
We extend the differential approximation method proposed in  \cite{Bell, Scha79}  to approximate $\mathrm{e}^{-t^{2}/2\sigma}$  in the weighted space $L_2({\mathbb R},  \mathrm{e}^{-t^{2}/2\rho})$ where $\sigma, \, \rho >0$.  We prove that the optimal frequency parameters $\lambda_1, \ldots , \lambda_{N}$  for this method in the approximation problem $ \min\limits_{\lambda_{1},\ldots, \lambda_{N},  \gamma_{1}  \ldots \gamma_{N}}\|\mathrm{e}^{-\cdot^{2}/2\sigma} - \sum\limits_{j=1}^{N} \gamma_{j} \, {\mathrm e}^{\lambda_{j} \cdot}\|_{L_{2}({\mathbb R}, \mathrm{e}^{-t^{2}/2\rho})}$,  are zeros of a scaled Hermite polynomial. This observation leads us to a numerically stable approximation method with low computational cost of ${\mathcal O}(N^{3})$ operations. Furthermore, we derive a direct algorithm to solve this approximation problem based on a matrix pencil method for a special structured matrix. The entries of this matrix are determined by hypergeometric functions. 
 For the weighted $L_{2}$-norm, we prove that the  approximation error decays exponentially with respect to the length $N$ of the sum. An exponentially decaying error in the (unweighted) $L^{2}$-norm is achieved using a truncated cosine sum.





\textbf{Keywords:}  sparse exponential sums, sparse cosine sum,  Gaussian function, differential operator, Hermite polynomials, Gauss-Hermite quadrature,  hypergeometric function.\\
\textbf{AMS classification:}
41A20, 42A16, 42C15, 65D15, 94A12.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}


Gaussian functions are widely used in statistics, signal processing,  molecular modeling, computational chemistry,  as well as in approximation theory, see e.g. \cite{Greengard2022,HR10,BCS,  DerPr2020}. However, since the Gaussian is not always simple to handle on finite intervals, an exact approximation of the Gaussian is helpful in different contexts. 
In this paper we propose a method to approximate the Gaussian  $f(t)=\mathrm{e}^{-t^{2}/2\sigma}$ for $\sigma>0$ on the real line ${\mathbb R}$ by short exponential sums. 
In particular, since $f(t)$ is a symmetric function, the obtained  approximation 
is a short cosine sum.

In this paper, we mainly study the approximation problem in the weighted space $L_{2}({\mathbb R}, \rho)$ with the norm
$$
 \| f\|^2_{L_{2}({\mathbb R}, \rho)} := \int_{-\infty}^{\infty} |f(t)|^{2} \, {\mathrm e}^{-t^{2}/2\rho} \, {\mathrm d} t 
$$
for some given $\rho >0$.
To find an exponential sum $y(t)= \sum\limits_{j=1}^{N} \gamma_{j} \, {\mathrm e}^{\lambda_{j} t}$ 
with $\gamma_{j}, \, \lambda_{j} \in {\mathbb C}$
%\setminus \{0\}$, and pairwise distinct $\lambda_{j} \in {\mathbb C}$ 
that optimally approximates $\mathrm{e}^{-t^{2}/2\sigma}$ on ${\mathbb R}$, we have to solve the minimization problem  
$$ \min_{\bslambda \in {\mathbb C}^{N}, \gamras \in {\mathbb C}^{N}} \|\mathrm{e}^{-\cdot^{2}/2\sigma} - \sum_{j=1}^{N} \gamma_{j} \, {\mathrm e}^{\lambda_{j} \cdot}\|_{L_{2}({\mathbb R}, \rho)}.
$$
This optimization is however non-linear and non-convex and therefore very difficult to solve.
Therefore, we will use a suboptimal approximation method, which is sometimes called differential approximation method.

\subsection{Differential approximation method}
\label{sec1.1}
This method, proposed in e.g.\ \cite{Bell, Kam1981,Scha79}, is based on the following observation.
For every exponential sum $y(t)= y_{N}(t)= \sum\limits_{j=1}^{N} \gamma_{j} \, {\mathrm e}^{\lambda_{j} t}$, there exists a differential operator $D_{N}= D_{N}(\blambda)$  given by 
\begin{equation}\label{difoper}
D_{N} f (t):= \frac{d^{N}}{d t^{N}}f(t)+b_{N-1}\frac{d^{N-1}}{d t^{N-1}} f(t)+\ldots+ b_0 f(t), \qquad f  \in C^{N}({\mathbb R}),
\end{equation}
 with constant coefficients $b_{0}, \ldots , b_{N-1} \in {\mathbb C}$ such that 
 $$D_{N} y(t) =0. $$ 
 This differential operator is determined by the coefficients of the monomial representation of the  (characteristic) polynomial
\begin{equation}\label{P} P_{N} (\lambda) = \prod_{j=1}^{N} (\lambda- \lambda_{j}) = \lambda^{N} + \sum_{k=0}^{N-1} b_{k} \, \lambda^{k}. 
\end{equation}
Therefore, if a function $f$ can be well approximated by a short exponential sum, it should be possible to find a differential operator $D_{N}$ such that $D_{N} f$ is small. 
Consequently, we apply the following strategy to approximate $f(t)= {\mathrm e}^{-t^{2}/2\sigma}$ by an exponential sum $y_{N}(t)$.
In a first step, we  determine a polynomial $P_{N}(\lambda)$ of the form (\ref{P}), i.e., we determine the vector ${\mathbf b}=(b_{0}, \ldots , b_{N-1})^{T}$ of coefficients of $P_{N}(\lambda)$ by solving 
\begin{align}\label{minb}
\argmin\limits_{{\mathbf b} \in {\mathbb C}^{N}}\|D_{N} f\|_{L^{2}({\mathbb R}, \rho)}.
\end{align}
Then the zeros $\lambda_{j}$ of the characteristic polynomial $P_{N}(\lambda)$ in (\ref{P}) are taken as the frequencies of the exponential sum to approximate $f$. In a second step, we compute  the vector $\gamra=(\gamma_{1}, \ldots , \gamma_{N})^{T}$ of coefficients of the exponential sum  by solving 
\begin{align}\label{ming}
\argmin\limits_{\gamras \in {\mathbb C}^{N}} \|f - \sum\limits_{j=1}^{N} \gamma_{j} \, {\mathrm e}^{{\lambda_j} \cdot}\|_{L^{2}({\mathbb R}, \rho)}.
\end{align}

\subsection{Contribution of this paper}
As we will show in Subsection \ref{sec:2.1}, surprisingly, the minimization problem 
(\ref{minb}) 
 can be solved analytically for $f(t)= {\mathrm e}^{-t^{2}/2\sigma}$, and the coefficient vector ${\mathbf b}$ can be explicitly given. Moreover, the corresponding  characteristic polynomial $P(\lambda)$ in (\ref{P}) is a normalized scaled Hermite polynomial of degree $N$, such that the zeros $\lambda_{j}$ can be simply precomputed with high accuracy.  For the first time it was noticed in \cite{Bell, Scha79} for $\sigma=\rho=1$ and we extended these ideas for other $\sigma,\rho>0$.
 
 The second minimization problem (\ref{ming}) leads to an equation system of size $N \times N$, where the coefficient matrix is positive definite.
Since $N$ is small, the obtained algorithm requires only a small computational effort while providing very good approximation results.

While searching for optimal parameter vectors $\blambda, \, \gamra \in {\mathbb C}^{N}$, we particularly obtain that the resulting ordered optimal parameters $\lambda_{j}$  are purely imaginary, i.e., $\lambda_{j} \in {\mathrm i} {\mathbb R}$,  and satisfy   $\lambda_{j}= -\lambda_{{N+1-j}}$, while the ordered optimal parameters $\gamma_{j}$ are real,  satisfying $\gamma_{j}= \gamma_{{N+1-j}}$.
In other words, our algorithm yields a cosine sum of length $\lfloor (N+1)/2\rfloor$.  

In Subsection \ref{appreal} we show that the minimization problem (\ref{minb}) can be rewritten as a matrix pencil problem with special matrices whose entries are defined via hypergeometric functions. This observation leads us to  the interesting side result that the eigenvalues of this special matrix pencil (see (\ref{matpen}) in Section  \ref{appreal}) are zeros of scaled Hermite polynomials. Related ideas can be also found  in  \cite{DI2007}, were eigenvalues of some special matrix pencils are approximated by zeros of orthogonal polynomials.  Furthermore, our  result shows the relation between approximation of the Gaussian in the weighted space $L_2({\mathbb R},  \mathrm{e}^{-t^{2}/2\rho})$ and  hypergeometric functions.  This connection was also noticed in  \cite{DerPr2020}, where the Gaussian has been approximated by partial Fourier sums with respect to the spherical Gauss-Laguerre basis.  Moreover matrix pencil approach gives us the opportunity to show that zeros of a scaled Hermite polynomial can be used also for approximation in a finite segment $[-T,T]$ for $T>0$ large enough.
\smallskip

In Section \ref{sec:error}, we show that the proposed method leads to an approximation error 
$$\Big\|\mathrm{e}^{-\cdot^{2}/2\sigma} - \sum\limits_{j=1}^{\lfloor (N+1)/2\rfloor} \tilde{\gamma}_{j} \, \cos( |\lambda_{j}| \cdot) \Big\|_{L_{2}({\mathbb R}, \rho)}
< \textstyle \Big(\frac{r}{\sqrt{2(2r+1)}}\Big)^{N} \, N^{3/4}  
,$$ where $r:=\frac{\rho}{\sigma}$. 
For example, for $r=\frac{1}{2}$, we therefore obtain the error decay rate 
$4^{-N} N^{3/4} < 3^{-N}$,  where $\lfloor (N+1)/2 \rfloor$ is the length of the cosine sum. 
The proof of Theorem \ref{maintheorem} is heavily based on the fact that the Gauss-Hermite quadrature rule leads to exponential  decay rates for Gaussian functions. The proof of Theorem \ref{maintheorem} employs explicitly given suboptimal coefficients $\gamma_{j}$, which are determined by  the weights of the Gauss-Hermite quadrature rule, see formula(\ref{gammaneu}).  Therefore the error estimate ist not sharp.

Since the Gaussian ${\mathrm e}^{-t^{2}/2\sigma}$ itself decays exponentially, we further derive an error estimate in the $L_{2}({\mathbb R})$ norm of the form 
$$ \textstyle \int\limits_{-\infty}^{\infty}  \Big|  {\mathrm e}^{-t^{2}/2\sigma} - \chi_{[-T,T]} {(t)}\, \sum\limits_{k=1}^{\lfloor {(N+1)/2}\rfloor} \tilde{\gamma}_{k} \, \cos(|{\lambda_{k}|t}) \Big|^{2} \, {\mathrm d} t \le  \frac{\tilde{c}}{16^{N/2}} \, N^{3/2}, $$
using a truncated cosine sum, where the choice of $T$ depends on $N$ and $\sigma$.

In Section \ref{sec:4}, we compare our approach of Section \ref{infinseg} with other methods to approximate  the Gaussian by an exponential sum.
We consider a Prony-method based on the differential operator, see \cite{PePl2013, StPl2020}, which uses only function and derivative values of the Gaussian at $t_{0}=0$. Our new approach outperforms both of these methods regarding the error, the numerical stability and the computational effort.

Finally, we consider the numerical methods ESPRIT \cite{RK89,PT13} and ESPIRA \cite{DPP21,DPR23} to approximate $f(t) = {\mathrm e}^{-t^{2}/2\sigma}$, which employ a finite number of equidistant function values of $f$. To achieve a cosine sum as a resulting approximation, one needs to employ corresponding variants of ESPRIT and ESPIRA, see e.g.\ \cite{DPR23}. The original algorithms ESPRIT and ESPIRA both provide very good approximation results, but yield complex exponential sums instead of real cosine sums.



\subsection{Related results}

Exponential sum models are widely used in applied sciences and several algorithms proposed for function approximation by short exponential sums. 
Several authors studied the approximation  of special completely monotone functions, as for example $f(t)= \frac{1}{1+t}$, see \cite{BM05, BM10, BH2005,H19, Kam1976, Kam1979, Kam1981, Scha79}.
Further, the approximation of Bessel functions \cite{BM05,CL20,DPR23}, and of the Dirichlet kernel \cite{BM05,DPP21} has been considered. 
To approximate functions by exponential sums on an interval, often the usual Prony-like reconstruction algorithms can be successfully employed, see e.g.\ \cite{PT13, Pbook}.  Other approaches lead to non-convex minimization problems, which are treated by  iterative methods \cite{OS95, ZP19}. 
As described before, our approach is related to \cite{Kam81, Scha79}, but extends it essentially to derive a stable algorithm and new error estimates for the Gaussian.

Jiang and Greengard \cite{Greengard2022} proposed to approximate the Gaussian on $[0, \infty)$ using the inverse Laplace transform
\begin{align}\label{green}  \frac{1}{\sqrt{{4}\pi t}} {\mathrm e}^{-\frac{|x|^{2}}{4t}} = \frac{1}{2\pi {\mathrm i}}
 \int\limits_{\Gamma} {\mathrm e}^{st} \, \frac{1}{2 \sqrt{s}} \, {\mathrm e}^{-\sqrt{s}|x|} \, {\mathrm d}s, 
\end{align} 
 where $\Gamma$ is a suitable contour.
An exponential sum to approximate the Gaussian is then achieved by discretization of this contour integral. Depending on the chosen contour, exponential error rates of $c^{-\sqrt{N}}$ or even $c^{-N}$ can be achieved numerically. Theoretically, this error decay is not  justified so far since the mentioned quadrature rules, see e.g.\ \cite{TW14,T21} do not apply in the considered case.
Approximations of the Gaussian by  scaling functions and biorthogonal scaling polynomials can be found in \cite{Lee2009}, while in \cite{DerPr2020}  a spherical Gauss-Laguerre basis has been used. 

Unfortunately,  there are not many theoretical results available investigating the corresponding approximation error more closely.
Results by Kammler \cite{Kam1981}, Braess, and Hackbusch \cite{Bbook, Br95, BH09}  show that special completely monotone functions on $[0, \infty)$ or on finite intervals $[a,b] \subset [0, \infty)$ can be approximated by exponential sums with exponential error decay  ${\mathrm e}^{-cN}$ or ${\mathrm e}^{-c\sqrt{N}}$  with respect to different (weighted) norms (including $L_{\infty}([a,b])$-norm \cite{Bbook}, Theorem VI, 3.4, weighted $L_{\infty}([a,b])$-norm \cite{Br95}, weighted $L_{1}([0, \infty))$-norm \cite{Br95}, $L_{2}([0, \infty))$-norm  \cite{Kam1981,Br95}, $L_{2}([a, b])$-norm \cite{Kam1981}).
We note that these results also imply that for example the sinc function, as a  product of a completely monotone function and an exponential sum, can be approximated with exponential decay.
Recently,  Koyama \cite{K23} studied exponential sum approximation for finite completely monotone functions on $[0,\infty)$.
However, for the approximation of the Gaussian on finite intervals or on the real line, we are not aware of any convergence results with exponential decay. This paper gives such decay errors for the first time.


%Finally, we would like to mention further attempts to approximate the Gaussian by exponentials sums or other simple functions.
%Considering the approximation in particular the Gaussian function, there exist several other special approaches.



%Our approach is connected to the so-called differential approximation algorithm (see \cite{Kam1976}-\cite{Kam81}, \cite{Scha79}) but essentially extends those ideas. In \cite{Scha79} the author proved that in case of approximation in $\mathbb{R}$, optimal for the method frequencies can be computed as zeros of scaled Hermite polynomial.  We extend this idea from \cite{Scha79} and show that optimal for the method frequencies $\lambda_{j}$ for approximation of the Gaussian in finite segment can be also computed as zeros of scaled orthogonal polynomials. Moreover, we present differential approximation method as matrix pencil method which shows the connection to Prony-type methods. 



\section{Differential Method for Approximation of the Gaussian on $\mathbb{R}$}
\label{infinseg}

\subsection{Approximation with frequency parameters being zeros of a scaled Hermite polynomial}
\label{sec:2.1}
We want to approximate $f(t)= {\mathrm e}^{-t^{2}/2\sigma}$ for $\sigma>0$ on ${\mathbb R}$ by an exponential sum of length $N$ using the differential approximation method described in Section \ref{sec1.1}.
%Since the Gaussian decays exponentially  for $|t| \to \infty$, we consider here the weighted $L_{2}$-norm 
%$$  \| f\|^2_{L_{2}({\mathbb R}, \rho)} := \int_{-\infty}^{\infty} |f(t)|^{2} \, {\mathrm e}^{-t^{2}/2\rho} \, {\mathrm d} t $$
%for some given $\rho >0$.
%
%As explained above, we will perform the following two steps.
%First, we  compute the differential operator $D_{N}(\tilde{\mathbf b}, f)$ that minimizes $\| D_{N}({\mathbf b}, f)\|_{L_{2}({\mathbb R}, \rho)}$ with respect to all ${\mathbf b} \in {\mathbb C}^{N}
%$ and compute 

\noindent
\textbf{Step 1.}
In the first step, we want to determine a polynomial $P_{N}(\lambda)= \lambda^{N} + \sum_{k=0}^{N-1} b_{k} \lambda^{k}$ where ${\mathbf b}$ is the solution of (\ref{minb}). 
%differential operator $D_{P_{N}}f (t)= \sum_{k=0}^{N} b_{k} \frac{d^{k}}{d t^{k}} f(t)$ (with $b_{N}=1$) 
% $\| D_{N}f\|_{L_{2}({\mathbb R}, \rho)}$  with $D_{N}f$ in (\ref{difoper}) is minimized with respect to ${\mathbf b}= (b_{0}, \ldots , b_{N-1})^{T}$.
Then, the zeros $\lambda_{j}$, $j=1, \ldots , N$, of $P_N$ in (\ref{P})  will serve as the frequencies of the exponential sum to approximate ${\mathrm e}^{-t^{2}/2\sigma}$.

For this purpose, we recall the definition of the  physicist's  Hermite polynomials 
using the Rodrigues formula, see \cite{AS}, 
\begin{equation}\label{rodfor}
H_n(t) :=(-1)^{n} \mathrm{e}^{t^{2}} \frac{\mathrm{d}^{n}}{\mathrm{d}t^{n}}  \mathrm{e}^{-t^{2}}. 
\end{equation}
Its monomial representation is of the form 
\begin{equation}\label{mono}
H_{n}(t) = n! \sum_{\ell=0}^{\lfloor \frac{n}{2} \rfloor} \frac{(-1)^{\ell}}{\ell! (n-2\ell)!} (2t)^{n-2\ell}. 
\end{equation}
The Hermite polynomials $H_{n}$  can be recursively defined with $H_{0}(t) := 1$, $H_{1}(t) := 2t$ and 
\begin{equation}\label{rec} H_{n+1}(t) := {2}t \, H_{n}(t) - H_{n}'(t) = 2 t H_{n}(t) - 2n H_{n-1}(t). 
\end{equation}
Obviously, $H_{n}$ possesses the leading coefficient $2^{n}$.
These Hermite polynomials are orthogonal with respect  to the 
 weight function  $w(t)=\mathrm{e}^{-t^{2}}$, and we have
\begin{equation}\label{orth}
\int\limits_{-\infty}^{\infty} H_n(t) \, H_m(t) \, \mathrm{e}^{-t^{2}} \, \mathrm{d} t=\sqrt{\pi} \, 2^{n}\, n! \, \delta_{n,m},
\end{equation}
where $\delta_{m,n}$ denotes the Kronecker symbol. The Rodrigues formula implies   for $f(t) = {\mathrm e}^{-t^{2}/2\sigma}$  that 
\begin{align}
D_{N} f(t) = \sum_{k=0}^{N} b_{k}  \frac{d^{k}}{d t^{k}} \, {\mathrm e}^{-t^{2}/2\sigma} 
&=  {\mathrm e}^{-t^{2}/2\sigma} \, \sum_{k=0}^{N} b_{k} \, (-1)^{k}(2\sigma)^{-\frac{k}{2}} \,   H_{k} \Big( \frac{t}{\sqrt{2\sigma}} \Big).
\label{2.5}
\end{align}
Let $\rho >0$ be given. For ${\mathbf b}=(b_{0},b_{1}, \ldots, b_{N-1})^{T} \in {\mathbb C}^{N}$ and $b_N=1$,  we consider the functional
\begin{equation}\label{Fbrho0} F_\rho({\mathbf b}):= \| D_{N} {\mathrm e}^{-\cdot^{2}/2\sigma}\|^2_{L_{2}({\mathbb R},\rho)} =
\int_{-\infty}^{\infty}  |D_{N} {\mathrm e}^{-t^{2}/2\sigma} |^{2} \, {\mathrm e}^{-t^{2}/2\rho} \, \mathrm{d}t
\end{equation}
with $D_{N}$ in (\ref{difoper}).

%\noindent
In the following theorem we generalize results from \cite{Scha79} given for $\sigma=\rho=1$ and  provide an explicit presentation of the minimizing vector 
 $\tilde{\mathbf b}= \argmin\limits_{{\mathbf b} \in {\mathbb C}^{N}} F_\rho({\mathbf b})$.  Moreover, we show that the polynomial $\tilde{P}_{N}(\lambda) = \lambda^{N} + \sum\limits_{k=0}^{N-1} \tilde{b}_{k} \lambda^{k}$ is a scaled Hermite polynomial with $N$ symmetric singular zeros on the imaginary axis. 

\begin{theorem}\label{theo1}
For ${\mathbf b} =(b_{0},b_{1}, \ldots, b_{N-1})^{T} \in {\mathbb C}^{N}$  and $\rho >0$ let
 the functional $F_\rho({\mathbf b})$ be determined as in $(\ref{Fbrho0})$.
Then, 
\begin{equation}\label{btil}
\tilde{\mathbf b} = \argmin_{{\mathbf b} \in {\mathbb C}^{N}} F_\rho({\mathbf b})
\end{equation}
is given by $\tilde{\mathbf b}= (\tilde{b}_{0}, \tilde{b}_{1}, \ldots , \tilde{b}_{N-1})^{T}$ with
\begin{align}
\label{bktilde}
\tilde{b}_{k} = \left\{ \begin{array}{ll}  \frac{{N!}}{k! (\frac{N-k}{2})!} \left( \frac{\rho+\sigma}{2\sigma(2\rho+\sigma)} \right)^{(N-k)/2}  & N-k \, \textrm{even},  \\
0 & N-k \, \textrm{odd}. \end{array} \right.
\end{align}
Moreover, we have
\begin{align}
 \tilde{P}_{N}(\lambda) = \lambda^{N} + \sum_{k=0}^{N-1} \tilde{b}_{k} \lambda^{k} =\left( -{\mathrm{i}} \, \sqrt{\frac{\rho+\sigma}{2\sigma(2\rho+\sigma)}}  \right)^{N}\,H_N\left({ \mathrm{i}} \, \sqrt{\frac{\sigma(2\rho+\sigma)}{2(\rho+\sigma)}}  \, \lambda \right) . 
 \end{align}
 \end{theorem}
 
 

\begin{proof} 
1. From (\ref{2.5}) it follows  with $b_N=1$, $\tau := ( \frac{1}{\sigma} + \frac{1}{2\rho})^{1/2} t $ and $c:= (2+ \frac{\sigma}{\rho})^{-1/2}$ that 
\begin{align} \nonumber
F_\rho({\mathbf b}) &=  \textstyle \int\limits_{-\infty}^{\infty}  |D_{N} {\mathrm e}^{-t^{2}/2\sigma} |^{2} \, {\mathrm e}^{-t^{2}/2\rho} \, \mathrm{d}t \\
\nonumber
&=  \textstyle \int\limits_{-\infty}^{\infty} \Big| \sum\limits_{k=0}^{N} (-1)^{k} (2\sigma)^{-\frac{k}{2}} b_{k} \, H_{k}\Big( \frac{t}{\sqrt{2\sigma}} \Big) \Big|^{2} \, {\mathrm e}^{-t^{2}( \frac{1}{\sigma} + \frac{1}{2\rho})} \, {\mathrm d}t \\
\label{Fbrho}
&= \textstyle c \sqrt{2\sigma}  \int\limits_{-\infty}^{\infty} | \sum\limits_{k=0}^{N} (-1)^{k} (2\sigma)^{-\frac{k}{2}}\,   b_{k} \, H_{k}(c \tau) |^{2} \, {\mathrm e}^{-\tau^{2}} \, {\mathrm d} \tau .
 \end{align}
 Now, 
 $ q_N(\tau ) := \sum\limits_{k=0}^{N} b_{k} \, (-1)^{k} (2\sigma)^{-\frac{k}{2}}   \, H_{k}(c \tau) $
 is a polynomial  of degree $N$ with leading  coefficient $(-2c)^N (2\sigma)^{-N/2}$. Therefore, it can  be rewritten in the form
 $$ \textstyle q_N(\tau) =  \Big(\frac{-\sqrt{2}c}{\sqrt{\sigma}}\Big)^N \sum\limits_{\ell=0}^N  \frac{\beta_{\ell}}{2^{N}} H_{\ell}(\tau), $$
 where $\beta_N=1$, since $H_{N}$ has  the leading coefficient $2^{N}$.  With this representation  we obtain from (\ref{Fbrho})
 \begin{align*}
F_\rho({\mathbf b}) &= \textstyle c \sqrt{2\sigma}  \int\limits_{-\infty}^{\infty} | \Big( \frac{- c}{\sqrt{2 \sigma}}\Big)^N  \sum\limits_{\ell=0}^N  \beta_{\ell} H_{\ell}(\tau) |^{2} \, {\mathrm e}^{-\tau^{2}} \, {\mathrm d} \tau  \\
&= \textstyle \Big( \frac{c}{\sqrt{2\sigma}}\Big)^{2N+1}  (2\sigma) \sum\limits_{\ell=0}^N |\beta_{\ell}|^2 \, \int\limits_{-\infty}^{\infty} |H_{\ell}(\tau)|^2 \, {\mathrm e}^{-\tau^{2}} d\tau 
=  \Big(\frac{c}{\sqrt{2\sigma}}\Big)^{2N+1}  (2\sigma) \sum\limits_{\ell=0}^N |\beta_{\ell}|^2  \sqrt{\pi} \, 2^{\ell} \,  \ell!,
\end{align*}
where we have used (\ref{orth}). Therefore, $F_\rho({\mathbf b})$ is minimal if $\beta_{{\ell}}=0$ for $\ell=0, \ldots , N-1$, such that 
\begin{equation}\label{mini} F_\rho(\tilde{\mathbf b}) = \min_{{\mathbf b} \in {\mathbb C}^N} F_\rho({\mathbf b}) = c^{2N+1} (2\sigma)^{-N+1/2} \sqrt{\pi} \, 2^{N}\, N! = c^{2N+1} \sigma^{-N+1/2} \sqrt{2\pi} \, N!
\end{equation}
is achieved for $q_N(\tau) = (- \frac{c}{\sqrt{2\sigma}})^N  H_N(\tau)$. We obtain
$$ \textstyle  H_N(\tau) = \Big( \frac{-\sqrt{2\sigma}}{c}\Big)^{N} \sum\limits_{k=0}^N \tilde{b}_k (-1)^k (2\sigma)^{-\frac{k}{2}} \, H_k(c\tau), $$
or equivalently,
\begin{equation}\label{Hscale}
 \textstyle H_N\Big( \frac{\tau}{c} \Big) = \frac{1}{c^N} \sum\limits_{k=0}^N \tilde{b}_k (-1)^{N-k} (2\sigma)^{\frac{N-k}{2}} H_k(\tau) 
=  \frac{1}{c^N} \sum\limits_{k=0}^N \tilde{b}_{N-k} (-1)^{k} (2\sigma)^{\frac{k}{2}} H_{N-k}(\tau).
\end{equation}
 
\noindent 
2. We observe that the Hermite polynomials  satisfy the scaling property
\begin{equation}\label{scal} 
\textstyle H_{N}(a \tau) = \sum\limits_{r=0}^{\lfloor N/2 \rfloor} \frac{N!}{r! (N-2r)!}  (a^{2}-1)^{r} \, a^{N-2r} \, H_{N-2r}(\tau), \qquad  N\in \mathbb{N},
\end{equation}
for $a \in {\mathbb R}$,
% where  $\lfloor m \rfloor$ denotes the greatest integer less than or equal to $m$,
see e.g. \cite[formula (4.16)]{AW84}.  For $a=c^{-1}$,  comparison with (\ref{Hscale}) yields $\tilde{b}_{N-k}=0$ for odd $k$, and 
for $k=2r$,
$$ \textstyle  \tilde{b}_{N-2r} = \Big( \frac{1-c^2}{2\sigma} \Big)^r \frac{N!}{r!(N-2r)!} = \Big(\frac{\rho+ \sigma}{2\sigma(2\rho+\sigma)}\Big)^r \frac{N!}{r!(N-2r)!} .
$$

\noindent
3. Finally a comparison with (\ref{mono}) implies the representation of the characteristic polynomial $\tilde{P}_{N}(\lambda)$ in (\ref{P}),
\begin{align*}
\tilde{P}_{N}(\lambda)&= \textstyle \sum\limits_{r=0}^{\lfloor N/2 \rfloor} \frac{{N!}}{r! (N-2r)! } \left( \frac{\rho+\sigma}{2 \sigma(2\rho+\sigma)} \right)^{r}  \lambda^{N-2r} 
=   \left(-{\mathrm{i}} \, \sqrt{\frac{\rho+\sigma}{2\sigma(2\rho+\sigma)}}  \right)^{N} \,H_N\left(\mathrm{i} \, \sqrt{\frac{\sigma(2\rho+\sigma)}{2(\rho+\sigma)}} \, \lambda \right) .
\end{align*}

 \end{proof}
 
 \begin{remark}
1. Theorem \ref{theo1} shows that the differential approximation method leads to optimal frequency parameters $\lambda_{j}$, which are zeros of a scaled Hermite polynomial and can therefore be precomputed with high accuracy. It remains to solve the minimization problem (\ref{ming}) in the second step.
Note that if  $\tilde{\mathbf b}$ is defined as in (\ref{bktilde}), it does not mean that  $F_\rho(\tilde{\mathbf b})$ decreases as $N$ increases, see (\ref{mini}).\\
2. For $\rho \to \infty$  we obtain the usual norm in $L_{2}({\mathbb R})$.
In this case,  we can derive from Theorem \ref{theo1} that the optimal frequency parameters obtained by this method are the zeros of $H_{N}(\sqrt{\sigma} {\mathrm i} \lambda)$.  
\end{remark}

\begin{remark}
1. The key point  to obtain the frequency parameters $\lambda_j$, $j=1,\ldots,N$ as zeros of a scaled Hermite polynomial %$H_N\left(\mathrm{i} \, \sqrt{\frac{\sigma(2\rho+\sigma)}{2(\rho+\sigma)}} \, \lambda \right) $, 
is the representation of differential operator $D_N f$ in (\ref{difoper}) by formula (\ref{2.5}). The Hermite polynomials naturally occur in the representation of the derivatives of the Gaussian, as given by the Rodrigues formula (\ref{rodfor}).  Furthermore, the Gaussian function $f(t)=\mathrm{e}^{-t^{2}}$ is also a weight function determining the scalar product that yields the orthogonality property of Hermite polynomials $H_N$ (\ref{orth}).\\
Thus, the question arises,  whether we can also approximate other weight functions $w$ by exponential sums using the differential  method, thereby obtaining suitable frequency parameters as zeros of scaled orthogonal polynomials.
We may consider orthogonal polynomials $p_n$ with respect to a weight function $w$ in the segment $(a,b)$ (or on ${\mathbb R}$)  satisfying
$$
\textstyle \int\limits_a^{b} p_n(t) p_m(t) w(t) \, \mathrm{d} t = \delta_{nm}. 
$$
These polynomials $p_n$ can be also defined by the Rodrigues formula 
\begin{equation}\label{rodgen}
\textstyle p_n(t)=\frac{1}{\alpha_n w(t)} \frac{\mathrm{d}^{n}}{ \mathrm{d} t^{n}} \left(w(t) (q(t))^{n} \right),
\end{equation}
where $\alpha_n$ is some constant and $q$ is some algebraic polynomial of degree at most $2$.
The question is  now the following: %replacing Hermite polynomials $H_N$ by other orthogonal polynomials $p_n$,  
Can we obtain an approximation of weight function $w$ (instead of a Gaussian function) by exponential sums $\sum_{j=1}^{n} \gamma_j \mathrm{e}^{\lambda_j t}$ in the segment $(a,b)$ such that the frequencies $\lambda_j$,  $j=1,\ldots,n$, can be expressed  via zeros of these (scaled) orthogonal polynomials $p_n$? 
The case of approximation of Gaussian functions using the described idea is the simplest one, since we have here $q(t)=1$ in (\ref{rodgen}).  %Therefore, other weight functions and {\color{red}{known}} orthogonal polynomials cannot be considered as simple generalization.

\noindent
 2.  For a sufficiently smooth function $f$ we can always determine the coefficients $b_j$ to minimize $\|D_N f\|_{L_2(\mathbb{R})}$ by applying the Fourier  transform and using the Parseval-Plancherel theorem, see \cite[P.230]{Bell}.
The problem  to determine $D_N(f)$ in (\ref{difoper}) is then equivalent to determining orthogonal polynomials on ${\mathbb R}$ with respect to the weight function $|\widehat{f}|^{2}$.
However, generally this method is very costly, since we first have to construct these orthogonal polynomials (using for example the Gram-Schmidt method) and then to compute their zeros. Only in the special case of Gaussians, we obtain the classical Hermite polynomials.%%% with the Newton method, which is much more expensive than to use Prony method.
%  Therefore we can assume that this method of using scaled zeros of orthogonal polynomials as optimal (for the method) location of frequencies for exponential approximation of some functions works only in case of Gaussians and Hermite polynomials.}}
\end{remark}

Theorem \ref{theo1} shows that the optimal frequencies $\lambda_{j}$, $j=1, \ldots, N$, are the zeros of the Hermite polynomial
$H_{N}\left( {\mathrm i} \sqrt{\frac{\sigma(2 \rho+\sigma)}{2(\rho+\sigma)}} \lambda \right)$. 
It is a trivial observation that for given zeros $t_j$, $j=1, \ldots , N$ of the Hermite polynomial $H_N(t)$, the scaled polynomial $H_{N}\left( {\mathrm i} \sqrt{\frac{\sigma(2 \rho+\sigma)}{2(\rho+\sigma)}} \lambda \right)$ has the zeros 
\begin{align}\label{zeros}  \textstyle \lambda_j =-{\mathrm i} \sqrt{\frac{2(\rho+\sigma)}{\sigma(2\rho+\sigma)}} t_j, \qquad j=1, \ldots , N.
\end{align}
 In particular, it follows that these zeros are all single zeros on the imaginary axis.
In the following, we always assume that the zeros $t_{j}$ of $H_{N}(t)$ are ordered by size, i.e., 
 $$  t_{1} > t_{2} > \ldots  > t_{N}. $$
Then, the symmetry of Hermite polynomials implies  that the zeros are symmetric with regard to zero, i.e., $t_{j} = -t_{N+1-j}$ for $j=1, \ldots , N$. Consequently,
we also have $\lambda_{j} = -\lambda_{N+1-j}$, $j=1, \ldots , N$.
As we will show, the obtained exponential sum to approximate the Gaussian ${\mathrm e}^{-t^{2}/2\sigma}$ is therefore a cosine sum.







%\subsection{Step 2: Computation of coefficients $\gamma_1,\ldots,\gamma_N$}
\noindent
\textbf{Step 2.}  Having determined the frequencies $\lambda_j$, $j=1, \ldots , N$, on the imaginary axis,  we want to determine the vector $\tilde{\gamra}=(\tilde{\gamma}_{1}, \tilde{\gamma}_{2}, \ldots , \tilde{\gamma}_{N})^{T} \in {\mathbb C}^{N}$ of optimal coefficients satisfying 
\begin{equation}\label{minnorm}
\tilde{\gamra} := \argmin_{\gamras \in {\mathbb C}^{N}} F(\gamra) := \argmin_{\gamras \in {\mathbb C}^{N}} \Big\| \mathrm{e}^{-\cdot^{2}/2\sigma} - \sum\limits_{j=1}^{N} \gamma_j \mathrm{e}^{\lambda_j \cdot} \Big\|_{L_2(\mathbb{R}, \rho )}^{2}\, .
\end{equation}
The minimization problem (\ref{minnorm}) is convex (see, for example, \cite[Chapter 1, ยง7]{Cbook}) and can be solved as described in \cite[Chapter 4, ยง1]{Cbook}. We obtain
\begin{align}
F(\gamra) =&
\int_{-\infty}^{\infty} \mathrm{e}^{-t^{2} \left(\frac{1}{\sigma}+\frac{1}{2\rho}  \right)} \,  \mathrm{d} t -\sum_{j=1}^{N}  \gamma_{j} \int_{-\infty}^{\infty} \mathrm{e}^{-t^{2} \left(\frac{1}{2\sigma}+\frac{1}{2\rho} \right) }  \mathrm{e}^{\lambda_j t} \, \mathrm{d} t \notag \\
& -   \sum_{m=1}^{N}  \overline{\gamma}_{m} \int_{-\infty}^{\infty}  \mathrm{e}^{-t^{2} \left(\frac{1}{2\sigma}+\frac{1}{2\rho} \right) }  \mathrm{e}^{\overline{\lambda}_m t} \, \mathrm{d} t   
+ \sum_{j=1}^{N} \sum_{m=1}^{N}  \gamma_{j}  \overline{\gamma}_{m} \int_{-\infty}^{\infty} \mathrm{e}^{-t^{2}/2\rho}  \mathrm{e}^{(\lambda_j+\overline{\lambda}_m ) t} \, \mathrm{d} t . \label{apper1}
\end{align}
We use the formula $\int\limits_{-\infty}^{\infty}  \mathrm{e}^{-at^{2}+bt} \, \mathrm{d}t=\sqrt{\dfrac{\pi}{a}} \, \mathrm{e}^{\frac{b^{2}}{4a}}$ for $a>0$ and  introduce the notations
\begin{align}\label{funcG}
g_{j}  &:= \textstyle \int\limits_{-\infty}^{\infty} \mathrm{e}^{-t^{2} \left(\frac{1}{2\sigma}+\frac{1}{2\rho} \right) }  \mathrm{e}^{\lambda_j t} \, \mathrm{d} t =\sqrt{\frac{2\pi \sigma \rho}{\sigma+\rho}} \, \mathrm{e}^{\frac{\sigma \rho {\lambda}_{j}^{2} }{2(\sigma+\rho)}}, \\
\label{funcH}
H_{j,m}  &:= \textstyle \int\limits_{-\infty}^{\infty} \mathrm{e}^{-t^{2}/2\rho}  \, \mathrm{e}^{(\lambda_j+\overline{\lambda}_m ) t} \, \mathrm{d} t = \sqrt{2\pi \rho} \, \mathrm{e}^{(\lambda_{j}+\overline{\lambda}_{m})^2 \rho/2} = 
 \sqrt{2\pi \rho} \, \mathrm{e}^{(\lambda_{j}-{\lambda}_{m})^2 \rho/2}.
\end{align}
  Since all frequencies $\lambda_{j}$, $j=1, \ldots , N$, in (\ref{zeros}) are purely imaginary values, it follows that $g_j$ and $H_{j,m}= H_{m,j}$ are real.  Taking into account that $\int\limits_{-\infty}^{\infty} \mathrm{e}^{-t^{2} \left(\frac{1}{\sigma}+\frac{1}{2\rho}  \right)} \,  \mathrm{d} t  = \sqrt{\frac{2\pi \rho \sigma}{2\rho + \sigma}}$,  we get
\begin{align*}
F(\gamra) =&  \textstyle \sqrt{\frac{2\pi \rho \sigma}{2\rho + \sigma}}  - \sum\limits_{j=1}^{N} g_j \, (\gamma_j + \overline{\gamma}_j) + \sum\limits_{j=1}^{N}  \sum\limits_{m=1}^{N} H_{j,m} \, \gamma_j \overline{\gamma}_m.
 \end{align*}
Assuming that $\gamma_{j} = \alpha_{j} + {\mathrm i} \beta_{j}$ for $j=1, \ldots , N$, we obtain 
\begin{align}\label{Ferror} 
F(\gamra) =  \textstyle \sqrt{\frac{2\pi \rho \sigma}{2\rho + \sigma}} - 2 \sum\limits_{j=1}^{N} g_j \, \alpha_{j}
+ \sum\limits_{j=1}^{N}\sum\limits_{m=1}^{N} H_{jm} (\alpha_{j}\alpha_{m} + \beta_{j} \beta_{m}).
\end{align}
For the vector $\gamra= \tilde{\gamra}$ in (\ref{minnorm}) that minimizes the functional $F$ we obtain the necessary conditions
\begin{align*}
\frac{\partial  F(\tilde\gamra)}{\partial \alpha_{\ell}} &= - 2 g_{\ell} + 2 \sum_{m=1}^{N} H_{\ell m}  \alpha_{m} =0,  \qquad 
\frac{\partial  F(\tilde\gamra)}{\partial \beta_{\ell}} =  2 \sum_{m=1}^{N} H_{\ell m}  \beta_{m} =0,  \quad \ell=1, \ldots , N.
\end{align*}
In matrix vector representation with ${\mathbf H}_N = (H_{jm})_{j,m=1}^N$, ${\mathbf g} := (g_{j})_{j=1}^{N}$, $\tilde{\gamra} = \balpha + {\mathrm i} \berta= (\tilde\gamma_{j})_{j=1}^{N}$ it follows that 
$$ {\mathbf H}_N \balpha = {\mathbf g}, \qquad {\mathbf H}_N \berta ={\mathbf 0}. $$
For $N>1$, the coefficient matrix ${\mathbf H}_N$ is real, symmetric and positive definite, since for any vector ${\mathbf x} \in {\mathbb R}^N \setminus \{ {\mathbf 0} \}$ we have 
\begin{align*} {\mathbf x}^T {\mathbf H}_N {\mathbf x} &=  \sqrt{2 \pi \rho} \sum_{j=1}^N \sum_{m=1}^N {x}_j {\mathrm e}^{-|\mathrm{Im} \lambda_j - \mathrm{Im}\lambda_m|^2 \rho/2} x_m  \\
& = \sqrt{2 \pi \rho} \sum_{j=1}^N \sum_{m=1}^N {x}_j {\mathrm e}^{-|\lambda_j |^2 \rho/2}  x_m {\mathrm e}^{-|\lambda_m |^2 \rho/2}  \Big( \sum_{\ell=0}^{\infty} \frac{\rho^\ell}{\ell!} (\mathrm{Im}\lambda_j)^\ell (\mathrm{Im} \lambda_m)^\ell \Big)\\
&=  \sqrt{2 \pi \rho} \sum_{\ell=0}^{\infty} \frac{\rho^\ell}{\ell!} \Big(  \sum_{j=1}^{N} {x}_j (\mathrm{Im}\lambda_j)^\ell {\mathrm e}^{-|\lambda_j |^2 \rho/2}  \Big)\Big( \sum_{m=1}^N {x}_m (\mathrm{Im} \lambda_m)^\ell {\mathrm e}^{-|\lambda_m |^2 \rho/2}  \Big) >0.
\end{align*}
Thus, $\berta ={\mathbf 0}$, i.e.,  the minimizing vector $\tilde{\gamra}$ is real and satisfies 
$ {\mathbf H}_N \tilde{\gamra} = {\mathbf g}$.
From $\lambda_{N+1-j} = -\lambda_{j}$ for $j=1, \ldots, N$, it follows that ${\mathbf J}_{N} {\mathbf H}_N {\mathbf J}_{N}={\mathbf H}_N $ and ${\mathbf J}_N {\mathbf g} = {\mathbf g}$, where ${\mathbf J}_{N}= (\delta_{j,N+1-k})_{j,k=1}^{N} $ denotes the counter identity.  Since we have on the one hand 
$$ {\mathbf H}_N \tilde{\gamra} = {\mathbf g}, $$
%$$ ({\mathbf J}_{N} {\mathbf H}_N {\mathbf J}_{N}) ({\mathbf J}_{N} \tilde\gamra) = {\mathbf J}_{N} {\mathbf H}_N  \tilde\gamra  = {\mathbf J}_{N} {\mathbf g} = {\mathbf g}$$
and on the other hand
$$  {\mathbf H}_N ({\mathbf J}_{N} \tilde\gamra) =  ({\mathbf J}_{N} {\mathbf H}_N {\mathbf J}_{N}) ({\mathbf J}_{N} \tilde\gamra) = {\mathbf J}_{N} {\mathbf g} = {\mathbf g}, $$
we can conclude 
that $\tilde{\gamra} = {\mathbf J}_{N} \tilde{\gamra}$, i.e., $\tilde\gamma_{j} = \tilde\gamma_{N+1-j}$ for $j=1, \ldots , N$.
To solve the system ${\mathbf H}_N \tilde{\gamra} = {\mathbf g}$ in a stable way, we can employ a Cholesky decomposition with pivot.

\begin{algorithm}[ht]
\caption{Algorithm for construction of an exponential sum to approximate $\mathrm{e}^{-t^{2}/2\sigma}$  in  $L_2(\mathbb{R},\rho)$}
\label{alg11}
\small{
\textbf{Input:} parameters $\sigma, \rho>0$, $N \in \nn$ the order of the exponential sum;\\
\phantom{\textbf{Input:}} precomputed zeros $t_{1} > t_{2} > \ldots  > t_{N}$ of the Hermite polynomial $H_N(t)$.

\begin{enumerate}
\item Compute the frequencies $\lambda_j := -{\mathrm i} \sqrt{\frac{2(\rho+\sigma)}{\sigma(2\rho+\sigma)}} t_j$, $j=1,\ldots,N$.


\item Compute coefficients  $\gamma_{j }$, $j=1,\ldots,N$ as the solutions of the system:
$$
\sum\limits_{j=1}^{N}  \mathrm{e}^{(\lambda_{j} - \lambda_{k})^2 \rho/2}  \tilde\gamma_j =\sqrt{\frac{ \sigma }{\sigma+\rho}} \, \mathrm{e}^{\frac{\sigma \rho \lambda_{k}^{2} }{2(\sigma+\rho)}}, \ k=1,\ldots,N.
$$
\end{enumerate}

\noindent
\textbf{Output:}  pairwise distinct frequencies $\lambda_j$ with $\lambda_j= -\lambda_{N+1-j}$, $j=1,\ldots,N$;\\
 \phantom{\textbf{Output:}}  coefficients  $\tilde{\gamma}_{j}$, with $\tilde\gamma_j= \tilde\gamma_{N+1-j}$, $j=1,\ldots,N$}  to approximate \\
 \phantom{\textbf{Output:}} $\mathrm{e}^{-t^{2}/2\sigma}$ by 
 $\sum\limits_{j=1}^{N} \tilde\gamma_j \mathrm{e}^{\lambda_j t} $
 in  $L_2(\mathbb{R},  \rho )$.
\end{algorithm}

Note that the zeros of the Hermite polynomials can be pre-computed with high accuracy, see \cite{Salzer,Tow}.
The numerical effort of Algorithm \ref{alg11}  is governed by the computational cost to solve the linear system of $N$ linear equations with $N$ unknowns in step 2. This takes at 
 most $\mathcal{O}(N^{3})$ flops and can be further reduced to $\mathcal{O}(N^{2.376})$ (see  \cite{CW}).
 
 
For the error of the approximation we obtain from (\ref{Ferror}) (with $\tilde{\gamra} = \balpha$)
\begin{align}\nonumber
F(\tilde{\gamra}) &= \textstyle  \Big\| \mathrm{e}^{-\cdot^{2}/2\sigma} - \sum\limits_{j=1}^{N} \tilde\gamma_j \mathrm{e}^{\lambda_j \cdot} \Big\|^{2}_{L_2(\mathbb{R}, \rho )} = \sqrt{\frac{2\pi \rho \sigma}{2\rho + \sigma}} - 2 {\mathbf g}^T \tilde{\gamra} + \tilde{\gamra}^T {\mathbf H}_N \tilde{\gamra}\\
& =
 \textstyle  \sqrt{\frac{2\pi \rho \sigma}{2\rho + \sigma}} - {\mathbf g}^{T} {\mathbf H}_N^{-1} {\mathbf g}. \label{error1}
\end{align}
Application of Algorithm \ref{alg11}  provides an approximation error that decays exponentially if $\rho$ is sufficiently small, see
Figure \ref{fig1} for $\sigma=0.8$ and weights $\rho=1$, $\rho=2$.

% Figure environment removed

\begin{remark}
For the minimization of $F_\rho({\mathbf b})$ in (\ref{Fbrho0}) we can also consider the limit case $\rho \to \infty$, i.e., we can replace the weighted $L_2$-norm by the usual $L_2$-norm. 
However, for the minimization of  (\ref{minnorm}) it is essential to employ $\rho<\infty$. The reason is obvious. Since the zeros of the Hermite polynomial $H_N$  are symmetric around zero and the coefficients $\gamma_j$ satisfy $\tilde\gamma_j= \tilde\gamma_{N+1-j}$, we indeed obtain a cosine sum to approximate ${\mathrm e}^{-t^2/2\sigma}$, i.e., 
$$ {\mathrm e}^{-t^2/2\sigma} \approx 2 \sum_{j=1}^{ \frac{N}{2}}  \tilde\gamma_j \, \cos(|\lambda_j|t) \quad \text{or} \quad  {\mathrm e}^{-t^2/2\sigma} \approx \tilde\gamma_{(N+1)/2} + 2 \sum_{j=1}^{ \frac{N-1}{2}}  \tilde\gamma_j \, \cos(|\lambda_j|t) $$
for even and odd $N$, respectively.
This approximation is only meaningful in the weighted $L_2$ norm, i.e., if we multiply  both sides with  the Gaussian window function ${\mathrm e}^{-t^2/2\rho}$.

Using the symmetry property, the coefficients $\tilde{\gamma}_j$,  $j=1,\ldots,N'$,  $N':=\frac{N}{2}$ for even $N$ and $N':=\frac{N+1}{2}$ for odd $N$,  can be computed  as
\begin{equation}\label{minnormcos}
\tilde{\gamra} = \argmin_{\gamras \in {\mathbb C}^{N}} \Big\|  \mathrm{e}^{-\cdot^{2}/2\sigma} -\sum\limits_{k=1}^{N'}  \gamma_k \, \cos(\lambda_k \cdot) \Big\|_{L_2((0,\infty), \rho)}^{2}\, ,
\end{equation}
where $\lambda_1 > \lambda_{2} > \ldots > \lambda_{N'} >0$ are the positive zeros of the scaled Hermite polynomial $H_{N}\left( \sqrt{\frac{\sigma(2 \rho+\sigma)}{2(\rho+\sigma)}} \lambda \right)$.
Similarly as above we obtain that $\tilde{\gamma}_k$, $k=1,\ldots, N'$, can be computed as solution of the system of linear equations 
$$
\sum \limits_{m=1}^{N'} \tilde{\gamma}_{m} H'_{j,m}=g'_{j}, \qquad j=1,\ldots,N',
$$
where we apply the formulas \cite[3.896(4), 3.898(2)]{GR} for $\beta>0$,
\begin{align*}
\textstyle \int\limits_0^{\infty} \mathrm{e}^{-\beta t^{2}}  \cos(bt)  \mathrm{d}t  = \textstyle \frac{1}{2} \sqrt{\frac{\pi}{\beta}}  \mathrm{e}^{-\frac{b^{2}}{4 \beta}}, \quad %\notag\\
\textstyle  \int\limits_0^{\infty} \mathrm{e}^{-\beta t^{2}}  \cos(bt)  \cos(at) \mathrm{d}t = \textstyle \frac{1}{4} \sqrt{\frac{\pi}{\beta}} \Big( \mathrm{e}^{-\frac{(a-b)^{2}}{4 \beta}} +  \mathrm{e}^{-\frac{(a+b)^{2}}{4 \beta}} \Big),
\end{align*}
to find
\begin{align*}
g'_{j}  :=& \textstyle \int\limits_0^{\infty} \mathrm{e}^{-t^{2}(\frac{1}{2\sigma}+\frac{1}{2\rho})} \, \cos(\lambda_j t) \, \mathrm{d}t=\sqrt{\frac{\pi \sigma \rho}{2(\rho+\sigma)}} \, \mathrm{e}^{-\frac{(\lambda_j)^{2}\sigma \rho}{2(\rho+\sigma)}},\notag\\
H'_{j,m}:=& \textstyle \int\limits_0^{\infty} \mathrm{e}^{-t^{2}/2\rho} \, \cos(\lambda_j t) \, \cos(\lambda_m t) \, \mathrm{d}t=\sqrt{\frac{\pi \rho}{8}} \Big(  \mathrm{e}^{-\frac{\rho(\lambda_j-\lambda_m)^{2}}{2}}+  \mathrm{e}^{-\frac{\rho(\lambda_j+\lambda_m)^{2}}{2}} \Big).
\end{align*}

\end{remark}

\begin{remark}
We observe that the coefficient vector  $\tilde\gamra =(\tilde\gamma_j)_{j=1}^N$ in (\ref{minnorm}), determined by ${\mathbf H} \tilde{\gamra} = {\mathbf g}$, only depends on the quotient $r := \rho/\sigma$, since  $(2\pi \rho)^{-1/2} {\mathbf H}$ as well as $(2\pi \rho)^{-1/2}{\mathbf g}$ only depend on $r$. 
This can be seen as follows.
Let again $t_1 > t_{2} > \ldots > t_{N}$ be the zeros of $H_N$ and $\lambda_j=-\mathrm{i}\sqrt{\frac{2(\rho+\sigma)}{(2\rho+\sigma)\sigma}} t_j$ for $j=1,\ldots,N$. 
Then, with $\rho=r\sigma$ we obtain for the components of $(2\pi \rho)^{-1/2}{\mathbf H}$ in (\ref{funcH}) that 
\begin{align}
	 \mathrm{e}^{(\lambda_{j}-{\lambda}_{k})^2 \frac{\rho}{2}}= 
	% \mathrm{e}^{\left(-\mathrm{i}\sqrt{\frac{\rho+\sigma}{(2\rho+\sigma)\sigma}}(t_j-t_k)\right)^2 \rho/2}  = 
	  \mathrm{e}^{-{2}\frac{\rho+\sigma}{(2\rho+\sigma)\sigma}(t_j-t_k)^2 \frac{\rho}{2}}  =  \mathrm{e}^{-\frac{(r+1)r}{{2r+1}}(t_j-t_k)^2}   
	\label{Hjk}
\end{align}
and for the components of $(2\pi \rho)^{-1/2}{\mathbf g}$ in (\ref{funcG}),
\begin{align}
\frac{1}{\sqrt{2\pi \rho}} g_{j} =	\textstyle \sqrt{\frac{ \sigma }{\sigma+\rho}} \, \mathrm{e}^{\frac{\sigma \rho {\lambda}_{j}^{2} }{2(\sigma+\rho)}} 
	 = \sqrt{\frac{ \sigma }{\sigma+\rho}} \, \mathrm{e}^{-\frac{\rho}{{2\rho+\sigma}} t_j^2} 
	 = \sqrt{\frac{ 1 }{1+r}} \, \mathrm{e}^{-\frac{r}{{2r+1}} t_j^2},
	 \label{gjneu}
\end{align}
such that the coefficients $\tilde\gamma_j$ are the solution of the linear system
\begin{align} \label{system1}
	 \textstyle \sum\limits_{j=1}^{N} \tilde{\gamma}_j \,  \mathrm{e}^{-\frac{(r+1)r}{2r+1}{(t_j-t_k)^2}}    = \sqrt{\frac{ 1 }{1+r}} \, \mathrm{e}^{-\frac{r}{{2r+1}}t_k^2}, \qquad k=1,\ldots,N.
\end{align}
\end{remark}


\subsection{Differential approximation method as a matrix pencil method}
\label{appreal}

Instead of using Theorem \ref{theo1}, we can solve the minimization problem $\tilde{\mathbf b} = \argmin_{\mathbf b} F_{\rho}({\mathbf b}) $ with the functional $F_{\rho}({\mathbf b}) $ in (\ref{Fbrho0}) directly. From (\ref{Fbrho}) it follows that 
\begin{align*}
F_{\rho}({\mathbf b}) &=  \textstyle c \sqrt{2\sigma}  \sum\limits_{j=0}^{N} \sum\limits_{m=0}^{N} (-1)^{j+m} (2\sigma)^{-(\frac{j+m}{2})} \, b_{j} \overline{b}_{m} \,  \int\limits_{-\infty}^{\infty} H_{j}(c\tau) \, H_{m}(c\tau) \, {\mathrm e}^{-\tau^{2}} d\tau \\
&= \textstyle \sum\limits_{j=0}^{N} \sum\limits_{m=0}^{N} A_{j,m} \, b_{j} \overline{b}_{m},
\end{align*}
where $c= (2+ \frac{\sigma}{\rho})^{{-1/2}} \neq 1$ and 
\begin{equation}\label{coefajk}
 A_{j,m} := c \sqrt{2\rho} (-1)^{j+m} \, (2\sigma)^{-(\frac{j+m}{2})} \int_{-\infty}^{\infty} H_{j}(c\tau) \, H_{m}(c\tau) \, {\mathrm e}^{-\tau^{2}} d\tau. 
\end{equation}
The minimization of $F_{\rho}({\mathbf b})$ then yields the linear system
$$ \textstyle \sum\limits_{m=0}^{N} A_{j,m} \, \tilde{b}_{m} =0, \qquad j=0, \ldots , N-1. $$
or in matrix form, 
\begin{equation}\label{asysdif}
{\mathbf A}_{N,N+1}  \tilde{\mathbf{b}} =0.
\end{equation}
with 
${\mathbf A}_{N, N+1} = (A_{j,m})_{j,m=0}^{N-1,N}$, $\tilde{\mathbf b}= (\tilde{b}_{0}, \ldots , \tilde{b}_{N-1}, 1)^{T}$. Since we are interested in the zeros of the characteristic polynomial $P_{N}(\lambda) = \lambda^{N}+ \sum\limits_{m=0}^{N-1} \tilde{b}_{k} \, \lambda^{m}$ as in (\ref{P}), we apply the matrix pencil method. 
%To this end we rewrite  
%$$
% F_\rho({\mathbf b})=\sum\limits_{j=0}^{N}  \sum\limits_{m=0}^{N} (-1)^{j+m} \sigma^{-\frac{j+m}{2}} b_j \overline{b}_m \int\limits_{-\infty}^{\infty}  He_j\left(\frac{t}{\sqrt{\sigma}} \right) He_m \left(\frac{t}{\sqrt{\sigma}} \right)   \, \mathrm{e}^{-t^{2}\left( \frac{1}{\sigma}+\frac{1}{2\rho} \right)  } \, \mathrm{d} t.
% $$
% Defining $A_{jm}:=(-1)^{j+m}   \sigma^{-\frac{j+m}{2}}  \int\limits_{-\infty}^{\infty}  He_j\left(\frac{t}{\sqrt{\sigma}} \right) He_m \left(\frac{t}{\sqrt{\sigma}} \right)  \, \mathrm{e}^{-t^{2}\left( \frac{1}{\sigma}+\frac{1}{2\rho} \right)  } \, \mathrm{d} t$, we get
% $$
% F_\rho({\mathbf b})=\sum\limits_{j=0}^{N}  \sum\limits_{m=0}^{N} A_{jm} b_j \overline{b}_m.
% $$
%We take all partial derivatives of the multivariate function $F$ with respect to variables $b_{j_0}=\alpha_{j_0}+\mathrm{i} \beta_{j_0}$, $\alpha_{j_0}, \beta_{j_0}\in \rr$, $j_0=0,...,N-1$,
%\begin{align*}
%\frac{\partial F}{\partial \alpha_{j_0}} & =\sum\limits_{j=0}^{N} b_j A_{j j_0}+ \sum\limits_{m=0}^{N} \overline{b}_m A_{j_0 m}=0, \\
%\frac{\partial F}{\partial \beta_{j_0}} & =(-\mathrm{i}) \sum\limits_{j=0}^{N} b_j A_{j j_0}+\mathrm{i} \sum\limits_{m=0}^{N} \overline{b}_m A_{j_0 m}=0.
%\end{align*}
%Multiplying $\frac{\partial F}{\partial \beta_{j_0}}$ on $(-\mathrm{i})$ and adding to $\frac{\partial F}{\partial \alpha_{j_0}}$ we get 
%$$
%\sum\limits_{m=0}^{N} b_m A_{j_0 m}=0, \ j_0=0,...,N-1.
%$$
%We define a matrix  
%\begin{equation}\label{amatrix}
%{\mathbf A}_{N,N+1} =\left( A_{jm} \right)_{j,m=0}^{N-1,N}.
%\end{equation}
% The a vector $\tilde{\mathbf b}=(b_0,\ldots,b_{N-1},1)$ that minimizes $ F_\rho({\mathbf b})$ can be found as  a solution of the system
%\begin{equation}\label{asysdif}
%{\mathbf A}_{N,N+1}  \mathbf{b}^{T} =0.
%\end{equation}
%Then frequencies $\lambda_1,...,\lambda_N$ are computed as zeros of the characteristic polynomial (\ref{P}). Further we can simplify the method of computation of frequencies $\lambda_1,...,\lambda_N$ using a matrix pencil method. 

We define the two matrices ${\mathbf A}_{N}(0)=\left(A_{j,m} \right)_{j,m=0}^{N-1}$
and  ${\mathbf A}_{N}(1)=\left(A_{j,m+1} \right)_{j,m=0}^{N-1}$.
%for $s=0,1$. Matrix ${\mathbf A}_N(0)$ is obtained from matrix ${\mathbf A}_{N,N+1}$ if we remove last column and matrix ${\mathbf A}_N(1)$ is obtained from matrix ${\mathbf A}_{N,N+1}$ if we remove first column. 
Let the companion matrix ${\mathbf C}_N(\tilde{\mathbf{b}})$ of $P_{N}(\lambda)$  be given by 
\begin{equation}\label{commatrix}
{\mathbf C}_N(\tilde{\mathbf b}) =
\left( \begin{array}{ccccc}
0 & 0 & \ldots & 0& -\tilde{b}_0 \\
1 & 0& \ldots  &  0 & -\tilde{b}_1 \\
0 &  1 &  \ldots & 0&  -\tilde{b}_2 \\
\vdots & \vdots &  \vdots &\ddots & \vdots  \\
0 & 0 &  \ldots & 1  & -\tilde{b}_{N-1}
\end{array} \right). 
\end{equation}
%  we have 
%\begin{equation}\label{propcom}
%\mathrm{det}(\lambda {\mathbf I}_N -{\mathbf C}_N(\mathbf{b}) )=P_N(\lambda),
%\end{equation}
%i.e.,  

Then the companion matrix ${\mathbf C}_N(\mathbf{b})$  has the property 
\begin{equation}\label{propcom}
\mathrm{det}(\lambda {\mathbf I}_N -{\mathbf C}_N(\tilde{\mathbf b}) )=P_N(\lambda).
\end{equation}
Using (\ref{asysdif}) we observe that 
$$
{\mathbf A}_N(0){\mathbf C}_N(\tilde{\mathbf{b}}) ={\mathbf A}_N(1).
$$
Taking into account (\ref{propcom}) we find the zeros $\lambda_1, \, \lambda_2,\ldots, \lambda_N$ of the polynomial $P_N(\lambda)$ by computing the eigenvalues of the matrix pencil 
\begin{equation}\label{matpen}
\lambda {\mathbf A}_N(0)-{\mathbf A}_N(1).
\end{equation}

To improve the numerical stability of this computation we employ the singular value decomposition (SVD) of the matrix   ${\mathbf A}_{N,N+1}$ of the form 
$${\mathbf A}_{N,N+1}  = {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1}$$ 
with  orthogonal matrices ${\mathbf U}_{N} \in {\mathbb R}^{N \times N}$  and  ${\mathbf W}_{N+1} \in {\mathbb R}^{(N+1) \times (N+1)}$.
Then, (\ref{matpen}) can be rewritten as 
\begin{equation}\label{matpen2}
\lambda \,  {\mathbf W}_{N}(0) - {\mathbf W}_{N}(1)
\end{equation}
with the submatrices ${\mathbf W}_{N}(0)={\mathbf W}_{N+1}(1:N,1:N) $  and ${\mathbf W}_{N}(1)={\mathbf W}_{N+1}(1:N,2:N+1)$, where we have used the usual Matlab notation for rows and columns. 



\begin{algorithm}[ht]
\caption{Differential approximation algorithm with MPM for construction of an exponential sum to approximate $\mathrm{e}^{-t^{2}/2\sigma}$ in the space $L_2(\mathbb{R},  \rho )$  }
\label{alg1}
\small{
\textbf{Input:} parameters $\sigma, \rho>0$, $N \in \nn$ the order of an exponential sum;


\begin{enumerate}
\item Create a matrix ${\mathbf A}_{N,N+1}$  with entries in (\ref{coefdef}) and compute the SVD\\ ${\mathbf A}_{N,N+1}  = {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1}$.  

\item Compute the frequencies  $\lambda_1,\ldots ,\lambda_N$ as eigenvalues of   $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}  {\mathbf W}_{N}(1)^{T},$
where $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}$ denotes the Moore-Penrose inverse of  ${\mathbf W}_{N}(0)^{T}$.


\item Compute coefficients  $\gamma_{j }$, $j=1, \ldots ,N$ as the solutions of the system:
$$
\textstyle \sum\limits_{j=1}^{N}  \mathrm{e}^{(\lambda_{j}+\overline{\lambda}_{k})^2 \rho/2}  \gamma_j =\sqrt{\frac{ \sigma }{\sigma+\rho}} \, \mathrm{e}^{\frac{\sigma \rho \overline{\lambda}_{k}^{2} }{2(\sigma+\rho)}}, \qquad k=1,\ldots,N.
$$
\end{enumerate}

\noindent
\textbf{Output:} frequencies $\lambda_j$ %with $\lambda_j= -\lambda_{N+1-j}$, 
$j=1,\ldots,N$; 
 coefficients  $\tilde{\gamma}_{j}$, 
 %with $\tilde\gamma_j= \tilde\gamma_{N+1-j}$, 
 $j=1,\ldots,N$}  to approximate \\
 \phantom{\textbf{Output:}} $\mathrm{e}^{-t^{2}/2\sigma}$ by 
 $\sum\limits_{j=1}^{N} \tilde\gamma_j \mathrm{e}^{\lambda_j t} $
 in the space $L_2(\mathbb{R},  \rho )$.
\end{algorithm}

%The matrix pencil problem (\ref{matpen}) can be further simplified. We employ an  SVD of the matrix  ${\mathbf A}_{N,N+1} $ ,
%\begin{equation}\label{SVD1}
%{\mathbf A}_{N,N+1}  = {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1}, 
%\end{equation} 
%where ${\mathbf U}_{N}$ and ${\mathbf W}_{N+1}$  are unitary  square matrices  and
%$$ {\mathbf D}_{N,N+1} = \left( \begin{array}{cc} \mathrm{diag} \left(  (\sigma_{j})_{j=1}^{N} \right) & {\mathbf 0}  \end{array} \right) \in {\mathbb R}^{N \times (N+1)}, $$
%where $\sigma_{1}, \ldots , \sigma_{N}$ denote the positive singular values of ${\mathbf A}_{N,N+1}$ in decreasing order.
%Similarly the singular value decompositions of ${\mathbf A}_{N}(0)$ and ${\mathbf A}_{N}(1)$ can be directly  derived  from (\ref{SVD1})  and the matrix pencil in (\ref{matpen}) now reads
%\begin{eqnarray*}
% & & \lambda \left(  {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,1:N)  \\
% & &  \hspace{5cm} -  \left(  {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,2:N+1),
%\end{eqnarray*}
%or equivalently
%$$ \lambda \left(   {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,1:N) -  \left(  {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,2:N+1). $$
%Multiplication from the left with $\left( \mathrm{diag}( \sigma_{j}^{-1} )_{j=1}^{N} , {\mathbf 0} \right) \in {\mathbb R}^{N \times (N+1)}$  yields
%the matrix pencil
%$$
%\lambda \,  {\mathbf W}_{N+1}(1:N,1:N) - {\mathbf W}_{N+1}(1:N,2:N+1)
%$$
%or
%\begin{equation}\label{matpen2}
%\lambda \,  {\mathbf W}_{N}(0) - {\mathbf W}_{N}(1)
%\end{equation}
%if we use notations ${\mathbf W}_{N}(s)={\mathbf W}_{N+1}(1:N,1+s:N+s) $ for $s=0,1$.
%Now frequencies $\lambda_1,\ldots,\lambda_N$ can be found as eigenvalues of the matrix $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}  {\mathbf W}_{N}(1)^{T},$
%where $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}$ denotes the Moore-Penrose inverse of  ${\mathbf W}_{N}(0)^{T}$.
%

What still remains is the computation of the entries $A_{j,m}$ of the matrix ${\mathbf A}_{N,N+1}$. We  use the following formula \cite[7.374(5)]{GR} 
\begin{align} \nonumber 
 & \textstyle \int\limits_{-\infty}^{\infty} \mathrm{e}^{-2\alpha^{2}t^{2}} H_j(t) H_m(t)\, \mathrm{d} t \\
 & \qquad = \textstyle 2^{\frac{m+j-1}{2}}
\alpha^{-m-j-1} (1-2\alpha^{2})^{\frac{m+j}{2}} \Gamma\left( \frac{m+j+1}{2} \right) \,  _2F_1\left(-m,j,\frac{1-m-j}{2},\frac{\alpha^{2}}{2 \alpha^{2}-1}\right) \label{int1}
\end{align}
if $j+m$ is even, $\alpha \neq \frac{1}{2}$ and $\int_{-\infty}^{\infty} \mathrm{e}^{-2\alpha^{2}t^{2}} H_j(t) H_m(t)\, \mathrm{d} t =0$ if $j+m$ is odd,
where $\Gamma$ is the Gamma function and $_2F_1$ is the hypergeometric function that can be defined for example by the series
$$
\textstyle _2F_1(a,b;c,z)= \sum\limits_{n=0}^{\infty} \frac{ (a)_n (b)_n}{(c)_n} \frac{z^{n}}{n!}, \ |z|<1,
$$
with $(x)_n$ be a Pochhammer symbol. 
Applying (\ref{int1}) to compute integral in (\ref{coefajk}),  we obtain the explicit representation
\begin{equation}\label{coefdef}
A_{j,m}= \textstyle 
%\begin{cases}
(-1)^{\frac{m+j}{2}}  \sqrt{\frac{2 \rho \sigma}{2\rho+\sigma}}  \left( \frac{2(\rho+\sigma)}{\sigma(2\rho+\sigma)}  \right)^{\frac{m+j}{2}} \Gamma\left( \frac{m+j+1}{2} \right)   \,  _2F_1\left(-m,j,\frac{1-m-j}{2},\frac{2\rho+\sigma}{2(\rho+\sigma)} \right) 
\end{equation}
if $j+m$ is even and $A_{j,m}=0$ if $j+m$ is odd.
Interestingly,  these hypergeometric functions $_2F_1$ are also involved in the representation of the approximation error in \cite{DerPr2020}, where the problem of weighted $L_2(\mathbb{R}^{3},  \rho )$ approximation of $\mathrm{e}^{-t^{2}/2\sigma}$ by so-called spherical Gauss-Laguerre basis was considered.

\noindent
The obtained algorithm is summarized in Algorithm  \ref{alg1}.



\begin{remark}\label{zeros}
1.  As a corollary of Theorem \ref{theo1} and our observations in this section it follows that the eigenvalues of the matrix pencil (\ref{matpen})  with entries (\ref{coefdef}) are the zeros of  the Hermite polynomials $H_{N}\left( {\mathrm i} \sqrt{\frac{\sigma(2 \rho+\sigma)}{2(\rho+\sigma)}} \lambda \right)$  (compare with second formula in the proof of Theorem 4.1 in \cite{DI2007}).
\end{remark}


\noindent
Since the Gaussian decays exponentially one may also consider its approximation on a finite interval $[-T,T]$ with properly chosen $T$ (see also Theorem \ref{th37}).  In the last part of this section, we  show that for sufficiently large $T$, the  zeros of $H_{N}\left( {\mathrm i} \sqrt{\frac{\sigma(2 \rho+\sigma)}{2(\rho+\sigma)}} \lambda \right)$ are still suitable frequency parameters. 

\noindent
To this end we rewrite 
the entries $A_{j,m}$ in a different form.
First observe that for $k \in {\mathbb N}$, 
%\begin{equation}\label{form11}
\begin{align} \nonumber
\textstyle \int\limits_{-\infty}^{\infty} \mathrm{e}^{-\alpha^{2} t^{2}} t^k \, \mathrm{d} t
&= \textstyle  (1+(-1)^k) \,  \int\limits_{0}^{\infty} \mathrm{e}^{-\alpha^{2} t^{2}} t^k \, \mathrm{d} t = \left(\frac{1}{\alpha} \right)^{k+1}  (1+(-1)^k) \,  \int\limits_{0}^{\infty} \mathrm{e}^{- t^{2}} t^k \, \mathrm{d} t   \\
&=\textstyle \frac{1}{2} \, \left(\frac{1}{\alpha} \right)^{k+1}  (1+(-1)^k) \,  \int\limits_{0}^{\infty} \mathrm{e}^{- t} \, t^{\frac{k+1}{2}-1} \, \mathrm{d} t  \nonumber  \\ 
&= \textstyle \frac{1}{2} \left( \frac{1}{\alpha}\right)^{k+1} (1+(-1)^{k} )\, \Gamma\left(\frac{k+1}{2} \right), \label{form11}
\end{align}
where the definition  $\Gamma(z)=\int\limits_0^{\infty} t^{z-1} \mathrm{e}^{-t} \, \mathrm{d}t$ of the Gamma function has been applied in the last step.
 Using now the explicit  representation (\ref{mono}) of Hermite  polynomials  in  (\ref{coefajk}),  we compute entries $A_{j,m}$ with (\ref{form11}) as 
\begin{align}\label{coefdef2nd}
A_{j,m}= \textstyle \sqrt{\frac{2 \sigma \rho}{2 \rho+\sigma}}    \left(\frac{2  \rho}{2 \rho+\sigma} \right)^{\frac{j+m}{2}} \sum\limits_{k=0}^{\lfloor j/2 \rfloor}  \sum\limits_{\ell=0}^{\lfloor m/2 \rfloor} \frac{(-1)^{k+\ell}  j! m! }{k! \ell! (j-2k)! (m-2\ell)! }  \left(\frac{2 \rho+\sigma}{4 \rho} \right)^{k+\ell} \Gamma\left( \frac{j+m-2k-2\ell+1}{2} \right)
\end{align}
if $j+m$ is even and $A_{j,m}=0$ if $j+m$ is odd.

\noindent

Let us now compute entries $A_{j,m}(T)$  which are defined as in (\ref{coefajk}) but instead of $(-\infty,\infty)$ we consider the interval $(-T,T)$. Similarly as above, we obtain for $a<0$, $b>0$, $k \in {\mathbb N}$, with the substitution $\eta t^{2}=t$,
\begin{align} \nonumber
& \textstyle   \int\limits_a^{b} \mathrm{e}^{-\eta \, t^{2}} t^{k} \, \mathrm{d}t
= \textstyle  \frac{1}{2\, {\eta}^{(k+1)/2}}  \left( (-1)^{k} \int\limits_{0}^{\eta \, a^{2}}  \mathrm{e}^{-t} t^{(k-1)/2} \, \mathrm{d}t+  \int\limits_0^{\eta \, b^{2}} \mathrm{e}^{-t} t^{(k-1)/2} \, \mathrm{d}t   \right)\\
\nonumber
& = \textstyle \frac{1}{2 \, {\eta}^{(k+1)/2}}  \Big( ( (-1)^{k}+1) \int\limits_{0}^{\infty}  \mathrm{e}^{-t} t^{\frac{k+1}{2}-1} \, \mathrm{d}t - (-1)^{k} \int\limits_{\eta \, a^{2}}^{\infty}  \mathrm{e}^{-t} t^{\frac{k+1}{2}-1} \, \mathrm{d}t -  \int\limits_{\eta \, b^{2}}^{\infty}  \mathrm{e}^{-t} t^{\frac{k+1}{2}-1} \, \mathrm{d}t  \Big)\\
&= \textstyle \frac{1}{2\, {\eta}^{(k+1)/2}}\left( (1+(-1)^{k}) \, \Gamma\left(\frac{k+1}{2} \right)-(-1)^{k} \Gamma\left(\frac{k+1}{2} ,\eta \, a^{2}\right) -\Gamma\left(\frac{k+1}{2} ,\eta \, b^{2}\right)  \right),\label{form1}
\end{align}
where  $\Gamma(z)$ is again the Gamma function and $\Gamma(z,a)=\int\limits_a^{\infty} t^{z-1} \mathrm{e}^{-t} \, \mathrm{d}t$ is  the (upper) incomplete Gamma function for $\mathrm{Re} \, z>0$. 
%\begin{equation}\label{form1}
%\textstyle \int\limits_a^{b} \mathrm{e}^{-\eta \, t^{2}} t^{k} \, \mathrm{d}t=\frac{1}{2\sqrt{\eta}} \left(\sqrt{\frac{1}{\eta}} \right)^{k} \left( (1+(-1)^{k}) \, \Gamma\left(\frac{k+1}{2} \right)-(-1)^{k} \Gamma\left(\frac{k+1}{2} ,\eta \, a^{2}\right) -\Gamma\left(\frac{k+1}{2} ,\eta \, b^{2}\right)  \right),
%\end{equation}
%where $\Gamma(z)$ is again the Gamma function and $\Gamma(z,a)=\int\limits_a^{\infty} t^{z-1} \mathrm{e}^{-t} \, \mathrm{d}t$ is  the (upper) incomplete Gamma function for $\mathrm{Re} \, z>0$.  Indeed, making changes of variables $\eta t^{2}=t$ we write
%\begin{align*}
%\textstyle \int\limits_a^{b} \mathrm{e}^{-\eta \, t^{2}} t^{k} \, \mathrm{d}t
%=& \textstyle  \frac{1}{2{\eta}^{(k+1)/2}}  \left( (-1)^{k} \int\limits_{0}^{\eta \, a^{2}}  \mathrm{e}^{-t} t^{(k-1)/2} \, \mathrm{d}t+  \int\limits_0^{\eta \, b^{2}} \mathrm{e}^{-t} t^{(k-1)/2} \, \mathrm{d}t   \right)\\
%=& \textstyle \frac{1}{2{\eta}^{(k+1)/2}}  \left( ( (-1)^{k}+1) \int\limits_{0}^{\infty}  \mathrm{e}^{-t} t^{\frac{k+1}{2}-1} \, \mathrm{d}t - (-1)^{k} \int\limits_{\eta \, a^{2}}^{\infty}  \mathrm{e}^{-t} t^{\frac{k+1}{2}-1} \, \mathrm{d}t -  \int\limits_{\eta \, b^{2}}^{\infty}  \mathrm{e}^{-t} t^{\frac{k+1}{2}-1} \, \mathrm{d}t  \right)
%\end{align*}
%and then using definitions of the Gamma function and the incomplete Gamma function we get needed formula. 
Then, as before, the explicit representation of Hermite polynomials in (\ref{mono}) yields that $A_{j,m}(T)=0$ if $j+m$ is odd, while for  $j+m$ even we obtain
\begin{align}
A_{j,m}(T)=& \textstyle \sqrt{\frac{2 \sigma \rho}{2 \rho+\sigma}}    \left(\frac{2  \rho}{2 \rho+\sigma} \right)^{\frac{j+m}{2}} \sum\limits_{k=0}^{\lfloor j/2 \rfloor}  \sum\limits_{\ell=0}^{\lfloor m/2 \rfloor} \frac{(-1)^{k+\ell}  j! m! }{k! \ell! (j-2k)! (m-2\ell)! }  \left(\frac{2 \rho+\sigma}{4 \rho} \right)^{k+\ell} \notag \\
 & \textstyle \times 
\Big(  \Gamma\left( \frac{j+m-2k-2\ell+1}{2} \right) - \Gamma\left( \frac{j+m-2k-2\ell+1}{2}, \frac{2 \rho+\sigma}{2 \sigma \rho} T^{2} \right)\Big) . \label{ajmfin1}
\end{align}
The property $\lim\limits_{t \to \infty} \Gamma(z,t)=0$ of the incomplete Gamma function together with (\ref{coefdef2nd}) leads to %and (\ref{ajmfin1}) we get that 
$$
\lim\limits_{T\rightarrow \infty} A_{j,m}(T)=A_{j,m}.
$$
Taking into account Remark \ref{zeros} we conclude that the zeros of the scaled Hermite polynomial $H_{N}\left( {\mathrm i} \sqrt{\frac{\sigma(2 \rho+\sigma)}{2(\rho+\sigma)}} \lambda \right)$ are also suitable to approximate the Gaussian in $[-T,T]$, if  $T$ is sufficiently large.


% Figure environment removed

%Therefore a matrix ${\mathbf A}_{N,N+1}$ given by entries (\ref{coefdef}) is a real symmetrical matrix with the entries equal to zero if a sum of the row and column numbers is odd.

%Ideas described above are summarized in Algorithm \ref{alg1}.  
The overall computational cost of Algorithm \ref{alg1} is $\mathcal{O}(N^{3})$, which is  the complexity of the SVD of an $N\times (N+1)$ matrix computed in the 1st step of Algorithm \ref{alg1} and the numerical complexity to solve the matrix pencil problem in the second step of this algorithm. The linear system  in the 3rd step also takes at most $\mathcal{O}(N^{3})$ operations. 


Algorithm \ref{alg1} is not numerically stable for large $N$.  We observe that it provides good computational results for $N=1,\ldots,13$ (see Fig.  \ref{fig2}).  For $N\geq 14$ we have to use Algorithm \ref{alg11} or high precision computations. 
% For $N\geq 14$ we have to use Algorithm \ref{alg11} or high precision computations. 






%The results of computation of exponential sum of good approximation for the function $f(t)=\mathrm{e}^{-t^{2}/2}$ for different $N$ using Algorithm \ref{alg1} are presented in Table \ref{tab1}.



\section{Error estimates for approximation of the Gauss function}
\label{sec:error}

\subsection{Error estimate in the weighted $L_{2}$-norm}
In this section we will show that the approximation error $F(\tilde{\gamra})$  in (\ref{minnorm}) and (\ref{error1}) decays exponentially with $N$ if $r = \rho/\sigma$ is chosen in a suitable range.
For the proof, we will use an explicit coefficient vector $\gamra$. % which is close to the optimal coefficient vector $\tilde{\gamra}$.
Furthermore, we will employ the Gauss-Hermite quadrature formula, which possesses an exponential rate of convergence for special functions.

Our main result shows  exponential convergence of the approximation of the Gaussian by exponential sums.
\begin{theorem}\label{maintheorem}
Let the zeros of the Hermite polynomial $H_{N}(t)$ in $(\ref{mono})$ be denoted by $t_{1} > t_{2} > \ldots > t_{N}$.
For $\rho>0$ and $\sigma>0$ let $r := \frac{\rho}{\sigma}$ and   $\lambda_j = - {\mathrm i} \sqrt{\frac{2(\rho+\sigma)}{\sigma(2\rho + \sigma)}} t_j$, $j=1, \ldots , N$.
Moreover, let  $\tilde{\gamma}_{j}$ be  the coefficients obtained by Algorithm $\ref{alg11}$.
Then, the approximation error  $F(\tilde{\gamra})$ in $(\ref{minnorm})$  is bounded by 
\begin{align*} \textstyle    \Big\| \mathrm{e}^{-\cdot^{2}/2\sigma} \!-\! \sum\limits_{j=1}^{N} \tilde\gamma_j \mathrm{e}^{\lambda_j \cdot} \Big\|_{L_2(\mathbb{R}, \rho )} 
&= \Big\|\mathrm{e}^{-\cdot^{2}/2\sigma} - 2 \!\!\!\!\!\!\sum\limits_{j=1}^{\lfloor (N+1)/2 \rfloor} \!\!\!\! \tilde\gamma_j' \cos(|\lambda_j| \cdot) \Big\|_{L_2(\mathbb{R}, \rho )} 
< \textstyle c   \left(\frac{r}{\sqrt{2(2r+1)}}\right)^N \!  N^{3/4}, %< \textstyle c \,  \left(\frac{ 3^{1/4} \, r}{\sqrt{2(2r+1)}}\right)^N  ,
\end{align*}
%  for even $N$ and $F(\tilde{\gamra})  < c \, \left(\frac{r^2}{2r+1}\right))^N  N^{3/2}$ for odd $N$, 
where  $\tilde{\gamma}_{j}'=\tilde{\gamma}_{j}$ for $j=1, \ldots , \lfloor \frac{N}{2} \rfloor$ and $\tilde{\gamma}_{(N+1)/2}' = \frac{1}{2}\tilde{\gamma}_{(N+1)/2}$ for odd $N$. The constant $c$ in the estimate depends on $\rho$ but  is independent of  $N$. 
%For $r<3$ we have the exponential decay
%$$ \textstyle F(\tilde{\gamra}) < c \left(\frac{3^{{1/4} \, }r}{\sqrt{2(2r+1)}}\right)^N . $$
Therefore,  we achieve exponential decay of the approximation error for  $r = \frac{\rho}{\sigma} < 2+ \sqrt{6}$.
\end{theorem}

\begin{remark}
 The error estimate above is not sharp. In particular this result is already achieved using 
 an explicit coefficient vector $\gamra$ in the proof instead of the optimal vector $\tilde{\gamra}$ which is the solution of the system (\ref{system1}).
\end{remark}

To prove Theorem \ref{maintheorem},  we  need the following notations and preliminary lemmas.

\noindent
We introduce the Gauss-Hermite quadrature formula of the form 
\begin{equation}\label{GH} \textstyle \sum\limits_{j=1}^{N} \omega_{j}\, f({t}_{j})  = \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}}\,  f(t)\,  dt + R_{f}, 
\end{equation}
where $t_{1}> t_{2}> \ldots > t_{N}$ are the $N$ zeros of the  Hermite polynomial $H_{N}(t)$  and where $R_{f}$ denotes the error of the quadrature formula.
The weights $\omega_{j}$  in (\ref{GH}) are taken as 
\begin{equation}\label{GHweights} \textstyle \omega_{j} := \textstyle \frac{2^{N-1} N!\sqrt{\pi}}{N^{2} [H_{N-1}({t}_{j})]^{2}} = \frac{2^{N+1} N!\sqrt{\pi}}{ [H_{N+1}({t}_{j})]^{2}}, \qquad j=1, \ldots , N, 
\end{equation}
see e.g.  \cite{Szego}, formula (15.3.6), where we have used (\ref{rec}), i.e.,
$$H_{N+1}({t}_j) = 2{t}_j \, H_N({t}_j) - H_N'({t}_j)= -H_N'({t}_j)=-2N \, H_{N-1}({t}_j). $$
The Gauss-Hermite quadrature formula is exact for polynomials of degree up to  $2N-1$, and the error $R_{f}$ can be represented as 
\begin{equation}\label{errorGH}
 \textstyle |R_{f}| \le \frac{N!}{(2N)!} \frac{\sqrt{\pi}}{2^{N}} \max_{\xi \in {\mathbb R}}|f^{(2N)}(\xi)|, 
\end{equation}
where $ f^{(2N)}= \frac{\mathrm{d}^{2N}}{\mathrm{d} t^{2N}} f$ denotes the $(2N)$-th derivative of $f$, see \cite{Hildebrandt}, formula (8.7.7).
\noindent
To prove Theorem \ref{maintheorem}, will employ the following Lemma to estimate the error $R_{f}$ for some special functions.
%which is based on a result of Kuo and Wo\'zniakowski, \cite{Kuo12}.

\begin{lemma}\label{lemma-Hermite}
For $s_{0} >0$ and $s_{1} >0$ let $f_{k}(t) := {\mathrm e}^{2(-s_{1}  t^{2}  +s_{0} {t}_{k} t)}$, where ${t}_{k}$, $k=1, \ldots , N$, denote the zeros of the Hermite polynomial $H_{N}(t)$ in $(\ref{rodfor})$. Then the error of the Gauss-Hermite quadrature formula for $f_{k}$ satisfies 
\begin{equation}\label{Hermite-error}
\textstyle \Big| \sum\limits_{j=1}^{N} \omega_{j} \, f_{k}({t}_{j})  - \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}}\,  f_{k}(t)\,  dt \Big| <
 \sqrt{\pi}\,  s_{1}^{N} \, {\mathrm e}^{\frac{s_{0}^{2}}{{2}s_{1}} {t}_{k}^{2}}. 
 \end{equation}
\end{lemma}

\begin{proof}
%Let $p(t)$ be a quadratic polynomial with derivatives $p'(t)$, and $p''(t)$, while higher derivatives vanish.
%Then it can be shown by induction that for each $n \in {\mathbb N}_{0}$ the  $n$-th derivative of ${\mathrm e}^{p(t)}$  is of the form 
%\begin{equation}\label{Dn} 
%\textstyle D^{n}{\mathrm e}^{p(t)}  = {\mathrm e}^{p(t)} \left(  
%\sum\limits_{m=0}^{\lfloor n/2 \rfloor} \frac{n!}{m! \, (n - 2m)! \, 2^{m}} (p'(t))^{n-2m} \, (p''(t))^{m} \right).
%\end{equation}
%Let  $G_{n}(t) :=\sum\limits_{m=0}^{\lfloor n/2 \rfloor} \frac{n!}{m! \, (n - 2m)! \, 2^{m}} (p'(t))^{n-2m} \, (p''(t))^{m}$.
%Obviously, (\ref{Dn}) is correct for $n=0$ and $n=1$.  For $n+1$ we obtain 
%\begin{align*}
%D^{n+1}{\mathrm e}^{p(t)} &= {\mathrm e}^{p(t)} ( p'(t) G_{n}(t) +  G_{n}'(t)) \\
%&= \textstyle {\mathrm e}^{p(t)} \left(  \sum\limits_{m=0}^{\lfloor n/2 \rfloor} \frac{n!\, ((p'(t))^{n-2m+1} \, (p''(t))^{m} + (n-2m) (p'(t))^{n-2m-1} \, (p''(t))^{m})}{m! \, (n - 2m)! \, 2^{m}}  \right) \\
%&= {\mathrm e}^{p(t)} \, (p'(t))^{n+1} \, (p''(t))^{0} \\
% & + \textstyle  {\mathrm e}^{p(t)} \left(  \sum\limits_{m=1}^{\lfloor (n+1)/2 \rfloor} \left( \frac{n!}{m! \, (n - 2m)! \, 2^{m}}  + \frac{n! (n-2m-2)}{(m+1)! \, (n - 2m-2)! \, 2^{m+1}} \right)
%(p'(t))^{n+1-2m} \, (p''(t))^{m} \right)  \\
%&= {\mathrm e}^{p(t)} \, G_{n+1}(t).
%\end{align*}
%We employ formula (\ref{Dn}) for $p_{k}(t)= -2s_{1}t^{2} + 2s_{0}{t}_{k} t$ with $p_{k}'(t) = -4s_{1}t + 2s_{0} {t}_{k}$ and $p_{k}''(t) = -4s_{1}$.
%Then it follows that 
%\begin{align*}
%D^{2N} f_{k}(t) &=  \textstyle {\mathrm e}^{p_{k}(t)} \left(  \sum\limits_{m=0}^{N} \frac{(2N)! \, (-1)^{m}}{m! \, (2N - 2m)! \, 2^{m}} (-4s_{1} t + 2 s_{0} {t}_{k})^{2N-2m} \, (4s_{1})^{m}  \right) \\
%&=  \textstyle {\mathrm e}^{p_{k}(t)}\, (4s_{1})^{N}  \left( \sum\limits_{m=0}^{N} \frac{(2N)! \, (-1)^{m}}{m! \, (2N - 2m)! \, 2^{m}} \left(\frac{-4s_{1} t + 2 s_{0} {t}_{k}}{2 \sqrt{s_{1}}})^{2N-2m}\right)  \right) \\
%&= \textstyle {\mathrm e}^{p_{k}(t)}\, (4s_{1})^{N} \, He_{2N} \left( \frac{-2s_{1}t + s_{0} {t}_{k}}{\sqrt{s_{1}}} \right)
%= {\mathrm e}^{p_{k}(t)}\, (2s_{1})^{N} \, H_{2N} \left( \frac{-2s_{1}t + s_{0} {t}_{k}}{\sqrt{2s_{1}}} \right),
%\end{align*}
%where we have used (\ref{mono}) and (\ref{Hn}). 

From (\ref{rodfor}) it follows that  
$ \textstyle \frac{\mathrm{d}^{2N}}{\mathrm{d} t^{2N}}\mathrm{e}^{-t^2}=\mathrm{e}^{-t^2} \, H_{2N}(t)$,  
and hence,  using the chain rule,
\begin{align*}
\textstyle f^{(2N)}_k(t) &= \textstyle \frac{\mathrm{d}^{2N}}{\mathrm{d} t^{2N}} \mathrm{e}^{-2s_1t^2+2s_0{t}_kt}
= \mathrm{e}^{\frac{s_{0}^{2} t_{k}^{2}}{2s_{1}}} 
\frac{\mathrm{d}^{2N}}{\mathrm{d} t^{2N}} \mathrm{e}^{-(\sqrt{2s_1}t-\frac{s_0{t}_k}{\sqrt{2s_1}})^2} \\
& = \textstyle \mathrm{e}^{\frac{s_{0}^{2} t_{k}^{2}}{2s_{1}}} \, (2s_{1})^{N} \, \mathrm{e}^{-(\sqrt{2s_1}t-\frac{s_0{t}_k}{\sqrt{2s_1}})^2}H_{2N}(\sqrt{2s_1}t-\frac{s_0{t}_k}{\sqrt{2s_1}})\\
& = \textstyle f_k(t)(2s_1)^NH_{2N}(\frac{-2s_1t+s_0{t}_k}{\sqrt{2s_1}}), 
\end{align*}
where we used in the last step that $H_{2N}$ is even. Therefore
\begin{align*} \textstyle 
\max\limits_{t \in {\mathbb R}} | f^{(2N)}_{k}(t)| &= \textstyle (2s_{1})^{N} \, \max\limits_{t \in {\mathbb R}} \Big({\mathrm e}^{-2s_{1} t^{2} + 2 s_{0} {t}_{k} t} \, |H_{2N} \Big( \frac{-2s_{1}t + s_{0} {t}_{k}}{\sqrt{2s_{1}}} \Big) |\Big)\\
&= \textstyle  (2s_{1})^{N}  \max\limits_{x \in {\mathbb R}} \Big({\mathrm e}^{-x^{2}+\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}}  | H_{2N}(x)| \Big)
=  \textstyle (2s_{1})^{N}   {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}}   \max\limits_{x \in {\mathbb R}} \Big({\mathrm e}^{-x^{2}}   |H_{2N}(x) |\Big). 
\end{align*}
The first derivative of ${\mathrm e}^{-x^{2}}  \, H_{2N}(x)$ is given by 
$$(-2x \, H_{2N}(x) + H_{2N}'(x)) {\mathrm e}^{-x^{2}} = - H_{2N+1}(x) \, {\mathrm e}^{-x^{2}},$$
 i.e., the local extrema of this function occur at the zeros of $H_{2N+1}(x)$.
We show that the global maximum of ${\mathrm e}^{-x^{2}}  \, |H_{2N}(x)|$ is obtained at $x=0$.
For this purpose, we apply a generalization of the Theorem of Sonin, see \cite{Szego}, Theorem 7.31.1 and the corresponding footnote. This theorem says that, if a function $y(x)$ satisfies the differential equation
$$ (k(x) \, y'(x))' + \phi(x) \, y(x) =0, $$
where  $k(x)>0$ and $\phi(x)>0$ are continuously differentiable, then the relative maxima of $|y(x)|$ for $x\ge 0$ form  an increasing or decreasing  sequence according as $k(x) \phi(x)$ is decreasing or increasing.
We simply observe that $y(x) = {\mathrm e}^{-x^{2}}  \, H_{n}(x)$ satisfies the above differential equation with $k(x) = {\mathrm e}^{x^{2}}$ and $\phi(x) = (2n+2) {\mathrm e}^{x^{2}}$. Since $k(x) \phi(x)$ is increasing for $x \ge 0$, it follows that the sequence of local maxima of ${\mathrm e}^{-x^{2}}  \, |H_{n}(x)|$ decreases for $x\ge 0$ and  $n \in {\mathbb N}$. 
Thus, we conclude that 
$$ \max_{x \in {\mathbb R}} |f^{(2N)}_{k}(t)| = \textstyle (2s_{1})^{N}  \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}} \,   |H_{2N}(0)| = (2s_{1})^{N}  \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}} \,   \frac{(2N)!}{N!}, $$
and  (\ref{errorGH}) finally implies that 
$$ |R_{f_{k}}| \le  \frac{\sqrt{\pi}}{2^{N}} (2s_{1})^{N}  \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}}{t}_{k}^{2}}  = \sqrt{\pi} \, s_{1}^{N}  \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}}{t}_{k}^{2}}. $$
\end{proof}

\begin{remark} 1. Using the result of \cite{Kuo12}, a similar estimate can be concluded, where we loose however a factor $2^{-N}$. In can be shown that the functions $f_{k}(t)$ in Lemma \ref{lemma-Hermite} are elements of the reproducing kernel Hilbert space generated by the Gaussian kernel
 $K_{2s_{1}}(x,y) := {\mathrm e}^{-2s_{1}(x-y)^{2}}$ for $x,\, y \in {\mathbb R}$.
In this reproducing kernel Hilbert space, these functions $f_{k}(t)$ have the norm  ${\mathrm e}^{\frac{s_{0}^{2}}{{2}s_{1}} {t}_{k}^{2}}$, such that an error of the Gauss-Hermite quadrature error of the form $2 (2s_{1})^{N} \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}}$ is obtained.
\end{remark}



%Further, we consider the weighted $L_{2}({\mathbb R}, \frac{1}{2})$-space  %with weight $ \frac{1}{\sqrt{\pi}}{\mathrm e}^{-x^{2}}$ 
%with the (normalized) inner product 
%\begin{equation}\label{ip1} \langle f,g \rangle  := \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} f(x) \, g(x) \, {\mathrm e}^{-x^{2}} d x. \end{equation}
%Then, the Gaussian kernel  can by Mercer's theorem  be rewritten as
%$$ K_{2s_{1}}(x,y) = \sum_{j=1}^{\infty} \lambda_{j} \, \varphi_{j}(x)\,  \varphi_{j}(y)$$
%with eigenvalues $\lambda_{j} >0$ and eigenfunctions $\varphi_{j}$, which are scaled Hermite functions that can be explicitly computed, see \cite{Kuo12}, page 428. Moreover, the set of eigenfunctions $\varphi_{j}$ forms an orthonormal basis  of $L_{2}({\mathbb R}, \frac{1}{2})$ with respect to the inner product in (\ref{ip1}). 
%The kernel  $K_{2s_{1}}(x,y)$ generates a reproducing kernel Hilbert space  $H(K_{2s_{1}})$,
%$$ H(K_{2s_{1}}) = \overline{ \textrm{span} \, \{ K(\cdot, y): \, y \in {\mathbb R} \}} ,$$
%where the closure is taken with respect to the norm 
%$$  \textstyle \| f\|_{H_{K_{2s_{1}}}} :=  \left(\sum\limits_{j=1}^{\infty}  \frac{1}{\lambda_{j}} \langle f, \varphi_{j} \rangle^2 \right)^{1/2}. $$
%Then, the functions 
%$$ \textstyle f_{k}(t) = {\mathrm e}^{2(-s_{1}  t^{2}  +s_{0} {t}_{k} t)} = {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}} \, {\mathrm e}^{-2s_{1}(t-\frac{s_{0}}{2s_{1}} {t}_{k})^{2}} =  {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}} \, K_{2s_{1}}(t, \frac{s_{0}}{2s_{1}} {t}_{k})$$ 
%are elements of $H(K_{2s_{1}})$ and we obtain the norm
%\begin{align*}
%\| f_{k} \|_{H_{K_{2s_{1}}}}^{2} &=  \textstyle \sum\limits _{\ell=1}^{\infty}  \frac{1}{\lambda_{\ell}} \langle f_{k}, \varphi_{\ell} \rangle^{2}
%= \sum\limits_{\ell=1}^{\infty}  \frac{1}{\lambda_{\ell}} \langle {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}} \, K_{2s_{1}}(\cdot, \frac{s_{0}}{2s_{1}} {t}_{k}), \varphi_{\ell} \rangle^{2} \\
%&= \textstyle {\mathrm e}^{\frac{s_{0}^{2}}{s_{1}} {t}_{k}^{2}} \sum\limits_{\ell=1}^{\infty}  \frac{1}{\lambda_{\ell}} \langle 
%\sum\limits_{j=1}^{\infty} \lambda_{j} \, \varphi_{j}(\cdot)\,  \varphi_{j}(\frac{s_{0}}{2s_{1}} {t}_{k}) , \varphi_{\ell} \rangle^{2}  \\
%&= \textstyle {\mathrm e}^{\frac{s_{0}^{2}}{s_{1}} {t}_{k}^{2}} \sum\limits_{\ell=1}^{\infty} \lambda_{\ell}  \,  \varphi_{\ell}(\frac{s_{0}}{2s_{1}} {t}_{k})^{2}
%= {\mathrm e}^{\frac{s_{0}^{2}}{s_{1}} {t}_{k}^{2}} \, K_{2s_{1}}(\frac{s_{0}}{2s_{1}} {t}_{k}, \frac{s_{0}}{2s_{1}} {t}_{k}) 
%= {\mathrm e}^{\frac{s_{0}^{2}}{s_{1}} {t}_{k}^{2}}.
%\end{align*} 
%Therefore, we can apply Theorem 4.1 in \cite{Kuo12} to obtain 
%$$
%\textstyle \Big| \sum\limits_{j=1}^{N} \omega_{j} \, f_{k}({t}_{j})  - \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}}\,  f_{k}(t)\,  dt \Big| < 2 \, (2s_1)^N \, \| f_k\|_{H_{K_{2s_{1}}}} =
% 2 (2s_{1})^{N} \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}}. 
%$$
%\end{proof}

\noindent
For the next estimate, we  employ the Hermite functions, which are for $n \in {\mathbb N}_{0}$ defined by
\begin{align}\label{hermitef} \psi_{n}(t):=  \textstyle (2^{n} N! \sqrt{\pi})^{-\frac{1}{2}} \, {\mathrm e}^{-\frac{t^{2}}{2}} H_{n}(t) .
\end{align}
Note that the functions ${\mathrm e}^{-x^{2}} H_{n}(x)$ occurring in the proof of Lemma \ref{lemma-Hermite} differ from the Hermite functions $\psi_{n}$ by the factor ${\mathrm e}^{-x^{2}/2}$ (and the normalization factor).

%Furthermore, we will need the estimate in the following lemma

\begin{lemma} \label{lem2}
Let ${t}_1 > {t}_2 > \ldots > {t}_N$ be the $N$ zeros of the $N$-th Hermite polynomial $H_N$ and let $\omega_j$ be the Gauss-Hermite weigths in $(\ref{GHweights})$.
Then we have 
%for even $N$ 
%$$  \textstyle M_N:= \sum\limits_{j=1}^N \omega_j \, {\mathrm e}^{{t}_j^2} < \frac{ C_e}{\sqrt{N}},$$
%and for odd $N$, 
$$ \textstyle M_N := \sum\limits_{k=1}^N \omega_k \, {\mathrm e}^{{t}_k^2} < C \, N^{3/2},$$
%where the constants $C_o$ and $C_e$ are independent of $N$.
where the constant $C$ is independent of $N$.
\end{lemma}

\begin{proof}
1. We recall that the weights $\omega_k$ in (\ref{GHweights}) can be rewritten with the Hermite functions in (\ref{hermitef}) as
$$ \textstyle \omega_k= 
 \frac{1}{(N+1) \, {\mathrm e}^{{t}_k^2} [\psi_{N+1}({t}_k)]^2} ,$$
%where %$\psi_{N-1}(t):=  \frac{ {\mathrm e}^{-t^{2}/2} H_{N-1}(t)}{{\pi}^{1/4}\, 2^{(N-1)/2}  \, \sqrt{(N-1)!} }$ 
%$\psi_{N+1}(t):=  \textstyle \frac{ {\mathrm e}^{-t^{2}/2} H_{N+1}(t)}{{\pi}^{1/4}\, 2^{(N+1)/2}  \, \sqrt{(N+1)!} }$
%is  the $(N+1)$-th Hermite function, 
such that 
$$  \textstyle M_N= \sum\limits_{k=1}^{N} \omega_{k} \,  {\mathrm e}^{ {t}_{k}^{2}} = \frac{1}{N+1} \sum\limits_{k=1}^{N} \frac{1}{[\psi_{N+1}({t}_k)]^2}. $$
Then the symmetry of $\psi_{N+1}$ implies that 
\begin{align}\label{MN1}
\textstyle M_N= \frac{2}{N+1}  
 \sum\limits_{k=1}^{N/2} \frac{1}{[\psi_{N+1}({t}_k)]^2} \quad \text{or} \quad 
M_N= \frac{1}{N+1} \left( \frac{1}{[\psi_{N+1}(0)]^2} + 
2 \sum\limits_{k=1}^{(N-1)/2} \frac{1}{[\psi_{N+1}({t}_k)]^2} \right), 
\end{align}
for even and odd $N$, respectively, where  ${t}_{(N+1)/2}=0$ for odd $N$.
We consider the zeros and local extrema of $H_{N+1}$ and $\psi_{N+1}$ on $[0,\infty)$ more closely. Obviously, $H_{N+1}$ and $\psi_{N+1}$ share the same $\lfloor \frac{N+1}{2} \rfloor$ 
zeros $t_{1}^{(N+1)} > t_{2}^{(N+1)} > \ldots > t_{\lfloor \frac{N+1}{2} \rfloor}^{(N+1)}$ in $(0, \infty)$.
% where $0$ occurs as a zero if  $N+1$ is odd. 
By (\ref{rec}), the Hermite polynomial $H_{N+1}$ possesses local extrema at the zeros 
$t_{1} > t_{2} >\ldots > t_{n}$ of $H_{N}$, where 
$n:=\lfloor \frac{N+1}{2} \rfloor$. We denote the locations of local extrema of $\psi_{N+1}$ by $t_0^*>t_1^* > t_2^* > \ldots > t_{n}^* \ge 0$. Then we have the well-known interlacing properties
\begin{align}\nonumber t_{1}^{(N+1)} &<  t_{0}^{*}\\
\label{inter}
t_{k+1}^{(N+1)} &< t_{k}^{*} < t_{k} < t_{k}^{(N+1)}, \quad k=1, \ldots , n-1, \\
\nonumber
0 &< t_{n}^{*} < t_{n} < t_{n}^{(N+1)}.
\end{align}
We employ now Theorem 7.6.3 in \cite{Szego},  showing that the local maxima of $|\psi_{N+1}(t)|$ are monotonically increasing for $t \ge 0$. That means, we have
$$ |\psi_{N+1}(t_0^*)| >|\psi_{N+1}(t_1^*)| > |\psi_{N+1}(t_2^*)| > \ldots > |\psi_{{N+1}}(t_n^*)|. $$
Furthermore, we  always have $|\psi_{N+1}(t_k^*)| > |\psi_{N+1}({t}_k)|$, since ${\mathrm e}^{-t^{2}/2}$ is positive and  monotonically 
decreasing  for $t > 0$.
For $k=1, \ldots, n$,  $\psi_{N+1}(t_k^*)$ and $\psi_{N+1}({t}_k)$ have always the same sign, and we obtain for the difference of function values  
\begin{align*}|\psi_{N+1}(t_k^*)  - \psi_{N+1}({t}_k)| = |\psi_{N+1}(t_k^*)|  - |\psi_{N+1}({t}_k)| \le |{t}_k- t_k^*|
\,  \max_{\xi \in [t_k^*, {t}_k]} | \psi_{N+1}'(\xi)|.
\end{align*}
Since $\psi_{N+1}$ satisfies the differential equation  $\psi_{N+1}''(\xi) = -(2N+3 - \xi^2) \psi_{N+1}(\xi)$ and  $\psi_{N+1}'(t_k^*)=0$, it follows that $|\psi_{N+1}'(\xi)|$ is monotonically increasing in $[t_k^*, {t}_k]$, and therefore
\begin{align}\label{ta}   |\psi_{N+1}(t_k^*)|  - |\psi_{N+1}({t}_k)| \le  |{t}_k- t_k^*|
\,   | \psi_{N+1}'({t}_k)|.
\end{align}
Now,  $\psi_{N+1}'({t}_k)$ can by $H_{N+1}'({t}_k)= 2(N+1) \, H_N({t}_k)=0$ be rewritten as
$$ \textstyle  \psi_{N+1}'({t}_k) =  \frac{ \left( -{t}_k\, H_{N+1}({t}_k) + H_{N+1}'({t}_k)\right)}{(\sqrt{\pi}\, 2^{N+1}  \, (N+1)!)^{1/2}}   {\mathrm e}^{-{t}_k^{2}/2}  = -{t}_k\, \psi_{N+1}({t}_k). $$
Hence, (\ref{ta}) yields
\begin{align*}
\textstyle \left| \frac{\psi_{N+1}(t_k^*)}{\psi_{N+1}({t}_k)}\right| \le  |{t}_k - t_k^*| \, |{t}_k| +1. 
\end{align*}
For even $N$, we conclude from (\ref{MN1})
$$ \textstyle M_N <  \textstyle \frac{2}{N+1} \sum\limits_{k=1}^{N/2}    \left( \frac{({t}_k- t_k^*) {t}_k + 1}{\psi_{N+1}({t}_k^*)}  \right)^2 <
 \frac{2}{(N+1) \, [\psi_{N+1}(t_{N/2}^*)]^2} \sum\limits_{k=1}^{N/2} (({t}_k- t_k^*) {t}_k + 1  )^2,
$$
and for odd $N$ similarly,
\begin{align*}
M_N <  \textstyle  \frac{1}{N+1} \left( \frac{1}{[\psi_{N+1}(0)]^2} + \frac{2}{[\psi_{N+1}(0)]^2}\sum\limits_{k=1}^{(N-1)/2}    \left(({t}_k- t_k^*) {t}_k + 1  \right)^2 \right).
\end{align*}
2.  According to \cite[formula (14)]{Salzer},  we have the relation for the positive zeros of $H_N$, 
$$ \textstyle \sum\limits_{k=1}^{\lfloor N/2 \rfloor} {t}_k^2 = \frac{N(N-1)}{4}. $$
Using the interlacing property (\ref{inter}) and 
observing that the largest zero of  $H_{N+1}$ is bounded by ${t}^{(N+1)}_{1} < \sqrt{2N+3}$, see \cite{Szego}, it follows that
$$ \textstyle \sum\limits _{k=1}^{\lfloor N/2 \rfloor} (t_k^*)^2 > \sum\limits_{k=2}^{\lfloor N/2 \rfloor } ({t}_k^{(N+1)})^2 = \frac{(N+1)N}{4} - ({t}_1^{{(N+1)}})^2 > \frac{N^2 + N}{4} -(2N+3) = \frac{N^2}{4}-\frac{7N}{4}-3.$$
Hence,
$$  \textstyle \sum\limits_{k=1}^{\lfloor N/2 \rfloor} {t}_k^2 - (t_k^*)^2  < \frac{N(N-1)}{4} - \frac{N^2}{4}+\frac{7N}{4}+3 = \frac{3N}{2} +3. $$
We conclude %from (\ref{mult}) that $t_k^* = c {t}$ with $\frac{\pi}{7} < c < 1$
\begin{align*} \textstyle  
 \textstyle \sum\limits_{k=1}^{\lfloor N/2 \rfloor} 
({t}_k - t_k^*)\, {t}_k < \sum\limits_{k=1}^{\lfloor N/2 \rfloor} 
({t}_k - t_k^*)\, ({t}_k + t_k^*) = \sum\limits_{k=1}^{\lfloor N/2\rfloor} {t}_k^2 - (t_k^*)^2   < \frac{3N}{2} +3. 
\end{align*}
Therefore 
\begin{align*} 
 \textstyle \sum\limits_{k=1}^{\lfloor N/2 \rfloor} (({t}_{k} - t_{k}^*)  {t}_k +1 )^2 
 &=  \textstyle  \sum\limits_{k=1}^{\lfloor N/2\rfloor}  ({t}_{k} - t_{k}^*)^2 \, {t}_k^2 + 2 \sum\limits_{k=1}^{\lfloor N/2\rfloor}  ({t}_{k} - t_{k}^*) \, {t}_k  + \lfloor\frac{N}{2}\rfloor \\
&<  \textstyle  \Big(\sum\limits_{k=1}^{\lfloor N/2\rfloor}  ({t}_{k} - t_{k}^*) {t}_k\Big)^2 + 2 \Big( \sum\limits_{k=1}^{\lfloor N/2\rfloor}  ({t}_{k} - t_{k}^*) {t}_k \Big)  + \lfloor\frac{N}{2}\rfloor \\
&<  \textstyle \left(\frac{3(N+2)}{2} \right)^2+ 3(N+2)+ \lfloor\frac{N}{2}\rfloor
 < \tilde{C} \, N^2
\end{align*} 
for each $N>0$ with some suitable constant $\tilde{C}$ being independent of $N$. 
 
\noindent 
Finally, we have to estimate $[\psi_{N+1}(0)]^2$ for odd $N$ and $[\psi_{N+1}(t_{N/2}^*)]^2$ for even $N$. 

 For odd $N$ we obtain from $H_{N+1}(0)^2= 2^{N+1} \, (N!!)^2$ that 
\begin{align*} 
[\psi_{N+1}(0)]^2 &= \textstyle \frac{2^{N+1} \, (N!!)^2}{\sqrt{\pi}  \, 2^{N+1} \, (N+1)!}= \frac{((N+1)!)^2}{\sqrt{\pi} ((N+1)!!)^2 (N+1)!} \\
&=  \textstyle \frac{(N+1)!}{\sqrt{\pi} \, 2^{N+1} \, ((\frac{N+1}{2})!)^2} = \frac{1}{\sqrt{\pi}  \, 2^{N+1}} \binom{N+1}{\frac{N+1}{2}} >
\frac{1}{\sqrt{\pi}  \, 2^{N+1}} \frac{2^{N+1}}{\sqrt{\pi(N+2)/2}} =  \frac{1}{\pi}\sqrt{\frac{2}{ N+2}},
\end{align*}
where we have used that $\frac{2^{N+1}}{\sqrt{\pi(N+2)/2}}< \binom{N+1}{\frac{N+1}{2}} < \frac{2^{N+1}}{ \sqrt{\pi (N+1)/2}}$ by Stirling's formula.
Therefore we can estimate $M_N$ by
\begin{align*} 
M_N &\le  \textstyle \frac{1}{(N+1)[\psi_{N+1}(0)]^2  }  ( 1 + 2\, \tilde{C} \, N^2) < \frac{\pi \sqrt{N+2}}{\sqrt{2}(N+1)} (1+2 \, \tilde{C} \, N^2) < C_o  \, N^{3/2}, 
\end{align*}
with some $C_o$ being independent of $N$.
For even $N$, we conclude from Formulas (8.65.2), (8.65.3) in \cite{Szego} (with the normalization weights considered for the Hermite functions) that  
\begin{align*}
[\psi_{N+1}(t_{N/2}^*)]^2 &=  \textstyle c_e  \, \Big(\frac{1}{\sqrt{\pi} \, 2^{N+1}  \, (N+1)!}  \Big) \, \frac{[H_{N+1}'(0)]^2}{2N+3} = c_e  \, \frac{(2N+2)^2 \, [H_N(0)]^2}{\sqrt{\pi} \, 2^{N+1}  \, (N+1)! (2N+3)} \\
&= \textstyle c_e  \, \frac{(2N+2)^2 \, 2^N \, [(N-1)!!]^2}{\sqrt{\pi} \, 2^{N+1}  \, (N+1)! (2N+3)} 
= c_e \, \frac{(N+1)^2}{\sqrt{\pi}\,  (N+3/2)}  \, \frac{[N!]^2}{(N+1)! \, [N!!]^2} \\
&= \textstyle c_e \, \frac{(N+1)}{\sqrt{\pi}\,  (N+3/2)}  \, \frac{1}{2^N}  \binom{N}{\frac{N}{2}}
> c_e \, \frac{(N+1)}{\sqrt{\pi}\,  (N+3/2)}  \, \frac{1}{2^N}  \frac{2^N}{\sqrt{\pi \, (N+1)/2}}  > c_e  \frac{1}{\pi} \frac{1}{\sqrt{N+1}}
\end{align*}
for some $c_e$ being independent of $N$,  and therefore,
\begin{align*} 
M_N &\le  \textstyle \frac{1}{ (N+1)\, [\psi_{N+1}(t_{N/2}^*)]^2  }  (2 \, \tilde{C} \, N^2)  <  {C_e} \, N^{3/2},
\end{align*}
where the constant $C_e$ is independent of $N$.
\end{proof}

\begin{remark} The estimate of Lemma \ref{lem2} will be crucial for the proof of Theorem \ref{maintheorem}.
Observe that we cannot use the Gauss-Hermite formula (\ref{GH}) here, since while 
$\sum\limits_{k=1}^N \omega_k \, {\mathrm e}^{{t}_k^2}$ is bounded by $C N^{3/2}$, we obtain with $f(t) = {\mathrm e}^{{t}^2}$ in (\ref{GH}) only
$$ \textstyle \sum\limits_{k=1}^N \omega_k \, {\mathrm e}^{{t}_k^2} = \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}}\, {\mathrm e}^{t^{2}} {\textrm d} t + R_{f}, $$
where the right-hand side ist not finite.
\end{remark}

\noindent
With these preliminaries we now  prove Theorem \ref{maintheorem} on the exponential convergence of the approximation of the Gaussian by exponential sums.


\begin{proof} (of Theorem \ref{maintheorem})
1. We introduce the notations $s_{0} := \frac{r(1+r)}{(2r+1)}$ and $s_{1}:=\frac{r^{2}}{2(2r+1)}$ and  let  again ${t}_k$ for $k=1, \ldots, N$ be the ordered zeros of $H_{N}$.
As shown in (\ref{error1}),  the approximation error $F(\gamra)$ for real $\gamra$ can be written in the form 
$$  \textstyle F({\gamra})  = \sqrt{\frac{2\pi \rho}{2r + 1}} - 2 {\mathbf g}^T {\gamra} + {\gamra}^T {\mathbf H}_N {\gamra}, 
$$
where the components  $g_j$  of ${\mathbf g}$ are by (\ref{gjneu}) given by 
$$g_{j} =  \textstyle \frac{\sqrt{2\pi \rho}}{\sqrt{1+r}} {\mathrm e}^{-\frac{r}{{2r+1}} t_{j}^{2}} = \frac{\sqrt{2\pi \rho}}{\sqrt{1+r}} {\mathrm e}^{-(s_0-2s_1) {t}_{j}^{2}}.$$
Further,  we recall  from (\ref{Hjk}) that the components of ${\mathbf H}_N$ can be represented as 
$$ H_{j,k} = H_{k,j}= \sqrt{2\pi \rho} \, {\mathrm e}^{-\frac{(r+1)r}{{2r+1}}(t_{j}-t_{k})^{2} }=  \sqrt{2\pi \rho} \,  {\mathrm e}^{-s_{0}({t}_{j}-{t}_{k})^{2}}.$$
We introduce now the coefficient vector $\gamra = (\gamma_j)_{j=1}^N$ with 
\begin{align}\label{gammaneu} \textstyle \gamma_{j} :=% \frac{1}{\sqrt{1+r}} {\mathrm e}^{s_{0}{t}_{j}^{2}} \, h_{j} =   
\frac{\sqrt{r+1}}{\sqrt{\pi(2r+1)}} \, {\mathrm e}^{(s_{0}-2s_{1}) {t}_{j}^{2}}\, \omega_{j} , \qquad j=1, \ldots , N,
\end{align} 
where $\omega_j$ are the weights of the Gauss-Hermite formula (\ref{GH}) given in (\ref{GHweights}).
Then, observing that $\sum\limits_{j=1}^{N} \omega_{j} =  \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}} dt = \sqrt{\pi}$,
we find
\begin{align*}
\textstyle  {\mathbf g}^{T} \gamra = \sum\limits_{j=1}^{N} g_{j} \gamma_{j} %&= \sum_{j=1}^{N} \frac{\sqrt{2\pi\rho}}{\sqrt{1+r}} {\mathrm e}^{-\frac{s_{0}}{(1+r)} {t}_{j}^{2}} \, \frac{1}{\sqrt{1+r}} {\mathrm e}^{s_{0}{t}_{j}^{2}} h_{j}
&= \textstyle \sum\limits_{j=1}^{N} \frac{\sqrt{2\pi\rho}}{\sqrt{1+r}} {\mathrm e}^{-(s_0-2s_1) {t}_{j}^{2}} \, \frac{\sqrt{r+1}}{\sqrt{\pi(2r+1)}} \, {\mathrm e}^{(s_{0}-2s_{1}) {t}_{j}^{2}}\, \omega_{j} \\
&= \textstyle \frac{\sqrt{2\pi\rho}}{\sqrt{\pi(2r+1)}}  \sum\limits_{j=1}^{N} \omega_{j} = \frac{\sqrt{2\pi\rho}}{\sqrt{\pi(2r+1)}} \sqrt{\pi} = \frac{\sqrt{2\pi\rho}}{\sqrt{2r+1}} .
\end{align*}
Therefore, with  $\gamra$ in (\ref{gammaneu}), the error $F({\gamra})$ in (\ref{error1}) can be written as
 $$ \textstyle   F({\gamra})  = \sqrt{\frac{2\pi \rho}{2r + 1}} - 2 {\mathbf g}^T {\gamra} + {\gamra}^T {\mathbf H}_N {\gamra} =
 {\gamra}^T {\mathbf H}_N {\gamra} - \sqrt{\frac{2\pi \rho }{2r + 1}}. $$


\noindent
2. Next, we consider  ${\gamra}^{T} {\mathbf H}_N {\gamra}$. For the components of ${\mathbf H}_N {\gamra}$  we have 
\begin{align} \nonumber 
\sum_{j=1}^{{N}} H_{j,k} \, \gamma_{j} &= \textstyle \sum\limits_{j=1}^{N} \sqrt{2\pi \rho} \, {\mathrm e}^{-s_{0}({t}_{j}-{t}_{k})^{2}} \frac{\sqrt{r+1}}{\sqrt{\pi(2r+1)}} \, {\mathrm e}^{(s_{0}-2s_{1}) {t}_{j}^{2}}\, \omega_{j} \\
&= \textstyle \frac{\sqrt{2\pi \rho (r+1)}}{\sqrt{\pi(2r+1)}}   {\mathrm e}^{-s_{0} {t}_{k}^{2}}\sum\limits_{j=1}^{N} \omega_{j} \, {\mathrm e}^{-2s_{1} {t}_{j}^{2} +2s_{0}{t}_{j}{t}_{k} }.
\label{fk1}
\end{align}
We introduce the functions $f_{k}(t) := {\mathrm e}^{(-2s_{1}  t^{2}  +2s_{0} {t}_{k} t)}$ for $k=1, \ldots , N$, and apply the Gauss-Hermite quadrature formula (\ref{GH}).
Using again the formula $\int\limits_{-\infty}^{\infty}  \mathrm{e}^{-at^{2}+bt} \, \mathrm{d}t=\sqrt{\dfrac{\pi}{a}} \, \mathrm{e}^{\frac{b^{2}}{4a}}$ for $a>0$, we have
$$ \textstyle \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}}\,  f_{k}(t)\,  dt = \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}(1+2s_{1}) + t(2s_{0}{t}_{k})}\,   dt
= \sqrt{\frac{\pi}{1+2s_1}} {\mathrm e}^{\frac{4s_{0}^{2}  {t}_{k}^{2}}{4(1+2s_1)}} =  \sqrt{\frac{\pi(2r+1)}{(r+1)^{2}}} {\mathrm e}^{2 s_{1} {t}_{k}^{2}}$$
and therefore,
$$ \textstyle \sum\limits_{j=1}^{N} \omega_{j}\, f_{k}({t}_{j}) = \sqrt{\frac{\pi(2r+1)}{(r+1)^{2}}} {\mathrm e}^{2 s_{1} {t}_{k}^{2}}+R_{f_{k}},$$
where the quadrature error $R_{f_{k}}$ can be estimated as given in Lemma \ref{lemma-Hermite}.
Hence, $\sum\limits_{j=1}^{{N}} H_{j,k} \, \gamma_{j} $ in (\ref{fk1}) can be written as 
\begin{align*}
\sum_{j=1}^{{N}} H_{j,k} \, \gamma_{j}  &=\textstyle \frac{\sqrt{2 \rho (r+1)}}{\sqrt{2r+1}} {\mathrm e}^{-s_{0} {t}_{k}^{2}} \sum\limits_{j=1}^{N} \omega_{j} \, f_{k}({t}_{j})\\
&= \textstyle \frac{\sqrt{2 \rho (r+1)}}{\sqrt{2r+1}} \, {\mathrm e}^{-s_{0} {t}_{k}^{2}} \left( \sqrt{\frac{\pi(2r+1)}{(r+1)^{2}}} \, {\mathrm e}^{2 s_{1} {t}_{k}^{2}}+R_{f_{k}} \right) \\
&= \textstyle \frac{\sqrt{2 \pi \rho}}{\sqrt{r+1}} \, {\mathrm e}^{(-s_{0}+2s_{1}) {t}_{k}^{2}} + \frac{\sqrt{2 \rho (r+1)}}{\sqrt{2r+1}}  \, {\mathrm e}^{-s_{0} {t}_{k}^{2}} \, R_{f_{k}}. %= \textstyle \frac{\sqrt{2 \pi \rho}}{\sqrt{r+1}} {\mathrm e}^{-\frac{1}{r+1} {t}_{k}^{2}} + \frac{\sqrt{2 \rho (r+1)}}{\sqrt{2r+1}} {\mathrm \sqrt{2\rhoe}^{-s_{0} {t}_{k}^{2}} \, R_{f_{k}}.
\end{align*}
Thus, 
\begin{align*}
{\gamra}^{T} {\mathbf H}_N {\gamra} &=
\textstyle \sum\limits_{k=1}^{N} \gamma_{k} \, \left(\frac{\sqrt{2 \pi \rho}}{\sqrt{r+1}} {\mathrm e}^{(-s_{0}+2s_{1}) {t}_{k}^{2}} + \frac{\sqrt{2 \rho (r+1)}}{\sqrt{2r+1}} {\mathrm e}^{-s_{0} {t}_{k}^{2}} \, R_{f_{k}}\right)\\
&=  \textstyle \sum\limits_{k=1}^{N} \frac{\sqrt{r+1}}{\sqrt{\pi(2r+1)}} \, {\mathrm e}^{(s_{0}-2s_{1}) {t}_{k}^{2}}\, \omega_{k} \left(\frac{\sqrt{2 \pi \rho}}{\sqrt{r+1}} {\mathrm e}^{(-s_{0}+2s_{1}) {t}_{k}^{2}} + \frac{\sqrt{2 \rho (r+1)}}{\sqrt{2r+1}} {\mathrm e}^{-s_{0} {t}_{k}^{2}} \, R_{f_{k}}\right) \\
&= \textstyle \frac{\sqrt{2 \pi \rho}}{\sqrt{\pi(2r+1)}} \textstyle \sum\limits_{k=1}^{N}  \omega_{k} + \frac{\sqrt{2\rho} (r+1)}{\sqrt{\pi}(2r+1)} \sum\limits_{k=1}^{N} \omega_{k} \, {\mathrm e}^{-2s_{1} {t}_{k}^{2}} \,  R_{f_{k}} \\
&= \textstyle \frac{\sqrt{2 \pi \rho}}{\sqrt{2r+1}}  + \frac{\sqrt{2\rho} (r+1)}{\sqrt{\pi}(2r+1)} \sum\limits_{k=1}^{N} \omega_{k}\,  {\mathrm e}^{-2s_{1} {t}_{k}^{2}} \,  R_{f_{k}} .
\end{align*}
%where we have used that $\sum\limits_{k=1}^{N} \omega_{k} =  \int\limits_{-\infty}^{\infty} {\mathrm e}^{-t^{2}} dt = \sqrt{\pi}. $
Hence, we arrive at 
$$ \textstyle F(\gamra) = {\gamra}^T {\mathbf H}_N {\gamra} - \sqrt{\frac{2\pi \rho }{2r + 1}} = \frac{\sqrt{2\rho} (r+1)}{\sqrt{\pi}(2r+1)} \sum\limits_{k=1}^{N} \omega_{k} \,  {\mathrm e}^{-2s_{1} {t}_{k}^{2}} \,  R_{f_{k}}. $$

\noindent
3. From Lemma \ref{lemma-Hermite} it follows that 
$$ |R_{f_{k}} | <   \sqrt{\pi} \, s_{1}^{N} \, {\mathrm e}^{\frac{s_{0}^{2}}{ 2s_{1}} {t}_{k}^{2}}. $$
Therefore we obtain with $-2s_{1}+ \frac{s_{0}^{2}}{2s_{1}} =1$ and $\sum\limits_{k=1}^{N} \omega_{k}= \sqrt{\pi}$,
\begin{align} \nonumber
\textstyle F(\gamra) &  \le  \textstyle \frac{\sqrt{2\rho} (r+1)}{\sqrt{\pi}(2r+1)} \sum\limits_{k=1}^{N} \omega_{k}\,  {\mathrm e}^{-2s_{1} \, {t}_{k}^{2}} \,  |R_{f_{k}}| \\
\nonumber
& <  \textstyle \frac{\sqrt{2\rho} \, (r+1)}{\sqrt{\pi}\, (2r+1)} \sum\limits_{k=1}^{N} \omega_{k} \,  {\mathrm e}^{-2s_{1} {t}_{k}^{2}} \,  \sqrt{\pi} \,  s_{1}^{N} \, {\mathrm e}^{\frac{s_{0}^{2}}{2s_{1}} {t}_{k}^{2}} \\
\label{app}
&= \textstyle   \frac{\sqrt{2\rho} \, (r+1)}{ 2r+1} \, s_{1}^{N} \, \sum\limits_{k=1}^{N} \omega_{k} \, {\mathrm e}^{ {t}_{k}^{2}}.
%&=  \textstyle 2 \, \frac{\sqrt{2\rho} (r+1)}{(2r+1)} \, (2s_{1})^{N} \,{\mathrm e}^{\frac{1+2r-r^{2}}{2(2r+1)} {t}_{1}^{2}}, 
\end{align}

\noindent
4. Finally, as shown in Lemma \ref{lem2}, we can estimate 
$ M_N=\sum\limits_{k=1}^{N} \omega_{k} \,  {\mathrm e}^{ {t}_{k}^{2}}$ by $C\,  N^{3/2}$ for all $N \ge 1$.
%if $M$ is even, or by $C_o N^{3/2}$, if $N$ is odd. 
Therefore, we obtain 
$$ F(\gamra) \le  \tilde{c} \, s_1^N \, N^{3/2}, $$
%for even $N$
%and $F(\gamra) \le c \, (2s_1)^N \,  N^{3/2}$ for odd $N$,
where   $\tilde{c}$ is independent of $N$.
Exponential decay is achieved for $s_{1} < 1$, i.e., $r < 2+\sqrt{6}$. 
%To eliminate $N^{{3/2}}$ in the estimate, we 
% observe that $N^{3/2} < 3^{{N/2}}$ for all $N \in {\mathbb N}$.
%Replacing the above estimate by $\tilde{c} \,  ( \sqrt{3} \, s_{1})^{N}$ we still achieve exponential decay for $\sqrt{3} \, s_{1} < 1$, which is satisfied for $r \le \frac{2}{\sqrt{3}}( 1 + \sqrt{1+ \sqrt{3}})$ and therefore for $r < 3$.}
Since the exponential decay is obtained for $\gamra$ and $ F(\tilde{\gamra})  < F(\gamra)$, the assertion of  Theorem \ref{maintheorem} follows.
\end{proof}

\subsection{Error estimate in the $L_{2}({\mathbb R})$-norm}

Obviously, since the approximating exponential sum  derived in  Section \ref{infinseg} is  a cosine sum, the error 
$${\mathrm e}^{-t^{2}/2\sigma} - \sum_{k=1}^{N} {\tilde{\gamma}_k} \, {\mathrm e}^{\lambda_{k} t } =  {\mathrm e}^{-t^{2}/2\sigma}- 2\sum\limits_{k=1}^{\lfloor {(N+1)}/2 \rfloor} \tilde{\gamma}_{k} \, \cos(|\lambda_{k}| t)$$ 
on the real line needs to be considered in a weighted $L_{2}({\mathbb R})$ norm to achieve an exponentially decaying error.
However, since  ${\mathrm e}^{-t^{2}/2\sigma}$  itself has exponential decay for ${|t|} \to \infty$, we can approximate ${\mathrm e}^{-t^{2}/2\sigma}$ also by the truncated exponential sum $\chi_{[-T, T]}(t) \, \sum\limits_{k=1}^{N} {\tilde{\gamma}_k} \, {\mathrm e}^{\lambda_{k} t }$, where $\chi_{[-T,T]}$ denotes the characteristic function of the interval $[-T, T]$ for positive $T$, and again obtain exponential decay of this approximation.
%We can show the following 

\begin{theorem}\label{th37}
For  $N >1$, let $ \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k}t} = {2}\sum\limits_{k=1}^{\lfloor {(N+1)}/2 \rfloor} \tilde{\gamma}_{k} \, \cos(|\lambda_{k}| t)$ be the exponential sum computed by Algorithm $\ref{alg11}$ for $\sigma>0$ and $\rho = \frac{\sigma}{2}$. Further, let 
$T:= \sqrt{2 \sigma N \, \ln(2)}$. Then 
\begin{align*}
%\label{errorneu}
R_{T} &:= \textstyle \Big\|  {\mathrm e}^{-\cdot^{2}/2\sigma} - \chi_{[-T,T]}  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k}  {\mathrm e}^{\lambda_{k} \cdot} \Big\|^{2}_{L^{2}({\mathbb R})} \\
&= \textstyle \int\limits_{-\infty}^{\infty}  \Big|  {\mathrm e}^{-t^{2}/2\sigma} - \chi_{[-T,T]}{(t)}  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k}  {\mathrm e}^{\lambda_{k}t} \Big|^{2}  {\mathrm d} t 
 \le  \textstyle \frac{\tilde{c}}{4^{N}} N^{3/2}, %<  \tilde{c} \Big(\frac{\sqrt{3}}{4} \Big)^{N},}
\end{align*}
where the constant $\tilde{c}$ does not depend on $N$.
\end{theorem}

\begin{proof}
With $\rho = \frac{\sigma}{2}$ and $r=\frac{\rho}{\sigma} = \frac{1}{2}$ we find $\frac{r^{2}}{2(2r+1)} = \frac{1}{16}$. Therefore Theorem \ref{maintheorem}  provides for this setting
$$ F(\tilde{\gamra})  = \textstyle \|{\mathrm e}^{{-\cdot^2}/2\sigma} -  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k} \cdot} \|^{2}_{L^{2}({\mathbb R},\rho)} <  \frac{c}{16^{N}} N^{3/2}. $$
For the $L_{2}({\mathbb R})$-error it follows now
\begin{align*}
R_{T} &<  \textstyle F(\tilde{\gamra}) + \int\limits_{-T}^{T} \Big|{\mathrm e}^{{-t^2}/2\sigma} -  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k} t} \Big|^{2} \, \Big(1 - {\mathrm e}^{-t^{2}/\sigma} \Big)\, {\mathrm d} t + 2 \int\limits_{T}^{\infty} {\mathrm e}^{-t^{2}/\sigma} {\mathrm d}{t} .
%\\
% & \quad - 2 \textstyle \int\limits_{T}^{\infty} \Big|{\mathrm e}^{t/2\sigma} -  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k} t} \Big|^{2} \,   {\mathrm e}^{-t^{2}/\sigma} {\mathrm d}{t}.
\end{align*}
By \cite{CDS2003}, we  have 
$$ \textstyle 2 \int\limits_{T}^{\infty} {\mathrm e}^{-t^{2}/\sigma} {\mathrm d}{t} = 2 \sqrt{\sigma}  \, \int\limits_{T/\sqrt{\sigma}}^{\infty} {\mathrm e}^{-t^{2}} \, {\mathrm d} t = \sqrt{\pi \, \sigma} \, \mathrm{erfc} ( \frac{T}{\sqrt{\sigma}} ) < \sqrt{\pi \, \sigma} \, {\mathrm e}^{-T^{2}/\sigma}. $$
Furthermore, 
\begin{align*}
I_{T}(\sigma) &:= \textstyle \int\limits_{-T}^{T} \Big|{\mathrm e}^{{-t^2}/2\sigma} -  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k} t} \Big|^{2} \, \Big(1 - {\mathrm e}^{-t^{2}/\sigma} \Big)\, {\mathrm d} t \\
&= \textstyle \int\limits_{-T}^{T} \Big|{\mathrm e}^{{-t^2}/2\sigma} -  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k} t} \Big|^{2} \,{\mathrm e}^{-t^{2}/\sigma} \,  ({\mathrm e}^{t^{2}/\sigma} - 1) \, {\mathrm d} t  \\
&\le  \textstyle \max\limits_{t \in [-T,T]} \Big({\mathrm e}^{t^{2}/\sigma} - 1 \Big) \, \int\limits_{-T}^{T} \Big|{\mathrm e}^{{-t^2}/2\sigma} -  \sum\limits_{k=1}^{N} \tilde{\gamma}_{k} \, {\mathrm e}^{\lambda_{k} t} \Big|^{2} \,{\mathrm e}^{-t^{2}/\sigma}  {\mathrm d} t  \\
& \le \textstyle \Big({\mathrm e}^{T^{2}/\sigma} - 1 \Big) \,  \frac{c}{16^{N}} N^{3/2}. 
\end{align*}
For $T=\sqrt{2 \sigma N \, \ln(2)}$ we obtain $\frac{T^{2}}{\sigma} = 2N \, \ln 2$ and ${\mathrm e}^{T^{2}/\sigma} = 4^{N}$. 
Therefore, we find the estimate
\begin{align*}
R_{T} &< F(\tilde{\gamra}) + I_{T}(\sigma) + \sqrt{\pi \, \sigma} \, {\mathrm e}^{-T^{2}/\sigma} \\
&\le \textstyle  \frac{c}{16^{N}} \, N^{3/2} + 4^{N} \,  \frac{c}{16^{N}} \, N^{3/2} + \sqrt{\pi \, \sigma} \frac{1}{4^{N}} \le \tilde{c} \frac{1}{4^{N}} \, N^{3/2}.  
\end{align*}
\end{proof}




\section{Relation to other approximation algorithms}
\label{sec:4}
%In this section, we will compare Algorithm \ref{alg11} with other methods to find a good approximation  of the  Gaussian by exponential sums.






%% Figure environment removed






%\section{Relation and comparison to other known methods}
%\label{prony}
%In this section we compare performance of the algorithm described in this paper with Prony-type algorithms which can be also used to construct an exponential sum of good approximation for the Gaussian function.  

 We want to compare Algorithm \ref{alg11} with known numerical approaches for reconstruction of exponential sums, which are based on discrete measurements of the function or its derivatives and can be interpreted as attempts to interpolate the measurements. These methods are therefore again only suboptimal and cannot find the best approximation. Note that there are no error estimates available for any of theses approaches which are comparable to our observations in Section \ref{sec:error}.
 
\subsection{Prony's method based on differential operator}
We compare our approach in Section \ref{infinseg} with the Prony method based on the differential operator,  see e.g.\ \cite{PePl2013},  \cite{StPl2020}.  
In Section \ref{infinseg}, we computed the differential operator of the form (\ref{difoper}) that minimizes $\| D_{N}f\|_{L_{2}({\mathbb R}, \rho)}$.  
Now, we consider only the discrete values $f^{(k)}({t_0})$, $k=0, \ldots , L$ (with $L\ge 2N-1$) for $f(t) = {\mathrm e}^{-t^{2}/2\sigma}$ and  solve the interpolation problem 
\begin{equation}\label{difpr}
 D_N f^{(k)} (t_{0})= f^{(N+k)}(t_{0}) + \sum\limits_{j=0}^{N-1} b_j f^{(j+k)}(t_{0})=0, \qquad  k=0,\ldots ,L-N,
 \end{equation}
 for some suitable $t_{0} \in {\mathbb R}$, to evaluate the coefficient vector ${\mathbf b}=(b_{0}, \ldots , b_{N-1},1)^{T}$ determining $D_{N}$. For $L=2N-1$, this interpolation problem is uniquely solvable, for $L> 2N-1$,  we obtain a least-squares problem.
This approach is again only suboptimal. If $f$ were an exponential sum, then it would lead to the reconstruction of $f$, since for 
$f= \sum\limits_{\nu=1}^{N} \gamma_{\nu} \, {\mathrm e}^{\lambda_{\nu}t}$ we obtain 
\begin{align*}
D_N f^{(k)} (t) &= \sum_{\nu=1}^{N} \gamma_{\nu} \, \lambda_{\nu}^{N+k} \, {\mathrm e}^{\lambda_{\nu}t} + \sum\limits_{j=0}^{N-1} b_j  \Big( \sum_{\nu=1}^{N} \gamma_{\nu} \, \lambda_{\nu}^{j+k} \, {\mathrm e}^{\lambda_{\nu}t} \Big) \\
&= \sum_{\nu=1}^{N} \gamma_{\nu}  \lambda_{\nu}^{k} \, \Big( \lambda_{\nu}^{N} + \sum\limits_{j=0}^{N-1} b_j  \lambda_{\nu}^{j} \Big) \, {\mathrm e}^{\lambda_{\nu}t} =0
\end{align*}
with $P_{N}(\lambda)$ in (\ref{P}).
The equations in (\ref{difpr}) can be rewritten as 
\begin{equation}\label{prmat}
{\mathbf H}_{L-N,N+1}  \mathbf{b}^{T} =0,
\end{equation}
with the Hankel matrix 
\begin{equation}\label{hankmat}
{\mathbf H}_{L-N,N+1}=\left( f^{(k+\ell)}(t_{0}) \right)_{k=0,\ell=0}^{L-N-1,N}.
\end{equation}
For $f(t) = {\mathrm e}^{-t^{2}/2\sigma}$ and $t_{0}=0$, the entries of the Hankel matrix are obtained in the form
\begin{align} \label{fk0}
f^{(k)}(0) &= \frac{{\mathrm d}^{k}}{{\mathrm d} t^{k}} {\mathrm e}^{-t^{2}/2\sigma}\left|_{t=0} \right. = (-1)^{k} \, (2\sigma)^{-\frac{k}{2}} \, H_{k}(0) 
=  \left\{ \begin{array}{ll}  (-1)^{{\frac{k}{2}}} \,  \sigma^{-\frac{k}{2}} \, (k-1)!! &  k \, \mathrm{even} \\
 0 &  k \, \mathrm{odd}.
\end{array} \right.
\end{align}




%Then frequencies $\lambda_1,\ldots,\lambda_N$ are computed as zeros of the characteristic polynomial 
%\begin{equation}\label{charpol}
%P_N(\lambda)=\lambda^{N}+\sum_{j=0}^{N-1} b_j \lambda^{j}.
%\end{equation}
%As usual for Prony-type methods we can rewrite the system (\ref{difpr}) in a matrix form 
%\begin{equation}\label{prmat}
%{\mathbf H}_{N,N+1}  \mathbf{b}^{T} =0,
%\end{equation}
%with $\mathbf{b}=(b_0,\ldots,b_{N-1},1)$ and a Hankel matrix 
%\begin{equation}\label{hankmat}
%{\mathbf H}_{N,N+1}=\left( f^{(k+\ell)}(t_0) \right)_{k=0,\ell=0}^{N-1,N}.
%\end{equation}
%For more convenient evaluation of entries of a matrix (\ref{hankmat}) for a function $f(t)=\mathrm{e}^{-t^{2}/2\sigma}$, $\sigma>0$, we apply the Rodrigues formula for Hermite polynomials. We get 
%\begin{align}
%f^{(j)}(t_0)=\frac{d^{j}}{dt^{j}}\Big|_{t=t_0} \left( \mathrm{e}^{-t^{2}/2\sigma} \right)= (-1)^{j} \, \sigma^{-j/2} \,  \mathrm{e}^{-t_0^{2}/2\sigma} He_j\left(\frac{t_0}{\sqrt{\sigma}} \right). \label{enthan}
%\end{align}
 Similarly as in Section \ref{appreal} we can show that the wanted frequencies $\lambda_1, \ldots, \lambda_N$, i.e., the zeros of the characteristic polynomial $P_{N}$ in (\ref{P}) are eigenvalues of matrix pencil 
\begin{equation}\label{matpenhan}
\lambda {\mathbf H}_{L-N, N}(0)-{\mathbf H}_{L-N,N}(1),
\end{equation}
where ${\mathbf H}_{L-N, N}(0) = (f^{(k+\ell)}(0))_{k=0,\ell=0}^{L-N-1,N-1}$ and 
${\mathbf H}_{L-N, N}(1) = (f^{(k+\ell)}(0))
_{k=0,\ell=1}^{L-N-1,N}$ are submatrices of ${\mathbf H}_{L-N, N+1}$. 
To solve the matrix pencil problem in a numerically stable way, we again we employ the  SVD of the matrix  ${\mathbf H}_{L-N,N+1} $ in (\ref{hankmat}) of the form 
%\begin{equation}\label{SVD2}
${\mathbf H}_{L-N,N+1}  = {\mathbf U}_{L-N} \, {\mathbf D}_{L-N,N+1} {\mathbf W}_{N+1}$, 
%\end{equation} 
with unitary square matrices ${\mathbf U}_{L-N}$ and ${\mathbf W}_{N+1}$.
Then, the frequencies can be also found by solving the matrix pencil problem 
% and   where the first $N$ diagonal entries of ${\mathbf D}_{L-N,N+1}$ are taken as frequencies 
%$$ {\mathbf D}_{L-N,N+1} = \left( \begin{array}{cc} \mathrm{diag} \left(  (\sigma_{j})_{j=1}^{N} \right) & {\mathbf 0}  \end{array} \right) \in {\mathbb R}^{N \times (N+1)}, $$
%$\sigma_{1}, \ldots , \sigma_{N}$ 
%%denote the positive singular values of ${\mathbf A}_{N,N+1}$ in decreasing order.
%Similarly the singular value decompositions of ${\mathbf A}_{N}(0)$ and ${\mathbf A}_{N}(1)$ can be directly  derived  from (\ref{SVD2})  and the matrix pencil in (\ref{matpen}) now reads
%\begin{eqnarray*}
% & & \lambda \left(  {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,1:N)  \\
% & &  \hspace{5cm} -  \left(  {\mathbf U}_{N} \, {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,2:N+1),
%\end{eqnarray*}
%or equivalently
%$$ \lambda \left(   {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,1:N) -  \left(  {\mathbf D}_{N,N+1} {\mathbf W}_{N+1} \right)(1:N,2:N+1). $$
%Multiplication from the left with $\left( \mathrm{diag}( \sigma_{j}^{-1} )_{j=1}^{N} , {\mathbf 0} \right) \in {\mathbb R}^{N \times (N+1)}$  yields
%the matrix pencil
%$$
%\lambda \,  {\mathbf W}_{N+1}(1:N,1:N) - {\mathbf W}_{N+1}(1:N,2:N+1)
%$$
%or
\begin{equation}\label{matpen3}
\lambda \,  {\mathbf W}_{N}(0) - {\mathbf W}_{N}(1)
\end{equation}
with ${\mathbf W}_{N}(0)={\mathbf W}_{N+1}(1:N,1:N) $, ${\mathbf W}_{N}(1)={\mathbf W}_{N+1}(1:N,2:N+1)$. 
Then, the  frequencies $\lambda_1,\ldots,\lambda_N$ can be found as eigenvalues of the matrix $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}  {\mathbf W}_{N}(1)^{T},$
where $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}$ denotes the Moore-Penrose inverse of  ${\mathbf W}_{N}(0)^{T}$.

Finally, the coefficients $\gamma_j$ can be computed by solving the overdetermined linear system
\begin{equation}\label{coefpr}
\sum\limits_{j=1}^{N} \gamma_j \,  \lambda_j^{k} =f^{(k)}(0), \ k=0,\ldots, L,
\end{equation}
where again values $f^{(k)}(0)$ are given by (\ref{fk0}). This method is summarized in Algorithm \ref{alg3}. The numerical complexity of Algorithm {\ref{alg3}} is governed by the SVD of the Hankel matrix of size $L-N \times N+1$ and is again  $\mathcal{O}(N^{3})$ flops for $2N-1 \le L \le c N$ for some constant $c$. 

In Figure \ref{fig21}, we compare Algorithm \ref{alg11} and Algorithm \ref{alg3} with regard to the weigthed  $L_{2}({\mathbb R},\rho)$-norm. 
%Since Algorithm {\ref{alg3}} is not based on minimization of some norm we compute here $L_{2}({\mathbb R},\rho)$ for both methods.  We present the decay of approximation error with respect to order of exponential sum $N$ in Fig.  \ref{fig21}.  
We also observe that Algorithm \ref{alg11} provides better approximation error and higher numerical stability for larger $N$.

% Figure environment removed
    

\begin{algorithm}[ht]
\caption{Algorithm for construction of exponential sum of good approximation for the function $f(t)=\mathrm{e}^{-t^{2}/2\sigma}$ }
\label{alg3}
\small{
\textbf{Input:} parameters $\sigma>0$, $N \in \nn$ the order of an exponential sum, $t_0=0$, $L \ge 2N-1$;


\begin{enumerate}
\item Create a Hankel matrix
$
{\mathbf H}_{L-N,N+1}= (f^{(k+\ell)}(0))_{k=0,\ell=0}^{L-N-1,N}$ with $f^{(k)}(0)$ given in (\ref{fk0}) 
%\left(  (-1)^{k+\ell} \, \sigma^{-(k+\ell)/2} \,  \mathrm{e}^{-t_0^{2}/2\sigma} He_{k+\ell}\left(\frac{t_0}{\sqrt{\sigma}} \right) \right)_{k=0,\ell=0}^{N-1,N}
%$$
and compute the SVD ${\mathbf H}_{L-N,N+1}  = {\mathbf U}_{L-N} \, {\mathbf D}_{L-N,N+1} {\mathbf W}_{N+1}$. 
\item Compute the frequencies  $\lambda_1,\ldots ,\lambda_N$ as eigenvalues of the matrix  $\left(  {\mathbf W}_{N}(0)^{T} \right)^{\dagger}  {\mathbf W}_{N}(1)^{T},$
with matrices as in (\ref{matpen3}), where $\left({\mathbf W}_{N}(0)^{T} \right)^{\dagger}$ denotes the Moore-Penrose inverse of  ${\mathbf W}_{N}(0)^{T}$.


\item Compute the coefficients  $\gamma_{j}$, $j=1,\ldots ,N$ as the least-squares solution of the system
$$
\sum\limits_{j=1}^{N} \gamma_j \,  \lambda_j^{k} =f^{(k)}(0), \ k=0,\ldots, L.
$$
\end{enumerate}

\noindent
\textbf{Output:}  pairwise distinct frequencies $\lambda_j$, $j=1,\ldots ,N$;coefficients  $\gamma_{j}$, $j=1, \ldots ,N$, such that \\
 \phantom{\textbf{Output:}}
$\sum\limits_{j=1}^{N} \gamma_j \mathrm{e}^{\lambda_j t}$ approximates $f(t)=\mathrm{e}^{-t^{2}/2\sigma}$.}
 %\phantom{\textbf{Output:}} for  a function $f(t)=\mathrm{e}^{-t^{2}/2\sigma}$.
\end{algorithm}

\subsection{Comparison with ESPRIT and ESPIRA }
Now we compare our method with Prony type methods such as ESPRIT and ESPIRA described in \cite{DPR23}.  Note that for Prony type methods  for approximation by cosine sums,  nodes $\mathrm{e}^{\lambda_j}$ are computed as eigenvalues of Hankel+Toeplitz or Loewner matrix pencil and they are based on using a finite number of samples.  The coefficients $\gamma_j$ are computed as a solution of some system of linear equation that can be written using generalized Vandermonde or Cauchy matrices (see \cite{DPR23}) for details.  Our method presented in Section \ref{appreal} is a matrix pencil method for matrices with special structure. The eigenvalues of this matrix pencil  are exactly zeros of scaled Hermite polynomial.
Such an interpretation of the eigenvalues  is not possible for the matrix pencils appearing for ESPRIT or ESPIRA.  
For a further comparison of Prony-kind methods and the differential approximation method we refer to  \cite{Scha79}.
%For which other functions differential approximation method can be used to explain location of frequencies in exponential approximation problem and for further connection to Prony method see  \cite{Scha79}.}}
 
Using Prony type methods we can construct approximations only in a finite segment.  We consider the approximation of a function $f(t)=\mathrm{e}^{-t^2/2\sigma}$ by a cosine sum of order $N$ in the interval  $[-b,b]$,  where $b$ is large enough (such that  outside $[-b,b]$ the Gaussian function close to zero).  We take $\sigma=1.25$ and $\rho=\sigma/2$ and construct a cosine sum of length  $N=8$ to approximate the Gaussian.  For  ESPRIT and ESPIRA we use $L=100$ sample values at points $t_k=\frac{h(2k+1)}{2}$,  $k=0,\ldots,99$,  with $h=\frac{\pi}{50}$.  
We obtain the maximum errors $1.2\cdot 10^{-8}$ for ESPRIT and $5.8\cdot10^{-7}$ for ESPIRA, see Figure \ref{fig33}.  Using the differential Prony type method we get the error $4.3\cdot 10^{-9}$. 
In Figure \ref{fig33}, we compare ESPRIT, ESPIRA and Algorithm 1 (with $\rho=\sigma/2$) in the maximum norm and in the weighted $L_\infty([0,5],\mathrm{e}^{-t^2/4\rho})$ norm. 
 
% Figure environment removed

Finally, we note that the direct application of the ESPRIT  or the ESPIRA algorithm for approximation of the Gaussian by an exponential sum $\sum_{j=1}^{N} \gamma_{j} z_{j}^{t}$ (without the restriction that the approximation has to be a sum of cosines with real coefficients) we obtain smaller absolut errors, as shown in Figure \ref{figneu}, where the performance of the two algorithms depends on the number of given data. ESPIRA outperforms ESPRIT for $L> 350$, for smaller $L$, ESPRIT provides the smaller error.
The obtained approximating functions are indeed complex. We have used here the algorithms from \cite{DPP21}.  Note also these methods have indeed larger complexity, $\mathcal{O}(L^{3})$ for ESPRIT and  $\mathcal{O}(L(N^{3}+\log L))$  for ESPIRA.
 
% Figure environment removed




%% Figure environment removed
\section{Conclusion}
In this paper, we have applied the differential approximation method to construct a cosine sum that approximates the Gaussian ${\mathrm e}^{-t^{2}/2\sigma}$ with exponential decay.  
The proof of this error estimate in Section \ref{sec:error}  heavily relies on the Gauss-Hermite quadrature  formula. 

At the first glance, our method seems to be not related to previous ideas to approximate entire functions by short exponential sums.
The known other methods for completely monotone functions employed the representation by the Laplace transform, see \cite{Bbook}, which can then be discretized by a quadrature rule.
In \cite{Greengard2022}, a quadrature rule for the inverse Laplace transform formula (\ref{green}) has been applied. 
We conclude that  the convergence analysis for the approximation with exponential sums is always closely related to quadrature formulas that converge exponentially for special analytic functions. Consequently, there is also always a close relation to
rational approximation, since these quadrature rules are related to rational or meromorphic functions \cite{TW14, T21}.
 For further study of the connection between exponential and rational approximation we refer to \cite{BM09, DPP21, PP19, WDT21}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement}
The authors thank Lennart Hรผbner and Yannick Riebe (University Gรถttingen) for several discussions and improvements of this manuscript.
The authors gratefully acknowledge support by the German Research Foundation in the framework of the RTG 2088. The second author acknowledges support by the EU MSCA-RISE-2020 project EXPOWER  and of the DFG CRC 1456.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\small
%\bibliography{myBib}{}
%\bibliographystyle{plain}
\begin{thebibliography}{99999}

\bibitem{AS} M. Abramowitz  and I.A. Stegun,   Handbook of Mathematical Functions with Formulas, Graphs,  and Mathematical Tables,  United States Department of Commerce,  National Bureau of Standards, Applied Mathematics Series - 55, 1964.

\bibitem{AW84} R. Askey and J. Wimp,  Associated Laguerre and Hermite polynomials, {\em Proc. Roy. Soc. Edinburgh, Sect. A} \textbf{96}(1-2) (1984),  15--37.

\bibitem{BCS}
M. Bachmayr, H. Chen, and R. Schneider, Error estimates for Hermite and even-tempered Gaussian approximations in quantum chemistry
{\em Numer. Math.} \textbf{128} (2014), 137--165.

\bibitem{Bell} R.  Bellman,  Methods of nonlinear analysis,  New York, Academic Press, 1970-73,  2 v. 

\bibitem{BM05}
G. Beylkin and L. Monz\'{o}n, On approximation of functions by exponential sums. {\em Appl. Comput. Harmon. Anal.}, \textbf{19} (2005), 17--48.

\bibitem{BM09}
G.~Beylkin and L.~Monz\'{o}n,
\newblock Nonlinear inversion of a band-limited {F}ourier transform.
\newblock {\em Appl.  Comput.  Harmon.  Anal.},  \textbf{27} (2009), 351--366.

\bibitem{BM10}
G.~Beylkin and L.~Monz\'{o}n,
Approximation by exponential sums revisited, {\em Appl.  Comput.  Harmon.  Anal.} \textbf{28}(2) (2010), 131--149.

%\bibitem{ASJQ2020} S. M. Abrarov, R. Siddiqui, R. K. Jagpal, and B. M. Quine (2020) A rational approximation of the Fourier transform by integration with exponential decay multiplier, https://arxiv.org/pdf/2001.07533.pdf

%\bibitem{AAK}  V.M. Adamjan, D.Z. Arov, and M.G. Krein, Analytic properties of the Schmidt pairs of a Hankel operator and the generalized Schur-Takagi problem, Mat. Sb. 86 (1971), 34โ75 (in Russian).

\bibitem{Bbook} D. Braess, Nonlinear Approximation Theory, Springer, Berlin, 1986.

\bibitem{Br95}
D. Braess, Asymptotics for the approximation of wave functions by exponential sums. J. Approximation Theory 83, 93-103 (1995) 


\bibitem{BH2005} D. Braess and W. Hackbusch, Approximation of $1/x$ by exponential sums in $[1, \infty)$, {\em IMA J. of Num. Analysis}, \textbf{25} (2005),  685--697.

\bibitem{BH09}
D. Braess and W. Hackbusch, On the efficient computation of high-dimensional integrals
and the approximation by exponential sums, In R. DeVore and A. Kunoth (eds.), Multiscale,
nonlinear and adaptive approximation, pp. 39โ74, Springer, Berlin, 2009.

\bibitem{Cbook} Cheney,  E.W. Introduction to Approximation Theory.  McGraw-Hill,  New York,  London,  1996.

\bibitem{CDS2003}
M. Chiani,  D. Dardari,  and M. K. Simon,
New exponential bounds and approximations
for the computation of error probability in
fading channels, {\em IEEE Trans. Wirel. Commun.} \textbf{2}(4) (2003), 840--845.


\bibitem{CW}  D. Coppersmith and S. Winograd,  Matrix multiplication via arithmetic progressions, {\em J. of Symb. Comput.}, \textbf{9} (1990),   251--280.


\bibitem{CL20}
A. Cuyt, W.-s. Lee, and M. Wu, High accuracy trigonometric approximations of the real Bessel functions of the first kind, {\em Comput. Math. and Math. Phys.}, \textbf{60} (2020), 119--127.

\bibitem{DerPr2020} N. Derevianko and J. Prestin,  Approximation of Gaussians by spherical Gauss-Laguerre basis in the weighted Hilbert space,  {\em Electron.  Trans.  Numer. Anal.} , \textbf{52} (2020),  249--269.


\bibitem{DPP21}
N.~Derevianko,  G.~Plonka, and M.~Petz
\newblock From ESPRIT to ESPIRA: Estimation of signal parameters by iterative rational approximation, {\em IMA J. Numer. Anal.} \textbf{43}(2), (2023), 789--827.

\bibitem{DPR23}
N.~Derevianko,  G.~Plonka,  and R. Razavi
\newblock ESPRIT versus ESPIRA for reconstruction of short cosine sums and its application, {\em Numer. Algor.},  \textbf{92} (2023)  437--470.



\bibitem{DI2007}
H. Dette and L. A. Imhof,  Uniform approximation of eigenvalues in Laguerre and Hermite $\beta$-ensembles by roots of orthogonal polynomials, 
{\em Trans.  Am.  Math.  Society},
\textbf{359}(10)   (2007),   4999--5018.

%\bibitem{EII}  Erd\'{e}lyi, A. et al., Tables of Integral Transforms, vol. II. McGraw Hill, New York, 1954.

\bibitem{GR} I.S. Gradshteyn and I.M. Ryzhik,  Table of Integrals, Series, and Products, translated from the Russian by Scripta Technica, Inc., Academic Press,  Boston, London,  1994.


\bibitem{H19}
W. Hackbusch,  Computation of best $L_{\infty}$ exponential sums for $1/x$ by Remezโ algorithm,
{\em Comput. Visual Sci.} \textbf{20} (2019),  1--11.

\bibitem{HR10}
T. Hangelbroek and  A. Ron,
Nonlinear approximation using Gaussian kernels,
J. Funct. Anal. \textbf{259}(1) (2010), 203--219.

\bibitem{Hildebrandt}
F.B. Hildebrand,  Introduction to Numerical Analysis, 2nd edn. McGraw-Hill, New York (1974).

\bibitem{Greengard2022} S. Jiang and L. Greengard,  Approximating the Gaussian as a sum of exponentials and its applications to the Fast Gauss Transform,  {\em Commun.  Comput.  Phys.} \textbf{31} (2022),  1--26.

\bibitem{Kam1976} D. W. Kammler,  Chebyshev approximation of Completely monotonic functions by sums of exponentials,  {\em SIAM J.  Numer.  Anal.},  \textbf{13} (1976),   761--774.


\bibitem{Kam1979} D. W. Kammler,  Least squares approximation of completely monotonic functions by sums of exponentials, {\em SIAM J. Numer. Anal.} \textbf{16} (1979), 801--818.

\bibitem{Kam1981} D.W. Kammler,  Differential approximation of completely monotonic functions, {\em SIAM J. Numer. Anal.} \textbf{18} (1981), 900--918.

\bibitem{Kam81}  D. W. Kammler,  A sufficient condition for the rapid convergence of differential approximation, {\em J. of Approx. Theory}, \textbf{33} (1981),  264--267.

\bibitem{K23}
Y. M. Koyama, Exponential sum approximations of finite completely monotonic functions, arXiv:2301.08931.


\bibitem{Kuo12} F.Y. Kuo amd H. Wo\'zniakowski, Gauss-Hermite quadratures for functions from Hilbert spaces with Gaussian reproducing kernels, {\em BIT Numer. Math.} \textbf{52} (2012), 425--436.

\bibitem{Lee2009}  S. L. Lee,   Approximation of Gaussian by scaling functions
and biorthogonal scaling polynomials, {\em Bull. Malays. Math. Sci. Soc (2)} \textbf{32}(3) (2009), 261--282.

%\bibitem{MB2019} E. H. Ait Mansour and S. Barth, Efficient approximation of Gaussian function for signal and image processing applications, 2019 Signal Processing Symposium (SPSympo), 2019, pp. 1-6. % doi: 10.1109/SPS.2019.8882020.

%\bibitem{AAA} Yuji Nakatsukasa, Olivier Sรจte, and Lloyd N. Trefethen (2018)
%The AAA Algorithm for Rational Approximation, SIAM J. Sci. Comput., 40(3), A1494โA1522.


\bibitem{OS95}
M.R. Osborne and G.K. Smyth, A modified Prony algorithm for exponential function fitting, {\em SIAM J. Sci. Comput.} \textbf{16}(1) (1995), 119--138.

\bibitem{PePl2013} T. Peter, G. Plonka, A generalized Prony method for reconstruction of sparse sums of eigenfunctions of linear operators, {\em Inverse Probl.} \textbf{29}(2) (2013), 025001.

\bibitem{PP19} G. Plonka and V. Pototskaia.  Computation of adaptive Fourier series by sparse approximation of exponential sums.  {\em J.  Fourier Anal.  Appl.}  \textbf{25}(4) (2019), 1580--1608.


\bibitem{Pbook} G. Plonka,  D. Potts,  G. Steidl,  and M. Tasche,  Numerical Fourier Analysis,  Birkh\"auser,  Basel, 2019.

\bibitem{PT13}
 D. Potts, M. Tasche, Parameter estimation for nonincreasing exponential sums by Prony-like methods, Linear Algebra Appl. \textbf{439} (2013), 1024--1039.

%\bibitem{Rice62} J.R. Rice,  Chebyshev Approximation by Exponentials, SIAM J. Appl. Math. \textbf{10}(1) (1962), 149--61.

\bibitem{RK89}
R. Roy and T. Kailath, ESPRIT estimation of signal parameters via rotational invariance techniques,  {\em IEEE Trans. Acoust. Speech Signal Process.} \textbf{37} (1989), 984--995. 

\bibitem{Salzer}  H.E. Salzer, R. Zucker, and R. Capuano,  Table of the zeros and weight factors of the first twenty Hermite polynomials. {\em J. Res. Natl. Inst. Stand. Technol.} \textbf{48} (1952),  111--116.

\bibitem{Scha79} R. Schaback,  Suboptimal exponential approximations, {\em SIAM J. Numer. Anal.} \textbf{16} (1979), 1007--1018.


\bibitem{StPl2020} K. Stampfer and G. Plonka,  The generalized operator based Prony method,  {\em Constr.  Approx.} \textbf{52} (2020),  247--282. %https://doi.org/10.1007/s00365-020-09501-6

%\bibitem{SSV2012} Schilling R.L., Song R., Vondraฤek Z.,
%Bernstein Functions. Theory and Applications (second ed.), de Gruyter Studies in Mathematics, vol. 37, Walter de Gruyter and Co., Berlin (2012).

\bibitem{Tow}
A. Towsend, T. Trogdon, and S. Olver, Fast  computation of Gauss quadrature  nodes and weights  on the whole real line, {\em IMA J. Numer. Anal.} \textbf{36}(1), (2016), 337--358.

\bibitem{T21}
L.N. Trefethen, Exactness of quadrature formulas, {\em SIAM Review}  \textbf{64}(1) (2022), 132--150.

\bibitem{TW14}
L.N. Trefethen and J.A.C. Weideman, The exponentially convergent trapezoidal rule, {\em SIAM Review} \textbf{56}(3), 385--458.


\bibitem{Szego}
G. Szeg\"o, Orthogonal Polynomials,  AMS,  Volume XXIII, 1939.


 \bibitem{WDT21}
H.~Wilber, A.~Damle, and A.~Townsend,  Data-driven algorithms for signal processing with trigonometric rational functions, 
\newblock {\em SIAM J. Sci. Comput.}, \textbf{44}(3) (2022), C185--C209.


\bibitem{ZP19}
R. Zhang and G. Plonka, Optimal approximation with exponential sums by maximum likelihood modification of Prony's method,
{\em Adv. Comput. Math.} \textbf{45}(3) (2019), 1657--1687. 
 
 
\end{thebibliography}








\end{document}
