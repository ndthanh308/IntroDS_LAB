%------------------------------------------------------------------------
\section{Introduction} \label{intro}
%------------------------------------------------------------------------

Datacenter applications impose increasingly stringent requirements for network communication, such as persistently high throughput, ultra-low latency ($\mu s$ scale), and low CPU overhead (to cut OpEx). To meet it, Remote Direct Memory Access (RDMA) is emerging as the de-facto networking technology going beyond 40Gbps links. Many tech giants have adopted RDMA into their production datacenters~\cite{gao2021cloud, zhu2015congestion, mittal2015timely, guo2016rdma}. These datacenters host various network-intensive applications, such as deep learning~\cite{li2014communication, jiang2020unified}, cloud storage~\cite{gao2021cloud, kalia2014using}, graph exploration~\cite{shi2016fast}, \etc, which benefit greatly from the underlying RDMA communication.

However, native RDMA transports only support one-to-one reliable connection (RC)~\cite{rocev2}, which mismatches various applications with group communication patterns~\cite{gao2021cloud, dean2004mapreduce, li2014communication} (\eg, one-to-many). Multicast is prevalent in datacenter applications ($\S$\ref{multicast-pattern}). Specifically, previous work shows that multicast is the top-2 communication pattern in a High-performance Computing (HPC) cluster~\cite{mpiusage}. In High-performance Linpack (HPL)~\cite{hpl}, an HPC's benchmark application, more than 90\% communication traffic is the multicast pattern.  

To meet applications' requirements, a straightforward solution is to provide a multicast primitive. However, while there are some existing multicast solutions~\cite{openmpi, nccl, shahbaz2019elmo, Infiniband} ($\S$\ref{multicast}), none of them simultaneously achieve two of the performance requirements: 1)~optimal forwarding multicast traffic; 2)~fully unleashing the distinguished RDMA capabilities.

On one hand, some primary distributed frameworks (\eg, MPI~\cite{openmpi} and NCCL~\cite{nccl}) choose to develop their private multicast protocols upon the RDMA one-to-one RC transport (\aka, application-layer multicast). Thus they can efficiently utilize the prominent RDMA capabilities. However, application-layer multicast cannot achieve optimal traffic forwarding, resulting in inefficient bandwidth utilization and communication bottleneck~\cite{diab2022orca, shahbaz2019elmo}. Although many multicast algorithms are proposed to mitigate the bottleneck (\eg, Ring and Double-binary tree~\cite{gibiansky2017bringing, mlsl, oneccl}), they inevitably incur longer communication distance and higher latency resulting from intermediate nodes' data forwarding. 

On the other hand, in-fabric multicast can achieve optimal multicast forwarding with efficient bandwidth utilization and minimized communication distance. However, the in-fabric multicast cannot benefit from advanced RDMA capabilities. In particular, IP-based multicast~\cite{crowcroft1988multicast, diab2022orca, shahbaz2019elmo} only supports layer-3 routing without any transport-layer functionality, thus is incompatible with the RDMA RC transport. IB multicast, defined in IB standard~\cite{Infiniband}, only supports UD transport. Therefore, applications that adopt IB multicast cannot utilize the advanced one-sided \rdwrite\footnote{The RDMA \rdread doesn't fit into the multicast context.}, the extended message size, and the hardware-supported reliability. 

In addition to the performance requirements, the multicast solution should also meet the practical deployment requirements in production environments. The fundamental challenge comes from the commodity RDMA Network Interface Cards (RNICs). Commodity RNICs provide limited capability for flexible programming~\cite{gao2021cloud, zhu2015congestion}. For instance, the complete network stacks of \cite{cx5} are statically built-in with specialized circuits.

In this work, we focus on simultaneously (\romannumerber{1})~supporting the optimal multicast forwarding, (\romannumerber{2})~fully exploiting the prominent features of RDMA, and (\romannumerber{3})~satisfying the deployment requirements. To this end, we propose \sys, an RDMA-accelerated multicast solution for datacenter networks ($\S$\ref{design}). Firstly, \sys inherits the classical in-fabric distribution manner~\cite{crowcroft1988multicast, diab2022orca, shahbaz2019elmo} to achieve optimal multicast forwarding. Secondly, \sys re-purposes the existing RDMA RC logic with careful switch coordination to process multicast traffic, unleashing RDMA's superior competencies. Finally, as Gleam reuses the standard RC transport, it is compatible with the large-scale deployed commodity RNICs.

The key challenge behind Gleam is how to integrate the optimal-forwarded multicast traffic with the existing RC logic ($\S$\ref{key-chalge}). Specifically, there are two main issues. Firstly, the current connection-oriented logic~\cite{rocev2} is targeted for one-to-one connection, which cannot directly support one-to-many data delivery. Secondly, the existing reliability logic~\cite{zhu2015congestion,guo2016rdma} is designed for single feedback (including ACK, NACK, CNP, \etc) stream from a single receiver; thus, multiple feedback streams in multicast can confuse it and degrade the overall performance.  

To reuse the connection-oriented logic of RC ($\S$\ref{connection-handle}), \sys elaborately extends the widely-adopted multicast forwarding table structure~\cite{crowcroft1988multicast}. Then, based on the extended table, \sys replaces the connection-related header fields in multicast data packets to match different QPs/connections\footnote{We use QP and connection equivalently in this paper.} on different receivers. As a result, \sys achieves a virtual one-to-many connection among multicast members, providing applications with the advanced capability of RDMA connected transport service.

%In this way, \sys achieves a virtual one-to-many connection among multicast members, and each member only maintains one QP for this group. As a result, \sys not only provides applications with the advanced capability of RDMA connected-transport, but also prevents intermediate nodes from forwarding traffic.

To reuse the reliability logic of RC ($\S$\ref{ack-aggregation}), \sys performs the many-to-one feedback aggregation in the fabric. In particular, \sys aggregates ACK so that the sender receives a unicast-like ACK stream, which is compatible with the existing ACK-interpretation logic. Moreover, \sys carefully filters NACK packets to enable the sender to correctly detect and retransmit the lost packet. Consequently, \sys can reuse the standard reliability logic and provides hardware-based reliability.

We implement a fully functional \sys switch ($\S$\ref{imple}), connected to four commodity servers. Each server is equipped with an unmodified commodity RNIC. \sys is evaluated through extensive testbed experiments and simulations ($\S$\ref{eva}). The testbed experiments show that \sys accelerates the multicast pattern by up to 2.2$\times$, and improves the realistic application's performance, such as 2.9$\times$ lower HPL communication time and 2.7$\times$ higher data replication throughput. Meanwhile, large-scale simulations demonstrate that \sys can maintain a high performance in large-scale topologies and a satisfying goodput when packet loss occurs.

% Figure environment removed

We summarize the design space for multicast solutions in Fig. \ref{fig:designspace}. \textit{To the best of our knowledge, this is the Ô¨Årst work to completely leverage RDMA capabilities to empower multicast communication.} The key contributions of this work are summarized below:
\begin{itemize}
\vspace{-0.2cm}
\item We observe that the reliable multicast delivery can be efficiently realized through fitting the multicast traffic into RDMA's performant RC transport.\vspace{-0.2cm}
\item We design \sys{}, an RDMA-accelerated multicast protocol that simultaneously supports optimal multicast forwarding, advanced capabilities of RDMA, and compatibility with commodity RNICs. \sys{} abstracts a virtual one-to-many connection and aggregates feedback in the middle fabric, which addresses the incompatibility between multicast traffic and existing RC logic.\vspace{-0.2cm}
\item We implement a fully functional \sys prototype. Extensive experiments demonstrate \sys's significant acceleration in multicast communication.\vspace{-0.2cm}
\end{itemize}

%\item We design a generic RDMA multicast protocol, called \sys, that achieves high application throughput while maintaining connection in multicast group.\vspace{-0.1cm}
%\item \sys provides reliability by performing the in-network ACK aggregation to support the general adoption among applications.\vspace{-0.1cm} 
%\item We implement a \sys prototype that demonstrates its friendly-deployment feature. We evaluate the performance of \sys through extensive testbend and simulation experiments, and show its significant improvement in task completion time and CPU utilization.\vspace{-0.1cm}
%\end{itemize}

%In one-to-many connection, all nodes only keep one connected QP and receivers are not involved for medium data forwarding; thus \sys relieves the connection scalability issue in sender and avoids additional CPU overhead. Besides, the sender only sends a copy of data and lets network to perform an optimal data distribution, which efficiently utilizes network bandwidth without redundant traffic transmitted. Thus the application throughput is maximized. 
%In one-to-many connection, the connected QP is standard which is supported by existing RDMA connected transports. Besides, through the many-to-one ACK aggregation, \sys can reuse the existing reliable functionality in RC transport. As a result, \sys can be built upon the standard RC transport and maintain a same programming interface to applications. Thus \sys can be friendly deployed upon the large-scale commodity RNICs, and requires minimized application code changes.

%We design \sys, which satisfies the above all design goals. To achieve high application performance, \sys maintains a one-to-many connection between sender and multiple receivers to utilize all RDMA primitives\footnote{The RDMA READ is not supported as it don't fit the multicast context.} In one-to-many connection, all nodes only keep one connected QP and receivers are not involved for medium data forwarding; thus \sys relieves the connection scalability issue in sender and avoids additional CPU overhead. Besides, the sender only sends a copy of data and lets network to perform an optimal data distribution, which efficiently utilizes network bandwidth without redundant traffic transmitted. Thus the application throughput is maximized. 

%Datacenter applications exhibit their increasing demand for high communication performance, such as high throughput \cite{gao2021cloud}, low latency \cite{msbing, kalia2014using}, and low CPU overhead \cite{gao2021cloud, msbing, kalia2014using}. Remote Direct Memory Access (RDMA) is emerging as the de-facto networking technology in high-speed datacenters due to its significant improvement in communication. In particular, RDMA enables a host to directly communicate with a remote host with kernel bypassing, which can avoid the traditional CPU overhead. Moreover, RDMA can achieve promising microsecond RTT and hundreds of Gbps throughput by offloading the network stack to hardware. Many applications \cite{gao2021cloud, li2014communication, kalia2014using, chen2016fast, jiang2020unified} have developed upon various types of RDMA transports. 

%Native RDMA transport only supports the one-to-one reliable connection \cite{rocev2}. Various distributed datacenter applications, on the other hand, exhibit group communication patterns in the form of one-to-many (\aka, multicast) and many-to-many communication, where many-to-many communication can be composed by multiple multicasts \cite{gao2021cloud, dean2004mapreduce, li2014communication}. For example, multicast can be used to deliver storage replicas to multiple storage servers in large-scale cloud storage systems \cite{gao2021cloud}. In a distributed machine learning system \cite{li2014communication, jiang2020unified}, multicast can be utilized to distribute the aggregated gradients to workers. 

%Datacenter applications exhibit their increasing demand for high communication performance, such as high throughput \cite{gao2021cloud}, low latency \cite{msbing, kalia2014using}, and low CPU overhead \cite{gao2021cloud, msbing, kalia2014using}. Remote Direct Memory Access (RDMA) is emerging as the de-facto networking technology in high-speed datacenters, and many applications \cite{gao2021cloud, li2014communication, kalia2014using, chen2016fast, jiang2020unified} have developed upon various types of RDMA transports. However, native RDMA transport only supports the one-to-one reliable connection \cite{rocev2}. Various distributed datacenter applications, on the other hand, exhibit group communication patterns that can be efficiently supported by multicast \footnote{the many-to-many communication can also be supported by multicast as it's composed by multiple multicasts} \cite{gao2021cloud, dean2004mapreduce, li2014communication}. For example, multicast can be used to deliver storage replicas to multiple storage servers in large-scale cloud storage systems \cite{gao2021cloud}. In a distributed machine learning system \cite{li2014communication, jiang2020unified}, multicast can be utilized to distribute the aggregated gradients to workers.

%However, application-layer multicast is inefficient, as it incurs a significant reduction in application throughput. In particular, there is inevitable bandwidth waste in the network. On the other hand, end-host either suffers from severe connection scalability issue, or incur extra CPU overhead. All of them are contradictory to the fundamental communication requirements in datacenters \cite{diab2022orca, shahbaz2019elmo, diab2022yeti}. We illustrate two primary application-layer multicast solutions in Fig. \ref{fig:intro:unicast} and \ref{fig:intro:overlay}.

%existing solutions \cite{crowcroft1988multicast,Infiniband,diab2022orca, shahbaz2019elmo}, including IP and IB multicast\cite{crowcroft1988multicast,Infiniband}, cannot provide connectivity and reliability (\ie, only supporting best-effort delivery). They are incompatible with the commonly used RDMA reliable connection (RC) transport, thus the excellent features of RC (including full RDMA primitives support, low CPU overhead, network stack offloading, \etc) have to be compromised to utilize them \cite{monga2021birds, dragojevic2014farm}. The lack of connectivity and reliability is the reason why the application-layer multicast is produced.

%In-fabric multicast can achieve the ideal bandwidth utilization, as we illustrate in Fig. \ref{fig:intro:infabric}. However, existing solutions \cite{crowcroft1988multicast,Infiniband,diab2022orca, shahbaz2019elmo}, including IP and IB multicast\cite{crowcroft1988multicast,Infiniband}, cannot provide connectivity and reliability (\ie, only supporting best-effort delivery). They are incompatible with the commonly used RDMA reliable connection (RC) transport, thus the excellent features of RC (including full RDMA primitives support, low CPU overhead, network stack offloading, \etc) have to be compromised to utilize them \cite{monga2021birds, dragojevic2014farm}. The lack of connectivity and reliability is the reason why the application-layer multicast is produced.

%Specifically, the commodity RNICs have been widely adopted since their low price and major occupation in the market. However, they provide little or even no programmability. 

%Deploying a new service in the datacenter should not only meet the performance demand but also tackle the practical deployment limitations. The two significant deployment limitations we face are the limited programmability of commodity RDMA Network Interface Cards (RNICs) and the resource limitation of commodity programmable smart NICs. Specifically, commodity RNICs provide little or even no programmability. Taking the ConnectX-5 RNIC from Mellanox as an example, all-most network stacks are fixed in specialized circuits. On the other hand, despite the programmability support, smart NICs are limited by their restricted deployment scope. Moreover, the on-chip resource in smart NICs are limited due to the unaffordable cost and power consumption; thus, it's challenging to implement a separate multicast protocol in smart RNICs with such limited on-chip resources.

%as well as maintain connections between multicast members to utilize benefits over RDMA connected transport

%Deploying new services in the datacenter should not only meet the performance demand but also the practical deployment requirements. The significant challenge of deploying multicast in RDMA comes from the commodity RDMA Network Interface Cards (RNICs). Commodity RNICs have been adopted in large-scale datacenter but they provide limited or even no programmability \cite{gao2021cloud, zhu2015congestion}. Taking the Mellanox ConnectX-5 \cite{cx5} as an example, its network stacks are almost fixed in specialized circuits.

%First, the new multicast protocol should requires minimized application program changes to enable its fast adoption. Moreover, we must handle the programmability limitation in the large-scale deployed commodity RDMA Network Interface Cards (RNICs). Taking the Mellanox ConnectX-5 \cite{cx5} as an example, all-most network stacks are fixed in specialized circuits.

%meeting both the performance and deployment demands of RDMA multicast. Thus we aim to design a generic RDMA multicast framework that primarily focuses on three design goals. (\romannumerber{1}) \textit{High application performance}: the framwork is expected to achieve high application throughput and leverage all the primitives of RDMA; (\romannumerber{2}) \textit{Reliability}: multicast communication must be reliable to support most types of applications; (\romannumerber{3}) \textit{Easy deployment}: the framwork must incur minimized application program changes and be compatible with existing commodity RNICs.

% (\romannumerber{1}) Optimal bandwidth utitization: data should be distributed efficiently in the network to ensure no transmission redundancy. (\romannumerber{2}) Avoiding CPU overhead: the multicast transport should be fully supported in hardware to avoid additional CPU overhead. (\romannumerber{3}) Minimizing resource consumption in RNIC: the resource consumption in RNIC should be minimized. (\romannumerber{4}) Be friendly to large-scale deployment: the design should carefully tackle the practical limitations to enable large-scale deployment.

%Specifically, \sys  maintains a one-to-many connection between sender and multiple receivers. The multicat members (including sender) only keep one connected QP and receivers are not involved for medium data forwarding, which relieves the connection scalability issue in sender and avoids additional CPU overhead in medium nodes. Moreover, \sys supports all RDMA primitives\footnote{The RDMA READ is not supported as it don't fit the multicast context.} by this connected communication. Upon the one-to-many connection, \sys supports the optimal in-network data distribution. Thus sender only sends a copy of data and lets network to perform an optimal data distribution, which efficiently utilizes network bandwidth without redundant traffic transmitted. 

%\sys is composed by three components: \textit{bandwidth-efficient data distribution}, \textit{one-to-many connection handling} and \textit{many-to-one ACK aggregation}. The first component enables no redundant traffic in the network. The second lets each multicast member to maintain only one standard RC connection, thus there is no CPU overhead for connection management. Besides, the second and third enable the multicast traffic to share the same connection management and reliability logic as used by unicast. Thus \sys incurs no additional resource consumption, and can be deployed upon the existing RC mode supported by commodity RNICs. 

%Specifically, \sys supports the \textit{bandwidth-efficient data distribution} by building an optimal multicast tree in network, as the existing in-fabric multicast solutions does. In \sys, the sender only sends a single copy of the data, and the network generates multiple data copies and distribute them into multiple paths, if necessary. 
%\sys building the specific multicast forwarding table in switches

%Besides the efficient data distribution, \sys performs in-network \textit{one-to-many connection handling} to let multicast member treat one-to-many connection just as normal one-to-one connection, thus avoiding additional CPU overhead for connection management in RNIC. In particular, \sys registers connection-related states in forwarding table, and automatically change options in packet header to enable the simoutanours communication between one sender connection and many receiver connections.