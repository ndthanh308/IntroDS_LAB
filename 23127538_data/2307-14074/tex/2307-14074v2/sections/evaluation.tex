%-------------------------------------------------------------------------------
\section{Evaluation}\label{eva}
%-------------------------------------------------------------------------------

We evaluate \sys's performance through extensive testbed and simulation experiments. In testbed experiments, we first examine \sys using the micro-benchmark ($\S$\ref{micro}); then we deploy two realistic applications ($\S$\ref{realapp}) upon \sys and show their performance improvement. The testbed's topology and configuration are described in $\S$\ref{imple}. In simulations ($\S$\ref{simu}), we evaluate \sys in an extended large-scale topology and explicitly estimate the goodput of \sys over different packet loss rate. Our experiment results reveal that:

\begin{itemize}
\vspace{-0.2cm}
\item \sys accelerates \mpibcast by up to 2.2$\times$ compared with OpenMPI.\vspace{-0.2cm}
\item \sys improves the performances of the realistic applications. Specifically, \sys speeds up the HPL communication by up to 2.9$\times$, improves the IOPS throughput of storage data replication by 2.7$\times$, and reduces the single IO latency by up to 65$\%$.\vspace{-0.2cm}
\item \sys achieves consistently high performance in large-scale topologies. Its communication speedup scales well with the multicast group size. Its performance gain is also resilient to packet losses. \vspace{-0.2cm}
\end{itemize}

\subsection{Micro-benchmark} \label{micro}
We first evaluate \mpibcast, one of the basic MPI primitives, through the modified OpenMPI~\cite{openmpi} and UCX~\cite{ucx}. We measure the job completion time (JCT) as the main metric. In our testbed, we select one server as \mpibcast source and three servers as \mpibcast receivers, forming a small one-to-three multicast group. The original OpenMPI performance is then compared with that of \sys. 
%\todo{(1) OpenMPI detial: which multicast algorithm, ring or unicast, used.}
% \todo{(1) \sys detail: which RDMA primitive used. (2) OpenMPI detial: which RDMA primitive and multicast algorithm, ring or unicast, used.}

We measure the JCT under various multicast message sizes, and calculate \sys's acceleration radio, shown in Fig.~\ref{fig:exp:bcast}. \sys achieves lower JCTs across various message sizes compared with OpenMPI. With larger message size, \sys achieves lower JCT. For instance, \sys achieves 12$\mu s$ reduction (1.6$\times$ acceleration radio) with 64KB message and 124$ms$ reducetion (2$\times$ acceleration radio) with 1GB message. For message size larger than  128KB, \sys stably achieve about 50$\%$ less JCT. 

The performance improvment of \sys stems from its optimal bandwidth utilization. \sys delivers data efficiently without bandwidth wastage. In contrast, OpenMPI inevitably wastes bandwidth because it transmits identical data multiple times. Consequently, with larger message size, \sys saves more bandwidth and thus offers lower JCT.
Through these micro-benchmark experiments, we validate the correctness of our \sys prototype and demonstrate that \sys can speed up multicast communication under simple settings.

\subsection{Realistic Applications} \label{realapp}

We deploy two realistic applications with multicast patterns (\S\ref{multicast-pattern}): HPL (\S\ref{eval:hpl}) and storage data replication (\S\ref{eval:storage}); and evaluate their performances with and without \sys. 

\subsubsection{High-performance Linpack (HPL)} \label{eval:hpl}
We integrate \sys into HPL~\cite{hpl}, and build a HPL cluster using our prototype testbed to evaluate the performance of HPL. Each HPL epoch contains three steps:  \textit{Panel Factorization}, \textit{Panel Broadcast (PB)} and \textit{Update}. The \textit{PB} is a standard multicast transmission, and the \textit{Update} includes a communication phase, called \textit{Row Swap (RS)}, which can be implemented with multicast. The volume of multicast traffic is around several GBytes at the first epoch and linearly decrease to nearly zero as the computing proceeds.

We test HPL's \textit{PB} and \textit{RS} stages separately. We measure the total JCT (including computation and communication) of HPL when speeding up \textit{PB} and \textit{RS} solely, and only the communication time of \textit{PB} and \textit{RS}. Moreover, the \textit{RS} communication volume depends on the real-time computing result; thus we evaluate \textit{RS} with two data distributions, which are uniform and centralized. The original HPL implementation is selected as alternative solution compared with \sys, where the \textit{PB} and \textit{RS} are recommended to use the \textit{increasing-ring} (abbreviated as \textit{ring} in following HPL evaluations) and \textit{long} algorithms, respectively~\cite{hpl}. Both algorithms are overlay multicast solutions implemented by multiple underlay RC unicasts.

As results illustrated in Fig.~\ref{fig:exp:hpl-total} and Fig.~\ref{fig:exp:hpl-commu}, \sys reduces JCT in all settings. Specifically, with communication time only, \sys reduces the JCT of \textit{PB}, \textit{RS (uniform)}, and \textit{RS (centralized)} by 67$\%$, 18$\%$, and 46$\%$, respectively. With both computation and communication time included, \sys reduces the JCT of \textit{PB}, \textit{RS (uniform)}, and \textit{RS (centralized)} by up to 12$\%$, 4.67$\%$, and 9.55$\%$, respectively.

% Figure environment removed

\parab{Tolerance of Data Distribution} Note that \sys achieves better improvement in non-random data distribution (46$\%$) compared with uniform data distribution (18$\%$). This is expected as the performance of \sys doesn't depend on data distribution. However, the \textit{long} algorithm is sensitive to data distribution. The \textit{long} algorithm achieves best performance when data is uniformly distributed, but degrades if the data is non-random distributed or even centralized, as more communication is needed to make the data evenly distributed before data exchanging between neighbors. 
%The tolerance of data distribution makes \sys more general that can be adopted by much applications.

% Figure environment removed

\subsubsection{Storage Data Replication}\label{eval:storage}
We also integrate \sys into a proprietary commodity distributed storage system. We utilize our testbed prototype to evaluate the performance of storage data replication. We select one node as client and three nodes as servers, thus forming a 3-copies writing setting. We compare the performance of \sys with 3-unicasts, and the one-copy writing is also measured as a baseline. The \rdwrite operation is used in all solutions. In 3-unicasts, client maintains three RC connections with three servers. In \sys, client only maintains one RC connection. We measure the writing throughput using IOPS (IO per second) as the main metric, and the single IO latency.

\parab{Throughput.} We set IO size as 8KB and let the client keep writing data to three servers. We measure the average IOPS achieved by the client. As Fig.~\ref{fig:exp:write-thro} shows, \sys can achieve nearly optimal writing IOPS, about 1.167M IOPS, which is comparable with the ideal one-copy writing's 1.188M IOPS. On the contrary, the 3-unicasts only achieves 0.413M IOPS, only about 35$\%$ of \sys. With \sys, the application goodput can achieve 76.5Gbps (1.1M $\times$ 8KB), while 3-unicasts only can reach 26.24Gbps. This is because the client only need to send one copy of data for 3-replications writing with \sys, effectively mitigating the bandwidth bottleneck at the client's output link. 

\parab{IO Latency.} In addition, we measure the single IO latency over different IO size, shown in Fig.~\ref{fig:exp:write-latency}. The single IO latency is defined as the period between the client submits the \rdwrite request and receives the CQE from the RNIC. The result shows that \sys achieves significantly lower latency than 3-unicasts, and delivers a comparable latency with the ideal one-copy. For instance, \sys reduces IO latency by about 40$\%$ and 60$\%$ in 64KB and 512KB, compared with 3-unicasts. Moreover, \sys accomplishes lower latency in larger IO size. As result shown, the gap between \sys and 3-unicasts is enlarged as IO size increasing.

The advantage of \sys comes from the reduction of end-host storage stacks involvement and the total transmitted traffic volume. 3-unicasts transmits the identical data and experiences the same storage stacks three times. As a result, the IO latency is increased. On the other hand, \sys enables the client to run the storage stack and transmit the data only once, which effectively reduces the IO latency.

\subsection{Simulations} \label{simu}
We provide complementary experiments using ns-3~\cite{ns3}. We first build a large-scale topology and evaluate \sys over different multicast scales. We then simulate a lossy environment and measure \sys over different packet loss rates.

\parab{Simulation setting.}
We simulate a large-scale 3-layer fat-tree topology with 16384 servers and a 1:1 oversubscription ratio. Each server is equipped with one 200Gbps RNIC, and each switch has 64 200Gbps ports. 

\parab{Workload and Metrics.} We again use the HPL pattern as the simulation workload. We provide various workload scales, each labeled as $N*N$, meaning that $N*N$ nodes form a logical $N*N$ matrix to run HPL. In each workload, each row node would perform one \textit{PB}, and each column node would perform one \textit{RS}. With \sys, the \textit{PB} procedure involves $N$ multicast groups transmitting simultaneously, each consisting of $N$ group members. The \textit{RS} involves another different $N$ multicast groups. The \textit{ring} algorithm for \text{PB} and \textit{long} algorithm for \textit{RS} are simulated as comparisons with \sys. We use the total JCT, \ie, the total communication time of \textit{PB} and \textit{RS}, as the experiment metric.  

The simulations are time-costly because of its large topology and the more than four million flows. To accelerate the simulation, we integrate MPI framework into the ns-3 simulator to enable the parallel simulation with multiple threads. The simulation run time is reduced by 70\% with 12 threads.

% Figure environment removed

% Figure environment removed

\parab{Large-scale multicast.}
We measure the average JCT of HPL workload over different multicast scales. In particular, we select five scales: $8*8$, $16*16$, $32*32$, $64*64$, $128*128$. For each scale, we run multiple times and calculate the average JCT. The original HPL implementation (\textit{ring} for \text{PB} and \textit{long} for \textit{RS}) is compared with \sys. Results in Fig.~\ref{fig:exp:large-scale} show that \sys achieves lower JCT in all multicast scales with a reduction from 62$\%$ to 73$\%$. For instance, \sys{} reduces the JCT from 13.7ms to 5.17ms (62$\%$ reduction) over $8*8$ scale, and from 19.4ms to 5.17ms (73$\%$ reduction) over $128*128$ scale. The improvement increases with the growing multicast scale, consistent with the results in $\S$\ref{micro}.

Note that \sys's JCT doesn't scale up with increasing multicast scale. The reason is that, in \sys, the larger multicast scale only brings more parallel-transmitted multicast groups, while the traffic volume transmitted by each group stays the same. Because of \sys's group-level load-balancing (Appendix~\ref{apx:regis}), more multicast groups don't incur much congestion deterioration. In contrast, with the \textit{ring} and \textit{long} algorithms, the number of parallel unicast flows expands linearly with the growing multicast scale, which causes congestion and results in JCT degradation.

\parab{Loss tolerance.}
We compare the JCT of HPL workload under different packet loss rates, from $10^{-8}$ to $10^{-3}$, which are emulated via randomly discarding packets in the middle switches. We choose two group sizes (64 and 512) to evaluate. As shown in Fig.~\ref{fig:exp:tct-loss}, \sys{} presents good loss tolerance as it can maintain lower JCT compared with original HPL implementation (\textit{ring} and \textit{long}) under all loss rates. In particular, \sys{} reduces the JCT by 46\%-52\% with size 64 and 86\%-94\% with size 512, respectively. 

To better show \sys's reaction to loss, we calculate the goodput under different packet loss rates, \ie, the normalized throughput compared with setting without loss, shown in Fig.~\ref{fig:exp:thro-loss}. \sys's goodput decreases largely than \textit{ring} and \textit{long} because the multicast sender is responsible for multiple receivers, \ie, when any receiver loses a packet, the sender must retransmit it. In contrast, the sender in \textit{ring} and \textit{long} is only responsible for one receiver. 

Even though \sys is more sensitive to packet loss, \sys imposes at most 10\% goodput degradation with a loss rate not larger than 0.01\%, which is a common packet loss rate in data center~\cite{zhu2015congestion}. Even under an excessive 0.1\% loss rate, \sys can maintain 42\% goodput and perform better (gains a 7$\times$ lower JCT) than the original HPL implementation.






