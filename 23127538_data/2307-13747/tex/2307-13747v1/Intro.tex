
 
Clustering is a fundamental problem in computer science,
which arises in approximation algorithms, unsupervised learning, computational
geometry, spam and community detection, image segmentation,
databases, and other areas \cite{hansen1997cluster,shi2000normalized,arthur2006k,schaeffer2007graph,fortunato2010community,coates2012learning,tan2013data}. The goal of clustering is to find a structure in data by grouping
together similar data points. Specifically, for $k$-clustering objectives, the goal is to output a set of $k$ cluster \emph{centers} from the underlying metric space; the cost of the clustering is then evaluated by various functions of the distances of each point to their nearest center. Due to the importance of the problem, clustering has been extensively studied and many algorithms \cite{
arya2001local, kanungo2002local, jain2003greedy,  charikar2005improved, arthur2007k, ahmadian2019better, byrka2017improved}
and heuristics \cite{lloyd1982least} have been proposed to solve the offline version of the
problem, where the data is fixed and provided to the algorithm up front. 
%Clustering algorithms optimize a givenobjective function which characterizes the quality of a clustering. 
%One of the classical and best studied clustering objectives is the $k$-center objective.

%While the approximately of many clustering tasks is fairly well understood in the static setting, the same
%is not true for \textit{dynamic datasets}. 

Recently, due to the proliferation
of enormous datasets and the rise of modern computational paradigms where data is constantly changing, there has been significant interest in developing
\emph{dynamic} clustering algorithms~\cite{cohen2016diameter,lattanzi2017consistent,ChaFul18,DBLP:conf/esa/GoranciHL18,schmidt2019fully,goranci2019fully,HenzingerK20,henzinger2020dynamic,fichtenberger2021consistent}, which can maintain a clustering of a set of points $P$ subject to updates.
The two primary settings in which these problems are studied are the \emph{incremental} setting, where $P$ undergoes solely point insertions, and the \textit{fully dynamic} setting, where points can be both inserted to and deleted from $P$.
The fully dynamic setting is strictly more general and  captures a much wider class of applications. 

\iffalse
Oftentimes, the goal of a dynamic algorithm is to maintain a good approximation to the optimal solution while minimizing the processing per-update time. In this setting, the problem of dynamic clustering has been studied extensively for multiple objectives. However, there are many properties that are desirable for a dynamic clustering algorithm beyond a small update time. A central example is \emph{consistency} of the solutions \cite{lattanzi2017consistent,cohen2019fully,jaghargh2019consistent,fichtenberger2021consistent}. Specifically, a dynamic clustering algorithm is said to be \emph{consistent} if the number of changes it makes to its set of $k$ cluster centers is small. In particular, it should be smaller than the trivial $k$ changes obtained from independently reclustering the data after every step. The number of changes made to the cluster centers after each point insertion or deletion is known as the \emph{recourse}. 
\fi

In this paper, we focus on dynamic \emph{consistent} algorithms, following a recent line of work~\cite{lattanzi2017consistent,cohen2019fully,jaghargh2019consistent,fichtenberger2021consistent}.
Specifically, a dynamic clustering algorithm is said to be \emph{consistent} if the number of changes it makes to its set of $k$ cluster centers is small. In particular, it should be smaller than the trivial $k$ changes obtained from independently reclustering the data after every step. The number of changes made to the cluster centers after each point insertion or deletion is known as the \emph{recourse}\footnote{To be more precise, we define the recourse as the number of swaps needed, that is, when $|C_1|=|C_2|=k$ and $|C_1 \triangle C_2| = 2$, we say that the recourse is one. }.
We note that a different commonly studied objective in the area of dynamic algorithms, including algorithms for clustering problems, is minimizing the running time of each insertion or deletion operation \cite{charikar2004incremental,ChaFul18,schmidt2019fully,henzinger2020fully,goranci2019fully,cohen2019fully, bateni2023optimal}.

Consistency is an important measure from both a theoretical and a practical point of view. From a theoretical perspective, establishing the optimal recourse achievable while maintaining a given approximation is a fundamental combinatorial question about how significantly updates to the data can change the set of all approximately optimal solutions. Problems for which low-recourse algorithms exist, therefore, must possess some smoothness in their set of approximate solutions. Consistency is also closely related, and in some settings equivalent, to the notion of low-recourse algorithms in the online algorithms literature, which has received significant attention in recent years for various problems \cite{gu2013power,gupta2014online,lkacki2015power,megow2012power,epstein2014robust,sanders2004online,gupta2014maintaining,bernstein2019online,gupta2017online,gupta2020fully,bhattacharya2023chasing}. Here, the typical restrictions of online algorithms are relaxed by allowing some of the past decisions of the algorithm to be changed. 

From a practical perspective, for many modern computational tasks, it is indeed possible to make changes to the solution maintained by the algorithm, rather than having decisions be totally irrevocable. For instance, load balancers must make decisions for which machine to place an item on the fly, however, data may later be reshuffled across machines, although this data shuffling is a costly operation. Furthermore, clustering is often employed as a preliminary step as part of a larger ML pipeline, such as for feature engineering and classification tasks; in such settings, changing the clustering can involve costly recomputations of the downstream pipeline. 

This notion of consistent clustering was first introduced by \cite{lattanzi2017consistent}, who demonstrated that in the \emph{incremental setting}, where points are inserted but not deleted, one can maintain a $O(1)$-approximate solution for commonly studied $k$-clustering objectives, including $k$-center, $k$-means, and $k$-median with $O((k^2 \polylog n) / n)$ amortized recourse,\footnote{For $k$-center, they showed that the classic ``doubling algorithm'' of \cite{charikar2004incremental} already achieved the tight $O((k \log n) / n)$ recourse bound.} and also proved a $\Omega((k \log n)/n)$ lower bound. Their results were later strengthened by \cite{fichtenberger2021consistent} to a $O((k \cdot \polylog n)/n)$ amortized recourse bound, which is tight up to $\polylog n$ factors in the incremental setting. Thus, for incremental clustering, it is possible to achieve a recourse that is significantly sublinear in the number of updates to the data. 

Observe that achieving subconstant recourse bounds (when $n \gg k$) relies heavily upon the fact that the input is incremental. Namely, in the fully dynamic setting, this is clearly no longer possible: simply by repeatedly inserting and deleting a highly significant point, any bounded-approximation algorithm will be forced to add and remove that point from its set of centers after every time step. Thus, for nearly all $k$-clustering objectives, $\Omega(1)$ is a lower bound for the \textit{amortized} recourse of any fully dynamic clustering algorithms. On the other hand, to date, there has been little success in obtaining better algorithms that come close to meeting this lower bound. In fact, for most $k$-clustering tasks, such as $k$-center, $k$-means, and $k$-median, the best-known algorithm is simply to fully recluster the data after every update, resulting in a trivial $O(k)$ amortized recourse. 

The aforementioned discrepancy between the upper and lower bounds for consistent fully dynamic clustering algorithms represents a strong gap in the literature. The lack of improved algorithms for the fully dynamic setting, despite the strong progress in the incremental setting, gives some indication that perhaps nothing better can be done than repeated reclustering of the data. This state of affairs motivates the following key question:


%For instance, notice that if the aspect ratio of the points is polynomially bounded, then in the incremental the cost of the optimal solution for many clustering tasks can only increase by a constant factor $O(\log n)$ times


%has received significant attention in the machine learning literature \cite{cohen2019fully,jaghargh2019consistent, lattanzi2017consistent}. 
\vspace{0.5em}

\begin{mdframed}
\begin{quote}
 \begin{center}
%  {\it  Do consistent clustering algorithms exist in the fully dynamic setting with better than the trivial $k$ recourse?}
  {\it  Are there consistent $k$-clustering algorithms with $o(k)$ recourse in the fully dynamic setting?}
 \end{center}
 \end{quote}
\end{mdframed}
\vspace{-0.5em}

Furthermore, the existing incremental consistent algorithms for $k$-clustering~\cite{charikar2004incremental,lattanzi2017consistent,fichtenberger2021consistent} all leverage the fact that the cost of the optimal solution can only increase over time.
Specifically, whenever the (approximate) cost of the solution increases by a constant factor, they recompute the solution from scratch.
Because of that, even though their amortized recourse can be subconstant, the \emph{worst-case} recourse can be as large as $\Omega(k)$.
This motivates the second question:

\vspace{0.5em}
\begin{mdframed}
\begin{quote}
 \begin{center}
%  {\it  Do consistent clustering algorithms exist in the fully dynamic setting with better than the trivial $k$ recourse?}
  {\it  Are there consistent $k$-clustering algorithms with $o(k)$ \emph{worst-case} recourse?}
 \end{center}
 \end{quote}
\end{mdframed}
\vspace{-0.5em}

In this work, we provide an affirmative answer to both above questions for the classic $k$-centers objective \cite{hsu1979easy,gonzalez1985clustering,hochbaum1986unified,feder1988optimal,bern1997approximation}. Recall that, given a metric space $(\mathcal{X},d)$ and a set of points
$P\subseteq\mathcal{X}$, the $k$-center problem is to output
a set $C\subset\mathcal{X}$ of at most $k$ centers such that the
maximum distance of any point $p\in P$ to the nearest center $c\in C$
is minimized; in other words, the goal is to minimize $\max_{p\in P}d(p,C)$ where $d(p,C)=\min_{c\in C}d(p,c)$. In the offline setting, $k$-center admits several well-known greedy $2$-approximation algorithms \cite{gonzalez1985clustering,hochbaum1986unified}. Moreover, it is known to be NP-hard to approximate the objective to
within a factor of $(2-\epsilon)$ for any constant $\eps>0$~\cite{hsu1979easy}.
Even restricted to Euclidean space, it is still NP-hard to approximate beyond a factor of $1.822$ \cite{feder1988optimal,bern1997approximation}. Thus, the majority of the literature on dynamic $k$-center focuses on obtaining constant approximations.

Our main result is a fully dynamic algorithm that maintains a constant approximation with constant amortized recourse, thereby matching the aforementioned consistency and approximation lower bounds up to a constant. Specifically, we prove the following theorem:



\begin{restatable}{theorem}{main}
\label{thm:main}
There exists a fully dynamic deterministic algorithm that maintains a constant-approximate solution of the $k$-center problem and obtains worst-case recourse of at most $1$ for an insertion and $2$ for a deletion.
\end{restatable}

Our algorithm works by assigning each point a \emph{rank}, which is a non-negative integer with the following property. For any $k'$ and at any time, the $k'$ points of the highest rank form a constant approximate solution to the $k'$-center problem.
Hence, in a sense, it solves the $k$-center problem for all possible values of $k$ at the same time.
%is based on a  idea: as the cost of the solution can both increase and decrease dramatically after some operations, we will not maintain only the solution for the given value of $k$. Instead, we maintain a rank function of every point such that 

While maintaining such a rank function is a standard idea in the dynamic clustering literature, it turns out that in our setting it is quite complicated to maintain the rank function while achieving constant recourse. 
We end up solving this problem by maintaining \emph{two} rank functions, a \emph{geometrical} rank function whose properties are directly tied to the distances between points of $P$, and a \emph{smooth} rank function that aims to approximate the geometrical rank function while only changing slowly. Ultimately, the properties of the geometric rank function control the  approximation ratio of our algorithm, while the smooth rank function ensures that we achieve constant recourse. The interplay between the two rank functions is quite subtle and we capture it by a hierarchical forest structure that we call a \emph{leveled forest}. 

\paragraph{Roadmap}
The paper is structured as follows. In \cref{sec:overview} we informally overview our algorithm and explain why it achieves both constant approximation ratio and worst-case constant recourse. \cref{sec:formal} then contains a formal analysis of the algorithm. 