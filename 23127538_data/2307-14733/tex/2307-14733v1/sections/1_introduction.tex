

\section{Introduction}
Unit testing is an important testing paradigm that focuses on the correctness of a single software component~\eg{class in \textsc{Java}}~\cite{DBLP:books/daglib/0020331}.
In practice, a class is commonly implemented to leverage other classes' functionality.
These classes constitute \emph{test dependencies}, which are invoked when testing the \emph{class under test} (CUT).
To test the CUT in isolation, developers often substitute dependencies with  \emph{test doubles}~\cite{mcdonough2021test}, which play the role of dependencies for testing purpose only.
There are five main types of test doubles: dummy, stub, mock, spy, and fake~\cite{mcdonough2021test}.
Following the popular terminology of the \mockito{} framework~\cite{Tool:mockito}, we use the term \emph{mock objects}~\cite{DBLP:journals/ese/SpadiniABB19} to collectively refer to dummy, stub, and mock test doubles.
The mock objects in \mockito can act like any of these three types of test doubles in a unit test~\cite{Tool:mockito}.
In a nutshell, mock objects are designed to simulate the reactions of test dependencies~\ie{via stub calls} or validate their interactions with the CUT~\ie{via mocking calls}~\cite{thomas2002mock}.

\definecolor{stubbg}{HTML}{FCF3D5}
% Figure environment removed

Listing~\ref{lst:stubbing-example} illustrates a \junit test case with mock objects implemented using the \mockito{} framework~\cite{Tool:mockito}.
The unit test aims to validate the login function of \code{LoginService}, which leverages its test dependency \code{UserDao} to establish a database connection.
The test case simulates the dependent database service \code{UserDao} and a database entity \code{User} with two mock objects.
Lines~\ref{code:stubbing-example:stubbing-begin}--\ref{code:stubbing-example:arrange-end} give the stub code that specifies the behaviors of the mock objects when their methods are (indirectly) invoked at Line~\ref{code:stubbing-example:act-end}.
The first invocation of the method \code{findUser} throws an exception to simulate an unstable connection (Line~\ref{code:stubbing-example:first-reaction}).
The second invocation of the method \code{findUser} returns a \code{User} object to simulate a successful database query (Line~\ref{code:stubbing-example:arrange-end}).
The returned \code{User} object is another mock object to simulate a database entity. It returns the SHA-1 digest of a predefined password when its method \code{getPasswordHash} is invoked (Line~\ref{code:stubbing-example:stubbing-begin}).
At Line~\ref{code:stubbing-example:assert-begin}, a mocking assertion \code{verify} checks whether the CUT invoked the  method \code{findUser} twice.

With such mock objects, developers no longer need to set up a database for testing or unplug the network cable to trigger an exception.
Similar to the example, developers often replace dependencies with mock objects and specify their behaviors with stub code when the dependencies are hard to set up, flaky, faulty, or even not yet implemented~\cite{DBLP:journals/ese/SpadiniABB19,DBLP:conf/kbse/ZhuWWLCSZ20}.
The use of mock objects allows developers to focus on the CUT without worrying about the correctness or availability of its dependencies.

Developing and maintaining stub code is challenging.
When developing stub code for a mock object, developers need to carefully consider its possible interactions with the CUT, and simulate the reactions accordingly when its methods are called.
Stub code is tightly coupled with a specific implementation of the CUT (and its dependencies) and is easy to get broken when the implementation of CUT evolves.
Take the test case in Listing~\ref{lst:stubbing-example} as an example.
When the implementation of \code{UserDao}, \code{User}, or \code{LoginService} changes, the stub code will become broken since it no longer specifies the behaviors for the APIs needed by the test case.
For example, when the signature of \code{findUser} is changed from \code{findUser(userName)} to \code{findUser(userName, passwordHash)}, the stub code are broken and does not compile.
In this case, the stub code needs to be updated to adapt to the new implementation.
In real-world projects, such updates need to be done frequently to keep the behaviors of mock objects consistent with the production code~\cite{DBLP:journals/ese/SpadiniABB19}.
This activity is labor-intensive and error-prone~\cite{DBLP:conf/icse/FazziniCCLKGO22}.

Previous works on automatic stub code generation for mock objects rely on a capture-and-replay approach~\cite{DBLP:conf/paste/SaffE04,DBLP:conf/kbse/SaffAPE05,DBLP:conf/kbse/FazziniGO20,DBLP:conf/icsm/JoshiO07,DBLP:conf/sigsoft/ElbaumCDD06}.
Given an executable test case, such techniques generate stub code in three steps: (1) execute the test cases capturing the interactions between the CUT and its dependencies, (2) replace the dependencies with mock objects, and (3) create stub code according to the captured interactions.
As such, capture-and-replay techniques are able to generate stub code for only those test cases without mock objects.
This is because they need to invoke the actual dependencies.
However, the study of Spadini \etal{}~\cite{DBLP:journals/ese/SpadiniABB19} shows that for 83\% of test cases that use mock objects, the mock objects are introduced when the actual dependencies are hard to set up, flaky, or unavailable.
Therefore, capture-and-replay techniques are inapplicable to the majority of situations where mock objects are used.

Our goal is to synthesize stub code for mock objects without capturing the actual behaviors of the dependencies. This is challenging because it requires identifying the desired mock object's behavior for a specific test case.
Indeed, mock objects are often test-specific because the same dependency class often has different behaviors in different test cases~\cite{DBLP:conf/kbse/ZhuWWLCSZ20}.

In regression testing, we want to synthesize stub code to test future versions of the CUT.
In such a context, we do not aim to generate a stub code that makes the test pass or fail depending on whether the current version is faulty or not.
We aim to generate the stub code that makes the test pass on the current CUT version, which aims to detect regression bugs introduced in future versions.
Our observation is that the expected behavior of such test-passing stub code is often encoded in the CUT execution code and test oracles.
For example, consider the test case of Listing~\ref{lst:stubbing-example} without the stub code
highlighted in yellow (Lines~\ref{code:stubbing-example:stubbing-begin} to~\ref{code:stubbing-example:arrange-end}).
The expected behavior of the test case is given by Lines~~\ref{code:stubbing-example:act-begin} and~\ref{code:stubbing-example:act-end}, which specify how the CUT should be invoked, and Lines~\ref{code:stubbing-example:assert-begin} and~\ref{code:stubbing-example:assert-end}, which specify the expected output of the method under test.
The desired stub code  (Lines~\ref{code:stubbing-example:stubbing-begin} to~\ref{code:stubbing-example:arrange-end}) is the one that makes the test pass.

In this paper, we present \textbf{\tool}, the first technique to automatically generate stub code without executing the actual dependencies.
Given an incomplete test case without stub code, \tool{} leverages the CUT execution code and test oracles as specifications to guide the synthesis of stub code to make the tests pass for the current implementation.
As mentioned above, the synthesized stub code satisfies the expected behavior of the test case in the regression testing scenario and could detect bugs in future versions.

Due to the huge search space of possible stub code, it is infeasible to randomly or systematically explore all the possible candidate stub code to find a test-passing one.
As such, we design \tool{} based on an evolutionary algorithm that drives the search by examining the runtime behavior of each candidate stub code.
In particular, \tool{} employs a novel fitness function that captures how close a candidate stub code is to test-passing stub code.
The fitness function captures several runtime aspects like the distance between the expected and actual value of each oracle assertion, which effectively directs the search towards the candidates that are more likely to pass the test.

Notably, \tool{} can also be used for repairing stub code that is broken due to code evolution.
In such cases, \tool{} prioritizes the selection of code elements in the broken stub code when constructing a new candidate stub code.
Indeed, test-passing stub code might be syntactically similar to the obsolete one.

\tool{} has two application scenarios: (1) When adding a new test case, developers can write the code that exercises the CUT using mock  objects and specify the expected behavior for the test case by writing oracle assertions \ie{\junit assertions and mocking calls}. \tool{} will automatically generate the stub code.
(2) When the stub code in some test cases becomes obsolete due to software evolution, developers can run \tool{} to repair the broken stub code.
By supporting these two scenarios, \tool{} helps relieve developers from the tedious manual effort of stub code development and maintenance.

We evaluated \tool{} on \numOfBenchmarkEntries{} test cases collected from \numOfBenchmarkProjects{} open-source projects in both application scenarios. Although modern program synthesis tools (e.g., GitHub Copilot) can suggest possible statements to complete test cases, they do not aim to synthesize  test-passing stub code.
Since there is no related tool that generates stub code under these scenarios, we compared with a variant of \tool{} based on an unguided strategy.
In the both scenario, \tool{} successfully generates stub code for 76\% of the test cases in half of the repetitions.
Compared with the unguided variant, \tool{} has a higher success rate and synthesizes the stub code faster.
Moreover, 57\% of the synthesized stub code have identical fault detection capability as those written by developers (measured by mutation coverage in Table~\ref{tab:fidelity-generation} and Table~\ref{tab:fidelity-repair}).

To summarize, this paper makes three major contributions:
\begin{itemize}
	\item
	      We design and implement \tool{}, the first automatic stub code synthesis technique that can effectively synthesize stub code for the test dependencies of a target unit test case.
	      We equipped \tool{} with a novel fitness function that examines the runtime behaviors of the test case to guide the search of the test-passing stub code.
	\item
	      We construct the first benchmark for evaluating stub code generation and repair techniques. It is composed of \numOfBenchmarkEntries{} test cases from \numOfBenchmarkProjects{} open-source projects.
	      \tool{} can effectively synthesize stub code for incomplete test cases in both application scenarios and it outperforms the baselines as well as its unguided variant.
	\item We publicly release \tool{} and the benchmark to facilitate future research in this area.
	      The dataset is available at \url{https://doi.org/10.5281/zenodo.7816758}.
\end{itemize}

The remainder of this paper is organized as follows:
Section~\ref{sec:problem-formulation} formulates the stub code synthesis problem with a motivating example and highlights the technical challenges.
Section~\ref{sec:approach} presents the design and implementation of \tool{}.
Section~\ref{sec:evaluation} describes our evaluation of \tool{} on 59 test cases collected from 13 open-source projects.
Section~\ref{sec:related-work} discusses the related work.
Section~\ref{sec:conclusion} concludes the paper and points out possible future work.
