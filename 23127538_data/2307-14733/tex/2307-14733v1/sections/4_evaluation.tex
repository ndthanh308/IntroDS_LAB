\section{Evaluation}\label{sec:evaluation}
This section presents the evaluation of~\tool{}.
Specifically, we aim to answer the following four research questions.
\begin{itemize}
	\item \textbf{RQ1 (Stub Code Generation):}
	      \textit{Is \tool{} effective in generating stub code?}
	\item \textbf{RQ2 (Stub Code Repair):}
	      \textit{
		      Is \tool{} effective in repairing obsolete test cases due to broken stub code?
		      Does it outperform state-of-the-art program repair techniques?
	      }
	\item \textbf{RQ3 (Effectiveness of the Fitness Function):} \textit{Can the fitness function effectively guide stub code synthesis?}
	\item \textbf{RQ4 (Fidelity of the Synthesized Stub Code):} \textit{To what extend does the stub code synthesized by \tool{} preserve the effect of the ground-truth stub code?}
\end{itemize}
\smallskip
RQ1 and RQ2 evaluate \tool{} in the two application scenarios (synthesis and repair of stub code, see Section~\ref{ssec:problem-statement}).
RQ3 evaluates the effectiveness of \tool's fitness function by comparing it with an unguided (random) strategy.
RQ4 compares the fidelity of the stub code synthesized by \tool{} with that written by developers.
Specifically, we evaluate to what extent the test case with synthesized stub code can preserve the runtime effect of the test case with the ground-truth stub code by comparing their executed instructions, execution paths, and ability to kill mutants.

\subsection{Evaluation Subjects}

\paragraph{Benchmark Description.}
To answer the four research questions, we constructed a benchmark of \numOfBenchmarkEntries{} real-world test cases selected from \numOfBenchmarkProjects{} open-source projects (see Table~\ref{tab:subjects}). Each entry in the benchmark contains:
\begin{itemize}
	\item The test case with the removed stub code, which is the input of \tool{} in RQ1.
	\item The broken version of the stub code, which is the additional input of \tool{} in RQ2.
	\item The ground-truth stub code written by developers that makes the test pass, which is used for comparison in RQ4.
	\item The production code and the dependent libraries, which are required to compile and run the test.
\end{itemize}

\paragraph{Project Selection.}
To build the benchmark, we searched on GitHub~\cite{github} for open-source \textsc{Java} projects and sorted the results by the number of stars, which is an indicator of popularity.
We manually went through the top 150 projects to identify those meeting the four criteria below:

\begin{enumerate}
	\item It has at least 1,000 lines of \textsc{Java} code.
	      This is to filter out small projects.
	\item It uses the \mockito{} framework~\cite{Tool:mockito} to simulate/verify the behaviors of test dependencies.
	      This is because we implemented \tool{} based on \mockito{}, which is the most popular mocking framework for \textsc{Java}~\cite{DBLP:conf/qsic/MostafaW14}.
	\item It is not an \textsc{Android} project since \textsc{Android} is currently not supported by our implementation.
	\item It uses \textsc{Maven} or \textsc{Gradle} as build automation tools so that we can automate the dependency collection procedure.
\end{enumerate}
We identified 40 candidate projects that satisfy our selection criteria.

\paragraph{Benchmark Preparation.}
For each candidate project, we automatically explored their commit history since 2018.
We performed an AST-level diff using \textsc{GumTree}~\cite{DBLP:conf/kbse/FalleriMBMM14} between the two versions of each commit to locate changes to the \mockito stub code.
The diff returned 2,295 code changes.
Since preparing the benchmark requires intensive manual effort, we performed a pre-selection on the code changes.
For each of the projects, we sampled at most 100 code changes, obtaining a total of 871 candidates.
Then, we manually read the code diff and commit messages to understand the semantics of the change.
We ignored the code changes that simply rename code elements~\ie{\textsc{GumTree} classifies the ASTs before and after the changes as isomorphic}.
This is because we do not regard such trivial code changes as the target application scenario of \tool{}.
Repairing stub code in such cases can be easily achieved using the refactoring feature of modern IDEs.

After dropping the trivial cases and duplicate commits due to git branch merges, we retained 261 code changes.
Each code change specifies two versions of a stub code: a broken one (before the change), and a correct one (after the change).
To turn each code change into a benchmark entry, we performed the following procedure:

\begin{itemize}
	\item We ran the \textsc{Gradle} or \textsc{Maven} build script to resolve the dependencies and compile the project.
    \item We rewrote each oracle assertion written with custom assertion frameworks into semantically equivalent \junit{} assertions.
	      Table~\ref{tab:rewritten-assertions} lists the rewritten rules applied by us.
	      Specifically, the rules were drafted by one author and then independently validated by two other authors independently.
	      One more author joined and resolved disagreements when they occurred.
	      In addition, we ran the test cases after rewriting the assertions to check that the test is still passing.
	      This was done for 14 subjects in project AZK, SBA, JIB, GRC, ZKN, and SPB.
	\item We executed the test case to ensure that it passes with the correct stub code written by developers, since we will use it as the ground truth.
	\item We removed the stub code so that the test fails. This is to ensure that the stub code is required to pass the test.
\end{itemize}
We discarded a code change if we failed to perform any of the steps above on it.
Finally, we constructed a benchmark of \numOfBenchmarkEntries{} test cases containing 167 mock objects collected from \numOfBenchmarkProjects{} projects.
Each of the entries in the benchmark consists of two elements \(\left\langle \tau_{bk}, \tau_{gt}\right\rangle\).
\(\tau_{bk} = \langle V, S_{bk}, E, A\rangle\) is the obsolete test case containing broken stub code \(S_{bk}\) and \(\tau_{gt} = \langle V, S_{gt}, E, A\rangle\) is the fixed version of the test case, which contains the ground-truth stub code \(S_{gt}\) written by developers.
Figure~\ref{fig:version-diagram} shows an example of a benchmark entry in project MB3.
The obsolete test case from version \code{4dfea24} contains broken stub code.
Developers fixed the broken stub code in version \code{963a8a5} by modifying the stub code.
Table~\ref{tab:subjects} summarizes the benchmark.


% Figure environment removed

\begin{table}[]
	\caption{List of Rewritten Assertions in Benchmark}\label{tab:rewritten-assertions}
	\centering
	\footnotesize
	\renewcommand{\arraystretch}{0.9}
	\input{tables/rewritten-assertions.tex}
\end{table}

\begin{table}[]
	\caption{Demographics of Benchmark}\label{tab:subjects}
	\centering
	\footnotesize
	\renewcommand{\arraystretch}{0.9}
	\input{tables/subjects.tex}
\end{table}

\begin{table}[]
	\centering
	\caption{RQ1 --- RQ3: Comparison of the Success Rate in Different Setups}\label{tab:results}
	\vspace{-1.2em}
	\captionsetup{style=base,singlelinecheck=off,font=small}
	\caption*{\scriptsize
		GT is for ground truth, which are the test cases written by developers.
		``\tool{} (G)'',  ``\tool{} (R)'', ``NSGA-II'', ``Weighted Sum'', and ``Unguided'' are \tool{} in generation mode, repair mode, with NSGA-II, with weighted sum, and random selection, respectively.
		\(|M|\) is the number of mock objects in \(V\) in the test case.
        \(|A|\) is the number of assertions (including \code{verify} assertions on mock objects) in the test case.
		\(|S|\) is the size of the stub code.
		SR is the number of successful runs.
		Gen, Time, and \(|S|\) are the number of generations taken, time taken (in seconds), and the size of stub code, respectively.
	}
	\vspace{-1em}
	\scriptsize
	\renewcommand{\arraystretch}{0.9}
	\input{tables/results.tex}
\end{table}


\subsection{RQ1: Stub Code Generation}\label{sec:rq1}

\paragraph{Experiment Setup.}
To answer RQ1, we ran \tool{} on the \numOfBenchmarkEntries{} test cases without stub code in our benchmark to generate stub code for them.
We ran \tool{} with population size \(N=200\), and set the generation budget \(MAX\_GEN = 400\).
We selected these parameters based on a few trial runs following previous work~\cite{DBLP:conf/sigsoft/TerragniJTP20}.
Due to the stochastic nature of evolutionary algorithms, we evaluated whether \tool{} can successfully synthesize the stub code in 10 repetitions.
Our algorithm relies on a pseudo-random number generator when making random decisions.
We chose 10 randomly-generated prime numbers as the random seeds for each repetitions, these random seeds are used across all the evaluation subjects.
We also designed two alternative optimization strategies in addition to our dominance based approach for comparison.
\begin{itemize}
	\item \textbf{Weighted Sum.}
	      The first alternative optimization strategy is to combine these objectives using weighted sum.
	      In this setup, we combined the three objectives into a single fitness function with different weights.
	      With the same rationale as for the NSGA-II variant, we assigned higher weights for the functions measuring the later stages of test execution, following the powers of 2.
	      \[
		      \text{\textbf{fitness}}(S) = 2^0\cdot SU(S)+ 2^1\cdot EC(S) + 2^2 \cdot AS(S)
	      \]
	\item \textbf{NSGA-II.}
	      In addition, we tried out the most popularly adopted MOEA, NSGA-II~\cite{DBLP:journals/tec/DebAPM02}.
	      NSGA-II employs a fast non-dominated sorting based on Pareto optimality~\cite{DBLP:conf/icec/TamakiKK96} and crowding-distance based comparison.
	      Such an approach will produce solutions that offer the best trade-off between competitive objectives~\cite{DBLP:conf/icec/TamakiKK96}.
\end{itemize}

% Figure environment removed

\paragraph{Results.}
Column ``\tool{} (G)'' of Table~\ref{tab:results} shows the results for each test case.
Column ``SR'' shows the number of successful runs for each test case.
For successful runs, we also report the generations taken, time taken (in seconds), and the size of stub code, in columns ``Gen'', ``Time'', and ``\(|S|\)'', respectively.

For 45 of \numOfBenchmarkEntries{} test cases, \tool{} successfully generated test-passing stub code in at least 5 of the 10 repetitions, which counts for 76\% of the subjects.
The median of time taken by all the successful syntheses is 182 seconds.

Column \(\mid S\mid\) shows the length of the stub code (in terms of lines of code).
\tool{} is able to synthesize non-trivial stub code.
The type of variables in the stub code contains primitive types, strings, and complex objects.
For the 48 subjects with at least one successful run, 30 of them contain complex objects in the synthesized stub code.
For most of the test cases, the length of the stub code synthesized by \tool{} is slightly longer than the ones written by developers.
This is because \tool{} does not inline the variables that are used only once in the synthesized stub code, which is often done by developers \eg{In Listing~\ref{lst:benchmark-36}, developers put the String literal right in the \code{thenReturn}}.
Such a refactoring can be done trivially by using some refactoring tools.
For some test cases~\eg{\#2, \#30, and \#32}, \tool{} synthesized much fewer lines of code, yet obtained a test-passing stub code.
We found that for these test cases, developers copied the same stub code across several test cases, creating redundant stub calls.
Such a bad practice can complicate future maintenance of the stub code.
In comparison, the stub code synthesized by \tool{} will be easier to maintain.

\tool{} is more likely to be successful for subjects with simple stub code.
For instance, the stub codes in project MB3 mostly comprise a single line and do not contain complicated string values.
\tool{} is more likely to fail when a subject needs a complicated stub code to pass.
For example, in project JIB, there are multiple stub calls in the developer-written test containing complicated string values.
Although our fitness component \(AS\) can capture the edit distance between the expected value and the actual value in the assertions,
it cannot help when the string value returned by the stub code does not flow directly to the assertions \eg{used as branch conditions}.
Also, for project ZKN, the developer-written stub code contains a custom implementation that mutates the variables outside the stub, which is beyond the capability of \tool{}.

Figure~\ref{fig:opt-comparison} shows a comparison of the average success rate versus generation budget between different optimization strategies.
The dominance-based approach adopted by \tool{} performs similarly as weighted sum while it requires less parameter tuning effort.
NSGA-II performs much poorer than dominance-based approach and weighted sum.
The reason for this performance drop is that, NSGA-II aims to produce solutions that offer the best trade-off between competitive objectives~\cite{DBLP:conf/icec/TamakiKK96}, which will treat all the optimization objectives as of equal importance.
However, in our scenario, the three objectives measures the quality of candidate stub code in different stages of test execution and they are of different importance.
In this case, we can observe a large performance gap between NSGA-II and dominance-based approach.

\begin{answertorq}
	\tool{} successfully synthesizes the stub code for 76\% of the test cases in our benchmark in at least half of the repetitions.
	Optimization strategies that consider the importance of each objectives help \tool{} achieve better performance.
\end{answertorq}

\subsection{RQ2: Stub Code Repair}
\paragraph{Experiment Setup.}
The application scenario of RQ2 is stub code repair.
To answer RQ2, we followed the same setup as in RQ1, with the only difference that we fed \tool{} with the broken stub code so that it could make use of its tokens to construct the symbol pool.

\tool{} aims to generate and repair stub code in unit tests.
It is the first technique of its kind.
Existing test script repair techniques either repair oracle assertions~\cite{daniel2009reassert,daniel2010test}, repair GUI test scripts~\cite{ choudhary2011water,gao2015sitar,stocco2018visual}, or focus only on CUT calls~\cite{li2019intent,mirzaaghaei2012supporting}.
These techniques cannot repair obsolete stub code.
As a result, we do not use them as baselines in our evaluation.
Instead, we selected state-of-the-art program repair techniques as our baselines.
Specifically, we selected the techniques using the following criteria:
\begin{itemize}
	\item It is most recently published at a peer-reviewed venue.
	\item It has an artifact that works for \java{} projects.
	\item It needs only faulty code and test cases as input.
\end{itemize}
Following these criteria, we selected two state-of-the-art program repair techniques: \arja{}~\cite{DBLP:journals/tse/YuanB20} and the \cardumen{} mode~\cite{DBLP:conf/ssbse/MartinezM18} of \astor{}, and we applied the two baselines on our subjects.
Since the stub code being repaired is in the form of test code, which is not supported by these baselines,
we make the following adaptations for our subjects.
\begin{itemize}
	\item
	      Since the two baselines can only repair application code, we migrate the test case that contains the broken stub code (together with its dependencies) to the application code directory.
	\item
	      The baselines rely on fault localization techniques to find repair candidates.
	      However, in our scenario, the statements to be repaired are already known.
	      Therefore, we implement a fault localizer that returns the statements containing the obsolete stub code as faulty locations.
	      This can force the baselines techniques to repair only the stub code.
	\item
	      For each migrated test case, we create a simple test case to trigger it.
	      We specify these simple test cases as the failing tests when applying the two baselines.
\end{itemize}
After applying the adaptation, we run the two baselines with their default configurations with a time budget of six hours.

\paragraph{Results.}
As shown in Table~\ref{tab:results} (Column~``\tool{} (R)''), \tool{} successfully repaired 76\% of the test cases in our benchmark in no fewer than 5 repetitions.
There are 30 subjects where \tool{} synthesized stub code with complex objects.
As shown in Figure~\ref{fig:opt-comparison}, \tool{} took fewer generations to find a test-passing stub code.
With the help of the tokens in the broken stub code, \tool{} is able to produce test-passing stub code for the subjects where it fails in generation mode.
Take subject \#36 as an example, a complicated string \code{"/actuator/health"} must be stubbed to pass the test.
During code evolution, the signature of the method being stubbed changed, and the stub code was broken.
Nevertheless, the string literal in the broken stub code is still useful for \tool{} and enables it to converge quickly to the stub code that makes the test pass.
As shown in Listing~\ref{lst:benchmark-36rep}, \tool{} synthesized a two-line stub code to pass the test by reusing the literal string in the broken stub code, which was done in only four generations.
In comparison, without the help of the tokens, it is hard for \tool{} to synthesize such a complicated string literal from scratch and therefore, \tool{} failed to synthesize a test-passing stub code in generations mode.

For the two state-of-the-art baseline techniques, they failed to repair the stub code in any of our evaluation subjects.
There are two reasons for the poor performance achieved by the baseline techniques.
First, for 35 of the 59 subjects, the broken stub code leads to a compilation error.
The baseline techniques require compiling tests to run and therefore, are not applicable to these subjects.
Second, for the remaining 24 subjects, they failed to repair the stub code because they lack awareness of the semantics of the APIs in the mocking frameworks (mocking APIs).
Without understanding the mocking APIs, it is difficult for such techniques to find a test-passing stub code by randomly mutating the AST nodes.

\begin{answertorq}
	\tool{} successfully repairs the stub code for 76\% of the test cases in our benchmark in no fewer than 5 repetitions.
	The tokens in the broken stub code can help reduce search effort and synthesize shorter stub code in some cases.
	State-of-the-art program repair techniques cannot repair the stub code in any of the evaluation subjects.
\end{answertorq}

\subsection{RQ3: Effectiveness of Fitness Function}

\paragraph{Experiment Setup.}
RQ3 aims to evaluate the contribution of the fitness function to steering the search for stub code.
Towards this goal, we constructed a variant of \tool{} with random selection, which conducts the search process without the guidance of the fitness function.
Enumerating and (uniformly) sampling the whole search space would have been the ideal random baseline.
However, it is infeasible due to the huge size of the search space.
As such, we opted for a variant of \tool{} that uses the same crossover and mutation operations to explore the search space, but without any guidance by the fitness function.
We ran this variant of \tool{} with the same configurations as in RQ1.

\paragraph{Results.}
Column ``Unguided'' of Table~\ref{tab:results} shows the performance of the unguided variant of \tool.
Without the support of the fitness function, the unguided variant only successfully synthesizes stub code for 54\% of the test cases in our benchmark in no fewer than 5 repetitions, which is less than the generation mode and the repair mode.
In general, when the unguided variant successfully synthesizes stub code, it takes significantly more generations~\eg{\#4, \#28, and \#56} than the guided version of \tool.
For five test cases, only the unguided variant of \tool{} fails to synthesize the stub code \eg{\#30 and \#33}.
Interestingly, four of them have multiple oracle assertions in their test oracle.
For such test cases, \tool{} with fitness guidance can successfully synthesize the stub code because the fitness function examines the status of each assertion in the test oracle, and thus can prioritize the candidate stub code that can satisfy some of the assertions.
Such results show that our fitness function can effectively guide the search for test-passing stub code.

\begin{answertorq}
	\tool{} outperforms its unguided variant in both the generation and repair modes.
	Our fitness function provides useful guidance for synthesis of stub codes.
\end{answertorq}

\subsection{RQ4: Fidelity of Synthesized Stub Code}\label{sec:rq4}


\paragraph{Experiment setup.}
RQ1 and RQ2 evaluate the effectiveness of \tool{} in generating and repairing stub code that makes the developer-specified assertions pass.
Different from them, RQ4 evaluates the fidelity of the stub code synthesized by \tool{} with respect to the ground-truth stub code.
Specifically, we evaluate to what extent the test cases with synthesized stub code can preserve the runtime behavior of the test case with ground-truth stub code.
For each of the test cases in which \tool{} successfully synthesizes stub code in at least one run, we prepared \(\tau_{gt}=\langle V, S_{gt}, E, A\rangle\) with the stub code written by developers, and \(\tau_s = \langle V, S, E, A \rangle\) with the stub code synthesized by \tool{}.
Next, we opted for the similarities in three metrics to estimate similarity in the runtime behaviors of \(\tau_s\) and \(\tau_{gt}\).
A higher similarity in the runtime behaviors indicates a higher fidelity of the synthesized stub code.
\begin{itemize}
	\item \textbf{Executed Instructions.}
	      This metric measures the behavior of the test case with respect to exercising the code under test.
	      In this paper, we identify the set of \java{} bytecode instructions in the production code that are executed by \(\tau_s\) and \(\tau_{gt}\), denoted as \(I(\tau_s)\) and \(I(\tau_{gt})\), respectively.
	      Test cases with similar runtime behaviors should execute similar sets of instructions.
	      Therefore, we also report the Jaccard similarity coefficient~\cite{jaccard} between \(I(\tau_s)\) and \(I(\tau_{gt})\).
	      However, similar sets of executed instructions are not our only metric, since it is not a sufficient condition for similar behaviors.
	      It is possible that two test cases behaving differently share similar sets of executed instructions.
	\item \textbf{Execution Path.}
	      In addition to the set of executed instructions, we also traced the execution paths, which are the ordered sequence of instructions that are executed by the test cases.
	      Comparing the execution paths of \(\tau_{gt}\) and \(\tau_{s}\) would give us more information about fidelity because, unlike executed instructions, the execution path captures the instruction execution order.
	      For each of the test cases where \tool{} successfully synthesizes stub code, we collected the execution paths generated by \(\tau_s\) and \(\tau_{gt}\), denoted as \(P(\tau_s)\) and \(P(\tau_{gt})\), respectively.
	      Since Jaccard similarity coefficient cannot be applied to execution paths, we report their similarity based on edit distances as follows.
	      \[
		      \text{Similarity}\left(P(\tau_s), P(\tau_{gt})\right) = 1 - \frac{DLev\left(P(\tau_s), P(\tau_{gt})\right)}{|P(\tau_s)| + |P(\tau_{gt})|}
	      \]
	      where \(DLev\) is the Damerauâ€“Levenshtein distance~\cite{Damerau_Levenshtein_distance}.
	      For test cases spawning multiple threads, we match the threads that share similar traces, and \(DLev\) denotes the sum of Damerau-Levenshtein distances between those thread pairs.
	      A small edit distance indicates that \(\tau_s\) and \(\tau_{gt}\) traverse similar execution paths.

	\item \textbf{Killed Mutants.}
	      Mutation analysis~\cite{DBLP:journals/tse/JiaH11} measures the adequacy of a test case with respect to detecting faults.
          It injects artificial faults in the program and checks if the test cases can ``kill'' them \ie{the test fails}.
	      In this paper, we mutated the CUT using \textsc{PIT}~\cite{DBLP:conf/issta/ColesLHPV16} by seeding faults and ran \(\tau_s\) and \(\tau_{gt}\) against the mutants.
	      We identified the mutants that are killed by \(\tau_s\) and \(\tau_{gt}\), denoted as \(K(\tau_s)\) and \(K(\tau_{gt})\), respectively.
	      Test cases with similar behaviors should be able to kill similar sets of mutants.
	      Therefore, we also report the Jaccard similarity coefficient~\cite{jaccard} between \(K(\tau_s)\) and \(K(\tau_{gt})\).
\end{itemize}

We choose these metrics because they estimate the intent or behaviors of test cases.
For example, the executed instructions and execution path are relaxed and tighten versions of path conditions.
They are validated to be a good abstraction of test intents in a recent study on test repair~\cite{li2019intent}.
Mutation coverage is a proxy for reflecting the behaviors of the test cases in terms of detecting potential bugs, and it was used to measure the behavioral similarity of test cases in a recent study that automatically refactors test cases with mocking~\cite{DBLP:conf/sigsoft/WangXYWW21}.

\begin{table}[]
	\caption{RQ4: Fidelity of the Synthesized Stub Code (Generation Mode)}\label{tab:fidelity-generation}
	\vspace{-0.6em}
	\captionsetup{style=base,singlelinecheck=off,font=scriptsize}
	\caption*{
		\(\tau_{gt}\) denotes the result generated by the ground truth.
		\(\tau_{s}\) denotes the result generated by the test with synthesized stub code.\\
		\(\tau_{gt}\cap\tau_s\) denotes the intersection of the ground truth and the test with synthesized stub code.\\
		Jaccard denotes Jaccard similarity coefficient.\\
		\(DLev\) denotes Damerau-Levenshtein distance.
		\smallskip{}
	}
	\vspace{-1em}
	\scriptsize
	\renewcommand{\arraystretch}{0.9}
	\input{tables/fidility-generation.tex}
\end{table}

\begin{table}[]
	\caption{RQ4: Fidelity of the Synthesized Stub Code (Repair Mode)}\label{tab:fidelity-repair}
	\vspace{-0.6em}
	\captionsetup{style=base,singlelinecheck=off,font=scriptsize}
	\caption*{
		\(\tau_{gt}\) denotes the result generated by the ground truth.
		\(\tau_{s}\) denotes the result generated by the test with synthesized stub code.\\
		\(\tau_{gt}\cap\tau_s\) denotes the intersection of the ground truth and the test with synthesized stub code.\\
		Jaccard denotes Jaccard similarity coefficient.\\
		\(DLev\) denotes Damerau-Levenshtein distance.
		\smallskip{}
	}
	\vspace{-1em}
	\scriptsize
	\renewcommand{\arraystretch}{0.9}
	\input{tables/fidility-repair.tex}
\end{table}
\paragraph{Results.}
For generation mode, Table~\ref{tab:fidelity-generation} shows the comparisons on executed instructions, execution path, and killed mutants by \(\tau_s\) and \(\tau_{gt}\) for each subject in our benchmark that \tool{} successfully synthesizes stub code in at least one run.\footnote{PIT crashed due to its bug on subject \#54 and therefore we cannot report the result for \#54 in this RQ4.}
In our experiment, \(\tau_s\) covers the similar set of the instructions as \(\tau_{gt}\), with the median of the Jaccard similarity coefficient to be 100\%.
In 24 of the 46 subjects, \(\tau_s\) covers exactly the same set of instructions as \(\tau_{gt}\).
In such cases, \(\tau_s\) is capable for exercising the same instructions as \(\tau_{gt}\).
The execution paths traversed by \(\tau_s\) and \(\tau_{gt}\) are also similar.
The median of similarity is 99.99\%, which indicates that \(\tau_s\) exercise the CUT in a way similar to what \(\tau_{gt}\) does.
In 22 of the 46 subjects, the edit distance between the execution paths generated by \(\tau_s\) and \(\tau_{gt}\) is 0, which means that they execute the instructions in the production code in exactly the same order.
In such subjects, \(\tau_s\) and \(\tau_{gt}\) exercise the CUT with the same intent.
This is because the same execution path indicates that the tests share the same path conditions, which was shown to be a good abstraction of test intent~\cite{li2019intent}.
The set of mutants killed by \(\tau_s\) and \(\tau_{gt}\) are also similar, with the median of similarity to be 100\%.
In 26 of the 46 subjects, \(\tau_s\) kills exactly the same set of mutants as \(\tau_{gt}\), which means \(\tau_s\) has the similar ability to detect injected bugs as \(\tau_{gt}\).

Table~\ref{tab:fidelity-repair} gives the fidelity comparison for the repair more.
The median of similarity in executed instructions is 99.78\%, and \(\tau_s\) covers the same set of instructions as \(\tau_{gt}\) does in 24 of 49 subjects.
The median of similarity in execution path is 99.39\%, and \(\tau_s\) shares exactly the same execution path as \(\tau_{gt}\) in 23 of 49 subjects.
The median of similarity in killed mutants is 100\%, and \(\tau_s\) kills the same set of mutants as \(\tau_{gt}\) does in 28 of 49 subjects.
% \valerio{we don't call them coverage anymore}
In summary, \tool{} synthesizes stub code similar to the ground truth with respect to executed instructions, execution path, and killed mutation.
The high similarities indicate that the test cases with the synthesized stub code have adequacy similar to that of the ground truth.
Such test cases are useful for detecting regression bugs when the CUT evolves.

Besides most of the high hifelities subjects, we also observed several cases that worth discussion:
\begin{itemize}
	\item \textbf{Synthesis is successful but fidelity is low.}
	      Such cases are caused by weak test oracles.
	      The test oracles in these subjects \eg{\#23 and \#25 in both modes} allow multiple execution paths to pass the test.
	      Listing~\ref{lst:weak-oracle-example} shows an example to illustrate such cases.
	      As shown in the code snippet, both \(\tau_{gt}\) and \(\tau\) can pass the test since both of them will result in a \code{CustomException} to be thrown.
	      However, the execution paths of \(\tau_{gt}\) and \(\tau\) are different, and the mutants injected into the branch cannot be killed by \(\tau\).
	      Such situations can be easily mitigated by enhancing the test oracle with a few mocking calls, specifying that certain methods should be called on the mock objects.
	      After that, \tool{} will synthesize stub code that helps cover the code lines invoking the methods specified by such mocking calls.
	\item \textbf{Killed mutants are the same but execution paths are different.}
	      Such cases \eg{\#47, \#49, and \#58 in both modes} happened because \(\tau_{gt}\) and \(\tau\) takes different ways to construct certain objects.
	      To illustrate such a difference, Listing~\ref{lst:alt-object-construction} show a comparison of two ways to construct a string \code{"10"}.
	      In the developer-written stub code, the string is constructed directly with a literal.
	      In the synthesized stub code, the string is converted from an interger value.
	      In this case, the instructions in \code{String.valueOf} will be included in the execution path of \(\tau\) but not \(\tau_{gt}\).
	      Nevertheless, the synthesized stub code is sill useful for developers as \(\tau\) kills exactly the same set of mutants as \(\tau_{gt}\).
	\item \textbf{Execution paths are similar but killed mutants are different.}
	      Such cases \eg{\#2, \#35 in repair mode} are due to different return values specified in the stub code.
	      Listing~\ref{lst:val-spec} illustrates such a case.
	      The generated stub code and the developer-written stub code specify different return values for the method \code{getOffset}.
	      Without mutation, both test cases can enter the then branch, and therefore they share the same set of executed instructions, and both of them make the test pass.
	      However, when the mutation operator changes the \code{+} to \code{-}, the test case with developer-written stub code does not enter the then branch while the test case with generated stub code enters the then branch.
	      In this case, the test fails with the developer-written code while it passes with the generated stub code.
	\item \textbf{Same executed instructions but different execution paths.}
	      This is because there are loops in the production code and \(\tau\) and \(\tau_{gt}\) executed the loops for different number of times.
\end{itemize}


% Figure environment removed

% Figure environment removed

% Figure environment removed

Overall, as shown in Table~\ref{tab:fidelity-generation} and Table~\ref{tab:fidelity-repair}, the encoded information is sufficient for obtaining a useful stub code most of the time.
This verifies our intuition that deriving the stub code from the information encoded in the CUT execution code and test oracle leads to adequate test cases.

\begin{answertorq}
	\tool{} synthesizes stub code with high fidelity, which means that they share a runtime behavior similar to that of the ground truth in terms of their effects on the code under test.
	The information encoded in CUT execution code and test oracle is useful for deriving stub code.
	\tool{} works well when the test oracle contains adequate information.
\end{answertorq}

\subsection{Threats to Validity}

\paragraph{Subject Collection.}
We evaluated \tool{} on \numOfBenchmarkEntries{} test cases collected from \numOfBenchmarkProjects{} projects.
Our results might not be generalized to other projects and test cases.
The subject collection requires intensive manual effort, which limited the number of projects and test cases that we could use.
To mitigate this threat, we selected
large, actively maintained, diverse, and popular GitHub projects.
These projects belong to different domains: big data, database, web apps, containers, etc.
Our benchmark dataset reflects the real-world usage of stub code in these areas.

Also, when preparing the evaluation subjects, we rewrote the assertions written in other libraries into those using \junit{} framework.
Such manual modification might be affected by human mistakes and thus change behavior of the test cases.
To mitigate this issue, we cross-checked the documentation of the corresponding assertion framework and \junit{} to make sure the rewritten assertions preserves the original semantics.
We also ran the test cases before and after modification to make sure that they yield the same result.

\paragraph{Fidelity Measurement.}
When measuring the fidelity of the stub code, we leveraged a metric based on instruction coverage.
However, the similarity on the instruction coverage may not be ideal to reflect the differences.
For example, when there are only 10\% of the instructions in the class under test are in branches, the similarity of the instruction coverage will be at least 90\%.
To mitigate this threat, we introduced additional metrics such a execution path and killed mutants to further characterize the behavior of the test cases.


\paragraph{Experiments.}
Evolutionary algorithms are stochastic by nature, and the evaluation results may be different across several runs.
In our experiments, we used 10 repetitions to evaluate \tool.
The effectiveness of \tool{} is likely to increase with more attempts and a higher budget.
However, \tool{} results are stable, as shown in Table~\ref{tab:results}.
There are only a few test cases where the 10 attempts gave inconsistent results.
Nevertheless, conducting more experiments is an important future work.

