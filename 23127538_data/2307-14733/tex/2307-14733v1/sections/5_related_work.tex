\section{Related Work}\label{sec:related-work}

\paragraph{Stub Code Generation.}
The first line of related work focuses on automatically generating stub code for mock objects.
Capture-and-replay is a popular approach adopted by these techniques.
In 2004, Saff~\etal{} were among the first to develop a mock object construction technique~\cite{DBLP:conf/paste/SaffE04,DBLP:conf/kbse/SaffAPE05} aiming to improve the efficiency of unit testing.
The technique runs a working test case to capture the interactions between the CUT and its dependencies.
Next, those dependencies are replaced with mock objects, and stub code are generated using the captured information.
A similar idea is used by Joshi~\etal{}~\cite{DBLP:conf/icsm/JoshiO07} and Elbaum~\etal{}~\cite{DBLP:conf/sigsoft/ElbaumCDD06} for test craving.
Fazzini~\etal{} proposed \textsc{MOKA}~\cite{DBLP:conf/kbse/FazziniGO20} to collect and generate mock objects for testing mobile applications by observing the interactions between the application and its environment.
More recently, Tiwari \etal{} designed \textsc{Rick}~\cite{DBLP:journals/corr/abs-2208-01321} to generate mock objects that mimic the behavior of the test dependencies in a production environment.
\textsc{Rick} works by analyzing the runtime data captured in production systems and it successfully mimics 52.4\% of the test executions as shown in evaluation.

These capture-and-replay techniques assume that dependencies are available when stub code is created.
Conversely, \tool{} does not make this assumption as it generates stub calls without executing the actual dependency, which makes it applicable to a wider range of scenarios \eg{for projects adopting TDD, the test dependencies may not be available when the test case is created}.
Moreover, capture-and-replay techniques may generate unreliable test cases when the captured behavior of the dependency is flaky or incorrect. Differently, \tool{} does not suffer from this issue.

Stub code is also generated by a few test generation techniques to increase test coverage.
For instance, Arcuri~\etal{} developed techniques for generating stub code for environment dependent classes~\cite{DBLP:conf/kbse/ArcuriFG14,DBLP:conf/sigsoft/ArcuriFG15}, which enable \textsc{EvoSuite}~\cite{DBLP:conf/sigsoft/FraserA11} to achieve higher coverage for the classes having such dependencies.
Similar approaches are also adopted to construct stub code for databases~\cite{DBLP:conf/kbse/TanejaZX10}, mobile apps~\cite{DBLP:conf/kbse/FazziniGO20}, and web services~\cite{DBLP:conf/apsec/Bhagya0G19,DBLP:journals/software/ZhangMLXTH12}.
However, they can  generate stub code for certain dependency types only (e.g., networking~\cite{DBLP:conf/sigsoft/ArcuriFG15,DBLP:conf/apsec/Bhagya0G19,DBLP:journals/software/ZhangMLXTH12}, database~\cite{DBLP:conf/kbse/TanejaZX10}, file system~\cite{DBLP:conf/kbse/ArcuriFG14,DBLP:conf/icse/MarriXTHS09}) because they follow predefined rules, which are not applicable to an arbitrary mock object.


These techniques are closely coupled with domain knowledge. New rules have to be manually defined to generate stub code for the mock objects that are not considered by these techniques.
As such, these techniques cannot be easily adapted to other types of mock objects.
Also, they do not use mocking frameworks to specify the behavior of the dependencies, but light-weight implementations similar to ``fake'' mock objects.
In comparison, \tool{} is domain agnostic and thus can synthesize stub code for an arbitrary mock object.
Also, it allows developers to specify the behavior of mock objects with oracle assertions, which gives developers more control over the behavior of the synthesized stub code.

\paragraph{Empirical Studies on Mocking.}
The second line of related work focuses on the practices adopted by developers when using mock objects in their projects.
Marri \etal~\cite{DBLP:conf/icse/MarriXTHS09} conducted an analysis on the usage of mock objects in testing file-system-dependent software and showed that mock objects can ease the process of unit testing.
Mostafa \etal~\cite{DBLP:conf/qsic/MostafaW14} analyzed the usage of mocking frameworks in 5,000 \textsc{Java} projects and revealed that mock objects are widely used although they are only used to substitute certain types of test dependencies.
They also raise the need for an automated technique for synthesizing stub code.
Spadini \etal~\cite{DBLP:journals/ese/SpadiniABB19} studied the usage of mock objects in three open-source projects and one commercial project.
They highlighted the practice adopted by developers when making mocking decisions found that developers choose to substitute the classes that are hard to setup with mock objects.
In addition, they reveal that stub code are frequently coupled with production code and need to be frequently updated, which make creating and maintaining stub code challenging.
More recently, Zhu \etal~\cite{DBLP:conf/kbse/ZhuWWLCSZ20} conducted an empirical study on four open-source projects and distilled 10 code-level rules that can affect the mocking decisions, based on which they proposed a machine learning based technique that recommends mocking decisions for developers.
Wang \etal{}~\cite{DBLP:conf/sigsoft/WangXYWW21} proposed an auto refactoring tool to migrate inheritance based mock objects to mocking frameworks.

All of these studies provide evidence of the popularity and importance of mocking.
They also discuss the challenges faced by developers when using mock objects.
In this paper, we propose \tool{} to automatically generate and repair stub code for mock objects, helping developers to address some of the challenges.

\paragraph{Test Case Repair.}
The third line of related work aim to repair the broken test cases due to the evolution of production code.
For example, Daniel \etal{}~\cite{daniel2009reassert} proposed \textsc{ReAssert}, a test case repair technique implemented for \junit{}.
\textsc{ReAssert} suggests repairs to failing tests to make them pass again.
The fixes suggested by it include replacing literals values and assertions.
Daniel \etal{}~\cite{daniel2010test} later enhanced the capability of \textsc{ReAssert} by proposing \textsc{Symbolic Test Repair}, which employs symbolic execution and constraint solving to update the expected values of the assertions.
Compared with \textsc{ReAssert}, \textsc{Symbolic Test Repair} can repair the test cases with complex control flow or operations on the expected values.
Similarly, Mirzaaghaei \etal{}~\cite{mirzaaghaei2012supporting} developed \textsc{TestCareAssistant} (TCA) to facilitate test evolution by repairing obsolete test cases and generating new test cases.
TCA identify five common actions adopted by developers to adapt the test cases to new version, and apply these actions to the obsolete test case.
While the above techniques can make the test pass again, they did not consider whether the intent of the test case are preserved.
To fill this gap, Li \etal{}~\cite{li2019intent} proposed a technique for preserving the intent of the test case during test repair.
They rank the repair candidates by the likelihood of preserving the intent of the original test case.
The intent of a test case is characterized by analyzing the path conditions generated from a dynamic symbolic execution.

Test case repair techniques are also developed for GUI or web applications.
Choudhary \etal{}~\cite{choudhary2011water} proposed \textsc{WATeR} to suggest repairs for automation script for testing web applications.
The repairs are suggest by analyzing the the difference between a passing-failing pairs.
\textsc{WATeR} can suggest repairs for the test failure due to the type change of the web page elements and displaced or changed web page elements.
Similarly, Stocco \etal{}~\cite{stocco2018visual} proposed \textsc{Vista} to repair the test script of web applications by analyzing the visual information captured from test execution.
They also equipped \textsc{Vista} with a local crawling mechanism to handle non-trivial breakage scenarios.
On the same theme, Gao \etal{}~\cite{gao2015sitar} developed \textsc{SITAR}, a semi-automated technique for repairing GUI test scripts.
The repair is generated by reverse engineering the test script and map it to an event-flow graph.
\textsc{SITAR} can amortizes the cost of human intervention across repairing multiple test scripts.

Although these techniques can effectively repair broken test cases, they focus on fixing the test exercise sequence and the assertions.
They are not capable for repairing the obsolete stub code in broken test cases.
In this paper, we proposed an application scenario for repairing the broken test cases by re-synthesizing the stub code to replace the obsolete ones.
