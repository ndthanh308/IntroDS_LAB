\section{Related Work}
\label{sec:related}

\subsection{Segmentation on medical data with U-Net}
\label{subsec:unet-like}
U-Net~\cite{Ronneberger_2015} was proposed for biomedical image segmentation back in 2015. Afterwards, a new class of models was developed based on U-Net-like architectures which established the state-of-the-art in segmentation. One promising approach was proposed by Isensee \etal in~\cite{Isensee_2021}, where nnU-Net was introduced. nnU-Net is a deep learning-based segmentation method that automatically configures itself for any new task. 
Its performance is not attained through a new architecture (thus the name nnU-Net, 'no new net'), as it only comprises minor modifications to the original U-Net. Rather, it automates the complicated process of manually configuring the method. 
Hatamizadeh \etal reformulate in~\cite{Hatamizadeh_2022} the task of volumetric medical image segmentation as a sequence-to-sequence prediction problem by leveraging the power of self-attention and transformers architectures. They introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder. The extracted representations are merged with a CNN-based decoder via skip connections at multiple resolutions. The ensemble of UNETR models has shown promising results on the BTCV dataset. Tang \etal introduced in~\cite{Tang_2022} a novel 3D transformer-based model dubbed Swin UNEt TRansformers (Swin UNETR). Swin UNETR comprises a Swin Transformer~\cite{Liu_2021} encoder and a CNN-based decoder. The transformer encoder is pre-trained with tailored, self-supervised tasks over 5,050 images. Overall, the ensemble of 20 Swin UNETR models achieved at the time of publication the top-ranking performance on the BTCV challenge, showing distinct improvements for the segmentation of organs that are smaller in size. 

\subsection{Video action recognition}
\label{subsec:video}
Spatio-temporal representation learning refers to the process of learning meaningful representations of both spatial and temporal information in a given dataset. In computer vision, this is particularly important for tasks such as video analysis and action recognition, where the goal is to accurately model the spatial and temporal evolution of objects and subjects over time. In particular, video action recognition has received increasing attention due to its potential applications such as video surveillance, human-computer interaction, and social video recommendation. This field presents however a fundamental challenge due to the space-time nature of the data. For years many efforts were made to trade off between temporal modelling and computation (\cite{Wang_2016}, \cite{Luo_2019}, \cite{Lin_2019}, \cite{Sudhakaran_2020}). Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships. Since a video can be seen as a temporally dense sampled sequence of images, expanding the 2D convolution operation to 3D convolution is an intuitive approach to spatiotemporal feature learning. While 3D CNN-based methods can achieve strong results, they require significant computational resources. Lin \etal proposed in their work~\cite{Lin_2019} a Temporal Shift Module (TSM) that can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts a fixed amount of the channels along the temporal dimension, facilitating information exchange among neighbouring frames yielding a 2D CNN that can learn spatiotemporal features. Li \etal \cite{li2019collaborative} propose an operation that encodes spatiotemporal features by imposing a weight-sharing constraint. In particular, they perform 2D convolution by sharing the convolution kernels of three orthogonal views of a video, allowing multi-view features to be learned collaboratively. 

