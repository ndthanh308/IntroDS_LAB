\section{Introduction}
\label{sec:intro}
Identifying organs through semantic segmentation is a crucial step in several clinical workflows, including diagnosis, intervention, therapy planning, treatment delivery, and tumour growth monitoring. However, the volumetric data generated by medical acquisition systems, such as Computer Tomography (CT), Magnetic Resonance Imaging (MRI), or Ultrasound, can make the segmentation task labour-intensive and time-consuming. For instance, a single 3D CT scan can contain hundreds of 2D slices (images). Therefore, developing robust and accurate automatic segmentation tools is a fundamental necessity in medical image analysis~\cite{Sun_2020, Tang_2019}.
% Figure environment removed
With the advent of deep learning, Convolutional Neural Networks (CNNs) have proved to be extremely effective at solving vision tasks due to their powerful representation learning capabilities. In particular, "U-shaped" encoder-decoder architectures have achieved state-of-the-art results in various medical semantic segmentation tasks~\cite{chen2019s3d,futrega2022optimized,Isensee_2021}. More recently, Vision Transformers (ViT)~\cite{dosovitskiy2021an} have achieved comparable results to CNN-based methods, and as a result, many transformer-based models have been proposed for both 2D and 3D medical image segmentation~\cite{Xie_2021,cao2021swin,chen2021transunet}.
Although 3D CNNs are designed to learn three-dimensional features, they require higher computation costs, resulting in higher inference latency compared to 2D CNNs. Besides, the large number of parameters may result in a higher risk of overfitting, especially when encountering small datasets~\cite{zhang2022bridging}. This is very common in the medical field as it is challenging to collect 3D medical datasets due to accessibility issues for ethical reasons, and limited time and budget for annotations.
To process volumetric data more efficiently, two main strategies can be used. The first one is cutting the volume into slices and training 2D CNNs to segment each slice separately ~\cite{cao2021swin,chen2021transunet}. Despite the computational efficiency, as the information between adjacent slices is neglected, it leads to segmentation results that are prone to discontinuity in 3D space~\cite{zhang2022bridging}. The second is using 2.5D segmentation methods (or pseudo-3D methods). A very common 2.5D strategy is "multi-view fusion" where three 2D CNNs are trained on the sagittal, coronal, and axial planes separately~\cite{yun2019improvement}, after that, the segmentation results from each plane are fused to get the final result. 

In this work, we propose a bi-dimensional UNet, for segmentation on volumetric medical data that extracts multi-view and multi-slice information thanks to a Slice SHift mechanism (SSH-UNet). To extract multi-view features as in~\cite{li2019collaborative} we impose weight sharing between the 2D convolution that processes the slices from the three orthogonal planes. While shifting is a well-established technique in video processing, we wondered if it could also be transferred to volumetric data since there is no inherent preferential direction like time in videos. As shown in Figure~\ref{fig:pipeline} intra-slice features are extracted by shifting a portion of the feature maps along the slices' axis following the work in~\cite{Lin_2019}. SSH-UNet is evaluated on two publicly available benchmark datasets the Multi-Modality Abdominal Multi-Organ Segmentation (AMOS)~\cite{ji2022amos} and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV)~\cite{BTCV_orig}. To the extent of our knowledge, no previous work in the medical field has explored the combination of shifting and shared weights across multiple views within a single model. 

To be more specific, the contributions of our work are as follows:
\begin{itemize}
    \item We propose the first network that repurposes the spatiotemporal modelling in video tasks to segment medical data. By interpreting the slices' axis as the time, we solve the problem of 2D CNNs that neglect information between adjacent slices by shifting a portion of the feature maps along the slices' axis.
    \item We revisit and extend the 2.5D multi-view fusion method by processing slices from the three orthogonal planes of a volume using a 2D UNet with shared weights rather than three separate networks, allowing multi-view features to be learned collaboratively while maintaining a light computational cost.
    \item We instantiate these ideas into the Slice-Shift (SSH) layer, a 2D convolution layer operating on 3D tensors. We validate the effectiveness of the proposed framework by training a UNet built of SSH layers on two publicly available benchmark datasets, AMOS and BTCV, showing that our approach with the same model complexity as 2D CNNs achieves the same performance as a fully 3D network with a similar architecture and can achieve comparable results with other popular state-of-the-art approaches with less than $1/5$ of parameters.
\end{itemize}
Our code will be released to facilitate follow-up research.