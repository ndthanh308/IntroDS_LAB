{
  "title": "Finite-sum optimization: Adaptivity to smoothness and loopless variance reduction",
  "authors": [
    "Bastien Batardi√®re",
    "Joon Kwon"
  ],
  "submission_date": "2023-07-24T08:44:10+00:00",
  "revised_dates": [
    "2024-04-09T01:36:56+00:00"
  ],
  "abstract": "For finite-sum optimization, variance-reduced gradient methods (VR) compute at each iteration the gradient of a single function (or of a mini-batch), and yet achieve faster convergence than SGD thanks to a carefully crafted lower-variance stochastic gradient estimator that reuses past gradients. Another important line of research of the past decade in continuous optimization is the adaptive algorithms such as AdaGrad, that dynamically adjust the (possibly coordinate-wise) learning rate to past gradients and thereby adapt to the geometry of the objective function. Variants such as RMSprop and Adam demonstrate outstanding practical performance that have contributed to the success of deep learning. In this work, we present AdaLVR, which combines the AdaGrad algorithm with loopless variance-reduced gradient estimators such as SAGA or L-SVRG that benefits from a straightforward construction and a streamlined analysis. We assess that AdaLVR inherits both good convergence properties from VR methods and the adaptive nature of AdaGrad: in the case of $L$-smooth convex functions we establish a gradient complexity of $O(n+(L+\\sqrt{nL})/\\varepsilon)$ without prior knowledge of $L$. Numerical experiments demonstrate the superiority of AdaLVR over state-of-the-art methods. Moreover, we empirically show that the RMSprop and Adam algorithm combined with variance-reduced gradients estimators achieve even faster convergence.",
  "categories": [
    "math.OC"
  ],
  "primary_category": "math.OC",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12615",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 5271283,
  "size_after_bytes": 1741049
}