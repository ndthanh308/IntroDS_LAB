% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{none/global//global/global}
  \entry{cauchy1847methode}{article}{}
    \name{author}{1}{}{%
      {{hash=CAL}{%
         family={Cauchy},
         familyi={C\bibinitperiod},
         given={Augustin-Louis},
         giveni={A\bibinithyphendelim L\bibinitperiod},
      }}%
    }
    \strng{namehash}{CAL1}
    \strng{fullhash}{CAL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1847}
    \field{pages}{536\bibrangedash 538}
    \field{title}{M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des
  systemes dâ€™{\'e}quations simultan{\'e}es}
    \field{volume}{25}
    \field{journaltitle}{Comptes rendus de l'Acad\'{e}mie des sciences}
    \field{year}{1847}
  \endentry

  \entry{robbins1951stochastic}{article}{}
    \name{author}{2}{}{%
      {{hash=RH}{%
         family={Robbins},
         familyi={R\bibinitperiod},
         given={Herbert},
         giveni={H\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Monro},
         familyi={M\bibinitperiod},
         given={Sutton},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {JSTOR}%
    }
    \strng{namehash}{RHMS1}
    \strng{fullhash}{RHMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{400\bibrangedash 407}
    \field{title}{A stochastic approximation method}
    \field{journaltitle}{The annals of mathematical statistics}
    \field{year}{1951}
  \endentry

  \entry{roux2012stochastic}{article}{}
    \name{author}{3}{}{%
      {{hash=LRN}{%
         family={Le\bibnamedelima Roux},
         familyi={L\bibinitperiod\bibinitdelim R\bibinitperiod},
         given={Nicolas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Schmidt},
         familyi={S\bibinitperiod},
         given={Mark},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
    }
    \strng{namehash}{LRNSMBF1}
    \strng{fullhash}{LRNSMBF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{A stochastic gradient method with an exponential convergence
  rate for finite training sets}
    \field{volume}{25}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2012}
  \endentry

  \entry{schmidt2017minimizing}{article}{}
    \name{author}{3}{}{%
      {{hash=SM}{%
         family={Schmidt},
         familyi={S\bibinitperiod},
         given={Mark},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LRN}{%
         family={Le\bibnamedelima Roux},
         familyi={L\bibinitperiod\bibinitdelim R\bibinitperiod},
         given={Nicolas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{SMLRNBF1}
    \strng{fullhash}{SMLRNBF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{83\bibrangedash 112}
    \field{title}{Minimizing finite sums with the stochastic average gradient}
    \field{volume}{162}
    \field{journaltitle}{Mathematical Programming}
    \field{year}{2017}
  \endentry

  \entry{blatt2007convergent}{article}{}
    \name{author}{3}{}{%
      {{hash=BD}{%
         family={Blatt},
         familyi={B\bibinitperiod},
         given={Doron},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HAO}{%
         family={Hero},
         familyi={H\bibinitperiod},
         given={Alfred\bibnamedelima O.},
         giveni={A\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
      {{hash=GH}{%
         family={Gauchman},
         familyi={G\bibinitperiod},
         given={Hillel},
         giveni={H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {SIAM}%
    }
    \strng{namehash}{BDHAOGH1}
    \strng{fullhash}{BDHAOGH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{29\bibrangedash 51}
    \field{title}{A convergent incremental gradient method with a constant step
  size}
    \field{volume}{18}
    \field{journaltitle}{SIAM Journal on Optimization}
    \field{year}{2007}
  \endentry

  \entry{johnson2013accelerating}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=JR}{%
         family={Johnson},
         familyi={J\bibinitperiod},
         given={Rie},
         giveni={R\bibinitperiod},
      }}%
      {{hash=ZT}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Tong},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{JRZT1}
    \strng{fullhash}{JRZT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in neural information processing systems}
    \field{pages}{315\bibrangedash 323}
    \field{title}{Accelerating stochastic gradient descent using predictive
  variance reduction}
    \field{year}{2013}
  \endentry

  \entry{SAGA_article}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=DA}{%
         family={Defazio},
         familyi={D\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
      {{hash=LJS}{%
         family={Lacoste-Julien},
         familyi={L\bibinithyphendelim J\bibinitperiod},
         given={Simon},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{DABFLJS1}
    \strng{fullhash}{DABFLJS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in neural information processing systems}
    \field{pages}{1646\bibrangedash 1654}
    \field{title}{SAGA: {A} fast incremental gradient method with support for
  non-strongly convex composite objectives}
    \field{year}{2014}
  \endentry

  \entry{allen2016improved}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=AZZ}{%
         family={Allen-Zhu},
         familyi={A\bibinithyphendelim Z\bibinitperiod},
         given={Zeyuan},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=YY}{%
         family={Yuan},
         familyi={Y\bibinitperiod},
         given={Yang},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{AZZYY1}
    \strng{fullhash}{AZZYY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International conference on machine learning}
    \field{pages}{1080\bibrangedash 1089}
    \field{title}{Improved {SVRG} for non-strongly-convex or sum-of-non-convex
  objectives}
    \field{year}{2016}
  \endentry

  \entry{hofmann2015variance}{article}{}
    \name{author}{4}{}{%
      {{hash=HT}{%
         family={Hofmann},
         familyi={H\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=LA}{%
         family={Lucchi},
         familyi={L\bibinitperiod},
         given={Aurelien},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LJS}{%
         family={Lacoste-Julien},
         familyi={L\bibinithyphendelim J\bibinitperiod},
         given={Simon},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MB}{%
         family={McWilliams},
         familyi={M\bibinitperiod},
         given={Brian},
         giveni={B\bibinitperiod},
      }}%
    }
    \strng{namehash}{HTLALJS+1}
    \strng{fullhash}{HTLALJSMB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Variance reduced stochastic gradient descent with neighbors}
    \field{volume}{28}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2015}
  \endentry

  \entry{gower2021stochastic}{article}{}
    \name{author}{3}{}{%
      {{hash=GRM}{%
         family={Gower},
         familyi={G\bibinitperiod},
         given={Robert\bibnamedelima M},
         giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Richt{\'a}rik},
         familyi={R\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{GRMRPBF1}
    \strng{fullhash}{GRMRPBF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{135\bibrangedash 192}
    \field{title}{Stochastic quasi-gradient methods: {V}ariance reduction via
  {J}acobian sketching}
    \field{volume}{188}
    \field{journaltitle}{Mathematical Programming}
    \field{year}{2021}
  \endentry

  \entry{defazio2014finito}{inproceedings}{}
    \true{moreauthor}
    \true{morelabelname}
    \name{author}{2}{}{%
      {{hash=DA}{%
         family={Defazio},
         familyi={D\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
      {{hash=DJ}{%
         family={Domke},
         familyi={D\bibinitperiod},
         given={Justin},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{DADJ+1}
    \strng{fullhash}{DADJ+1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{1125\bibrangedash 1133}
    \field{title}{Finito: {A} faster, permutable incremental gradient method
  for big data problems}
    \field{year}{2014}
  \endentry

  \entry{mairal2013optimization}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=MJ}{%
         family={Mairal},
         familyi={M\bibinitperiod},
         given={Julien},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{MJ1}
    \strng{fullhash}{MJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{783\bibrangedash 791}
    \field{title}{Optimization with first-order surrogate functions}
    \field{year}{2013}
  \endentry

  \entry{konevcny2017semi}{article}{}
    \name{author}{2}{}{%
      {{hash=KJ}{%
         family={Kone{\v{c}}n{\`y}},
         familyi={K\bibinitperiod},
         given={Jakub},
         giveni={J\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Richt{\'a}rik},
         familyi={R\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Frontiers Media SA}%
    }
    \strng{namehash}{KJRP1}
    \strng{fullhash}{KJRP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{9}
    \field{title}{Semi-stochastic gradient descent methods}
    \field{volume}{3}
    \field{journaltitle}{Frontiers in Applied Mathematics and Statistics}
    \field{year}{2017}
  \endentry

  \entry{li2020convergence}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LB}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Bingcong},
         giveni={B\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Meng},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GGB}{%
         family={Giannakis},
         familyi={G\bibinitperiod},
         given={Georgios\bibnamedelima B},
         giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{LBMMGGB1}
    \strng{fullhash}{LBMMGGB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Artificial Intelligence and
  Statistics}
    \field{pages}{223\bibrangedash 233}
    \field{title}{On the convergence of {SARAH} and beyond}
    \field{year}{2020}
  \endentry

  \entry{shalev2013stochastic}{article}{}
    \name{author}{2}{}{%
      {{hash=SSS}{%
         family={Shalev-Shwartz},
         familyi={S\bibinithyphendelim S\bibinitperiod},
         given={Shai},
         giveni={S\bibinitperiod},
      }}%
      {{hash=ZT}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Tong},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{SSSZT1}
    \strng{fullhash}{SSSZT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{title}{Stochastic dual coordinate ascent methods for regularized
  loss minimization.}
    \field{volume}{14}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2013}
  \endentry

  \entry{lin2014accelerated}{article}{}
    \name{author}{3}{}{%
      {{hash=LQ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Qihang},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Lu},
         familyi={L\bibinitperiod},
         given={Zhaosong},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=XL}{%
         family={Xiao},
         familyi={X\bibinitperiod},
         given={Lin},
         giveni={L\bibinitperiod},
      }}%
    }
    \strng{namehash}{LQLZXL1}
    \strng{fullhash}{LQLZXL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{An accelerated proximal coordinate gradient method}
    \field{volume}{27}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2014}
  \endentry

  \entry{shalev2014accelerated}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=SSS}{%
         family={Shalev-Shwartz},
         familyi={S\bibinithyphendelim S\bibinitperiod},
         given={Shai},
         giveni={S\bibinitperiod},
      }}%
      {{hash=ZT}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Tong},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{SSSZT1}
    \strng{fullhash}{SSSZT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International conference on machine learning}
    \field{pages}{64\bibrangedash 72}
    \field{title}{Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization}
    \field{year}{2014}
  \endentry

  \entry{reddi2016stochastic}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=RSJ}{%
         family={Reddi},
         familyi={R\bibinitperiod},
         given={Sashank\bibnamedelima J},
         giveni={S\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Hefny},
         familyi={H\bibinitperiod},
         given={Ahmed},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Sra},
         familyi={S\bibinitperiod},
         given={Suvrit},
         giveni={S\bibinitperiod},
      }}%
      {{hash=PB}{%
         family={Poczos},
         familyi={P\bibinitperiod},
         given={Barnabas},
         giveni={B\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Smola},
         familyi={S\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{RSJHASS+1}
    \strng{fullhash}{RSJHASSPBSA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International conference on machine learning}
    \field{pages}{314\bibrangedash 323}
    \field{title}{Stochastic variance reduction for nonconvex optimization}
    \field{year}{2016}
  \endentry

  \entry{lin2015universal}{article}{}
    \name{author}{3}{}{%
      {{hash=LH}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Hongzhou},
         giveni={H\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Mairal},
         familyi={M\bibinitperiod},
         given={Julien},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HZ}{%
         family={Harchaoui},
         familyi={H\bibinitperiod},
         given={Zaid},
         giveni={Z\bibinitperiod},
      }}%
    }
    \strng{namehash}{LHMJHZ1}
    \strng{fullhash}{LHMJHZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{A universal catalyst for first-order optimization}
    \field{volume}{28}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2015}
  \endentry

  \entry{allen2017katyusha}{article}{}
    \name{author}{1}{}{%
      {{hash=AZZ}{%
         family={Allen-Zhu},
         familyi={A\bibinithyphendelim Z\bibinitperiod},
         given={Zeyuan},
         giveni={Z\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {JMLR. org}%
    }
    \strng{namehash}{AZZ1}
    \strng{fullhash}{AZZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{8194\bibrangedash 8244}
    \field{title}{Katyusha: {T}he first direct acceleration of stochastic
  gradient methods}
    \field{volume}{18}
    \field{journaltitle}{The Journal of Machine Learning Research}
    \field{year}{2017}
  \endentry

  \entry{lan2018optimal}{article}{}
    \name{author}{2}{}{%
      {{hash=LG}{%
         family={Lan},
         familyi={L\bibinitperiod},
         given={Guanghui},
         giveni={G\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Yi},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{LGZY1}
    \strng{fullhash}{LGZY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{167\bibrangedash 215}
    \field{title}{An optimal randomized incremental gradient method}
    \field{volume}{171}
    \field{journaltitle}{Mathematical programming}
    \field{year}{2018}
  \endentry

  \entry{lan2019unified}{article}{}
    \name{author}{3}{}{%
      {{hash=LG}{%
         family={Lan},
         familyi={L\bibinitperiod},
         given={Guanghui},
         giveni={G\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Zhize},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Yi},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{LGLZZY1}
    \strng{fullhash}{LGLZZY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{A unified variance-reduced accelerated gradient method for
  convex optimization}
    \field{volume}{32}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2019}
  \endentry

  \entry{joulani2020simpler}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=JP}{%
         family={Joulani},
         familyi={J\bibinitperiod},
         given={Pooria},
         giveni={P\bibinitperiod},
      }}%
      {{hash=RA}{%
         family={Raj},
         familyi={R\bibinitperiod},
         given={Anant},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Gyorgy},
         familyi={G\bibinitperiod},
         given={Andras},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SC}{%
         family={Szepesv{\'a}ri},
         familyi={S\bibinitperiod},
         given={Csaba},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{JPRAGA+1}
    \strng{fullhash}{JPRAGASC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{4984\bibrangedash 4993}
    \field{title}{A simpler approach to accelerated optimization: {I}terative
  averaging meets optimism}
    \field{year}{2020}
  \endentry

  \entry{song2020variance}{article}{}
    \name{author}{3}{}{%
      {{hash=SC}{%
         family={Song},
         familyi={S\bibinitperiod},
         given={Chaobing},
         giveni={C\bibinitperiod},
      }}%
      {{hash=JY}{%
         family={Jiang},
         familyi={J\bibinitperiod},
         given={Yong},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=MY}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Yi},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{SCJYMY1}
    \strng{fullhash}{SCJYMY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{833\bibrangedash 844}
    \field{title}{Variance reduction via accelerated dual averaging for
  finite-sum optimization}
    \field{volume}{33}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2020}
  \endentry

  \entry{li2021anita}{article}{}
    \name{author}{1}{}{%
      {{hash=LZ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Zhize},
         giveni={Z\bibinitperiod},
      }}%
    }
    \strng{namehash}{LZ1}
    \strng{fullhash}{LZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{{ANITA: A}n optimal loopless accelerated variance-reduced
  gradient method}
    \field{journaltitle}{arXiv preprint arXiv:2103.11333}
    \field{year}{2021}
  \endentry

  \entry{liu2022kill}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yuanyuan},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=SF}{%
         family={Shang},
         familyi={S\bibinitperiod},
         given={Fanhua},
         giveni={F\bibinitperiod},
      }}%
      {{hash=AW}{%
         family={An},
         familyi={A\bibinitperiod},
         given={Weixin},
         giveni={W\bibinitperiod},
      }}%
      {{hash=LH}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Hongying},
         giveni={H\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Zhouchen},
         giveni={Z\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{LYSFAW+1}
    \strng{fullhash}{LYSFAWLHLZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{14008\bibrangedash 14035}
    \field{title}{Kill a bird with two stones: {C}losing the convergence gaps
  in non-strongly convex optimization by directly accelerated {SVRG} with
  double compensation and snapshots}
    \field{year}{2022}
  \endentry

  \entry{woodworth2016tight}{article}{}
    \name{author}{2}{}{%
      {{hash=WBE}{%
         family={Woodworth},
         familyi={W\bibinitperiod},
         given={Blake\bibnamedelima E},
         giveni={B\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Srebro},
         familyi={S\bibinitperiod},
         given={Nati},
         giveni={N\bibinitperiod},
      }}%
    }
    \strng{namehash}{WBESN1}
    \strng{fullhash}{WBESN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Tight complexity bounds for optimizing composite objectives}
    \field{volume}{29}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2016}
  \endentry

  \entry{nguyen2017sarah}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=NLM}{%
         family={Nguyen},
         familyi={N\bibinitperiod},
         given={Lam\bibnamedelima M},
         giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Jie},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Scheinberg},
         familyi={S\bibinitperiod},
         given={Katya},
         giveni={K\bibinitperiod},
      }}%
      {{hash=TM}{%
         family={Tak{\'a}{\v{c}}},
         familyi={T\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{NLMLJSK+1}
    \strng{fullhash}{NLMLJSKTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{2613\bibrangedash 2621}
    \field{title}{{SARAH}: {A} novel method for machine learning problems using
  stochastic recursive gradient}
    \field{year}{2017}
  \endentry

  \entry{allen2017natasha}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=AZZ}{%
         family={Allen-Zhu},
         familyi={A\bibinithyphendelim Z\bibinitperiod},
         given={Zeyuan},
         giveni={Z\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{AZZ1}
    \strng{fullhash}{AZZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{89\bibrangedash 97}
    \field{title}{Natasha: {F}aster non-convex stochastic optimization via
  strongly non-convex parameter}
    \field{year}{2017}
  \endentry

  \entry{fang2018spider}{article}{}
    \name{author}{4}{}{%
      {{hash=FC}{%
         family={Fang},
         familyi={F\bibinitperiod},
         given={Cong},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LCJ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Chris\bibnamedelima Junchi},
         giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Zhouchen},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=ZT}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Tong},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{FCLCJLZ+1}
    \strng{fullhash}{FCLCJLZZT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{{SPIDER}: {N}ear-optimal non-convex optimization via
  stochastic path-integrated differential estimator}
    \field{volume}{31}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2018}
  \endentry

  \entry{cutkosky2019momentum}{article}{}
    \name{author}{2}{}{%
      {{hash=CA}{%
         family={Cutkosky},
         familyi={C\bibinitperiod},
         given={Ashok},
         giveni={A\bibinitperiod},
      }}%
      {{hash=OF}{%
         family={Orabona},
         familyi={O\bibinitperiod},
         given={Francesco},
         giveni={F\bibinitperiod},
      }}%
    }
    \strng{namehash}{CAOF1}
    \strng{fullhash}{CAOF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Momentum-based variance reduction in non-convex sgd}
    \field{volume}{32}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2019}
  \endentry

  \entry{latorre2020lipschitz}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LF}{%
         family={Latorre},
         familyi={L\bibinitperiod},
         given={Fabian},
         giveni={F\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Rolland},
         familyi={R\bibinitperiod},
         given={Paul},
         giveni={P\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Cevher},
         familyi={C\bibinitperiod},
         given={Volkan},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{LFRPCV1}
    \strng{fullhash}{LFRPCV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Learning Representations}
    \field{title}{Lipschitz constant estimation of Neural Networks via sparse
  polynomial optimization}
    \field{year}{2020}
  \endentry

  \entry{armijo1966minimization}{article}{}
    \name{author}{1}{}{%
      {{hash=AL}{%
         family={Armijo},
         familyi={A\bibinitperiod},
         given={Larry},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Mathematical Sciences Publishers}%
    }
    \strng{namehash}{AL1}
    \strng{fullhash}{AL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{1\bibrangedash 3}
    \field{title}{Minimization of functions having {L}ipschitz continuous first
  partial derivatives}
    \field{volume}{16}
    \field{journaltitle}{Pacific Journal of mathematics}
    \field{year}{1966}
  \endentry

  \entry{nesterov2015universal}{article}{}
    \name{author}{1}{}{%
      {{hash=NY}{%
         family={Nesterov},
         familyi={N\bibinitperiod},
         given={Yurii},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{NY1}
    \strng{fullhash}{NY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{381\bibrangedash 404}
    \field{title}{Universal gradient methods for convex optimization problems}
    \field{volume}{152}
    \field{journaltitle}{Mathematical Programming}
    \field{year}{2015}
  \endentry

  \entry{lemarechal1995new}{article}{}
    \name{author}{3}{}{%
      {{hash=LC}{%
         family={Lemar{\'e}chal},
         familyi={L\bibinitperiod},
         given={Claude},
         giveni={C\bibinitperiod},
      }}%
      {{hash=NA}{%
         family={Nemirovskii},
         familyi={N\bibinitperiod},
         given={Arkadii},
         giveni={A\bibinitperiod},
      }}%
      {{hash=NY}{%
         family={Nesterov},
         familyi={N\bibinitperiod},
         given={Yurii},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{LCNANY1}
    \strng{fullhash}{LCNANY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{111\bibrangedash 147}
    \field{title}{New variants of bundle methods}
    \field{volume}{69}
    \field{journaltitle}{Mathematical programming}
    \field{year}{1995}
  \endentry

  \entry{lan2015bundle}{article}{}
    \name{author}{1}{}{%
      {{hash=LG}{%
         family={Lan},
         familyi={L\bibinitperiod},
         given={Guanghui},
         giveni={G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{LG1}
    \strng{fullhash}{LG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1-2}
    \field{pages}{1\bibrangedash 45}
    \field{title}{Bundle-level type methods uniformly optimal for smooth and
  nonsmooth convex optimization}
    \field{volume}{149}
    \field{journaltitle}{Mathematical Programming}
    \field{year}{2015}
  \endentry

  \entry{barzilai1988two}{article}{}
    \name{author}{2}{}{%
      {{hash=BJ}{%
         family={Barzilai},
         familyi={B\bibinitperiod},
         given={Jonathan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BJM}{%
         family={Borwein},
         familyi={B\bibinitperiod},
         given={Jonathan\bibnamedelima M},
         giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Oxford University Press}%
    }
    \strng{namehash}{BJBJM1}
    \strng{fullhash}{BJBJM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{141\bibrangedash 148}
    \field{title}{Two-point step size gradient methods}
    \field{volume}{8}
    \field{journaltitle}{IMA journal of numerical analysis}
    \field{year}{1988}
  \endentry

  \entry{vrahatis2000class}{article}{}
    \name{author}{4}{}{%
      {{hash=VMN}{%
         family={Vrahatis},
         familyi={V\bibinitperiod},
         given={Michael\bibnamedelima N},
         giveni={M\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=AGS}{%
         family={Androulakis},
         familyi={A\bibinitperiod},
         given={George\bibnamedelima S},
         giveni={G\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=LJN}{%
         family={Lambrinos},
         familyi={L\bibinitperiod},
         given={John\bibnamedelima N},
         giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=MGD}{%
         family={Magoulas},
         familyi={M\bibinitperiod},
         given={George\bibnamedelima D},
         giveni={G\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier}%
    }
    \strng{namehash}{VMNAGSLJN+1}
    \strng{fullhash}{VMNAGSLJNMGD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{2}
    \field{pages}{367\bibrangedash 386}
    \field{title}{A class of gradient unconstrained minimization algorithms
  with adaptive stepsize}
    \field{volume}{114}
    \field{journaltitle}{Journal of Computational and Applied Mathematics}
    \field{year}{2000}
  \endentry

  \entry{malitsky2020adaptive}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=MY}{%
         family={Malitsky},
         familyi={M\bibinitperiod},
         given={Yura},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=MK}{%
         family={Mishchenko},
         familyi={M\bibinitperiod},
         given={Konstantin},
         giveni={K\bibinitperiod},
      }}%
    }
    \strng{namehash}{MYMK1}
    \strng{fullhash}{MYMK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 37th International Conference on
  Machine Learning}
    \field{pages}{6702\bibrangedash 6712}
    \field{title}{Adaptive gradient descent without descent}
    \field{year}{2020}
  \endentry

  \entry{vaswani2019painless}{article}{}
    \name{author}{6}{}{%
      {{hash=VS}{%
         family={Vaswani},
         familyi={V\bibinitperiod},
         given={Sharan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mishkin},
         familyi={M\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LI}{%
         family={Laradji},
         familyi={L\bibinitperiod},
         given={Issam},
         giveni={I\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Schmidt},
         familyi={S\bibinitperiod},
         given={Mark},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GG}{%
         family={Gidel},
         familyi={G\bibinitperiod},
         given={Gauthier},
         giveni={G\bibinitperiod},
      }}%
      {{hash=LJS}{%
         family={Lacoste-Julien},
         familyi={L\bibinithyphendelim J\bibinitperiod},
         given={Simon},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{VSMALI+1}
    \strng{fullhash}{VSMALISMGGLJS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Painless stochastic gradient: {I}nterpolation, line-search,
  and convergence rates}
    \field{volume}{32}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2019}
  \endentry

  \entry{dvinskikh2019adaptive}{article}{}
    \name{author}{6}{}{%
      {{hash=DD}{%
         family={Dvinskikh},
         familyi={D\bibinitperiod},
         given={Darina},
         giveni={D\bibinitperiod},
      }}%
      {{hash=OA}{%
         family={Ogaltsov},
         familyi={O\bibinitperiod},
         given={Aleksandr},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Gasnikov},
         familyi={G\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=DP}{%
         family={Dvurechensky},
         familyi={D\bibinitperiod},
         given={Pavel},
         giveni={P\bibinitperiod},
      }}%
      {{hash=TA}{%
         family={Tyurin},
         familyi={T\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SV}{%
         family={Spokoiny},
         familyi={S\bibinitperiod},
         given={Vladimir},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{DDOAGA+1}
    \strng{fullhash}{DDOAGADPTASV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Adaptive gradient descent for convex and non-convex
  stochastic optimization}
    \field{journaltitle}{arXiv preprint arXiv:1911.08380}
    \field{year}{2019}
  \endentry

  \entry{tan2016barzilai}{article}{}
    \name{author}{4}{}{%
      {{hash=TC}{%
         family={Tan},
         familyi={T\bibinitperiod},
         given={Conghui},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Shiqian},
         giveni={S\bibinitperiod},
      }}%
      {{hash=DYH}{%
         family={Dai},
         familyi={D\bibinitperiod},
         given={Yu-Hong},
         giveni={Y\bibinithyphendelim H\bibinitperiod},
      }}%
      {{hash=QY}{%
         family={Qian},
         familyi={Q\bibinitperiod},
         given={Yuqiu},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{TCMSDYH+1}
    \strng{fullhash}{TCMSDYHQY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Barzilai-{B}orwein step size for stochastic gradient descent}
    \field{volume}{29}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2016}
  \endentry

  \entry{liu2019class}{article}{}
    \name{author}{3}{}{%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yan},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=HC}{%
         family={Han},
         familyi={H\bibinitperiod},
         given={Congying},
         giveni={C\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Guo},
         familyi={G\bibinitperiod},
         given={Tiande},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{LYHCGT1}
    \strng{fullhash}{LYHCGT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{A class of stochastic variance reduced methods with an
  adaptive stepsize}
    \field{journaltitle}{URL http://www. optimization-online.
  org/DB\_FILE/2019/04/7170. pdf}
    \field{year}{2019}
  \endentry

  \entry{li2019adaptive}{article}{}
    \name{author}{2}{}{%
      {{hash=LB}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Bingcong},
         giveni={B\bibinitperiod},
      }}%
      {{hash=GGB}{%
         family={Giannakis},
         familyi={G\bibinitperiod},
         given={Georgios\bibnamedelima B},
         giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \strng{namehash}{LBGGB1}
    \strng{fullhash}{LBGGB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Adaptive step sizes in variance reduction via regularization}
    \field{journaltitle}{arXiv preprint arXiv:1910.06532}
    \field{year}{2019}
  \endentry

  \entry{li2020almost}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LB}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Bingcong},
         giveni={B\bibinitperiod},
      }}%
      {{hash=WL}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Lingda},
         giveni={L\bibinitperiod},
      }}%
      {{hash=GGB}{%
         family={Giannakis},
         familyi={G\bibinitperiod},
         given={Georgios\bibnamedelima B},
         giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{LBWLGGB1}
    \strng{fullhash}{LBWLGGB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International conference on machine learning}
    \field{pages}{5969\bibrangedash 5978}
    \field{title}{Almost tune-free variance reduction}
    \field{year}{2020}
  \endentry

  \entry{yang2021accelerating}{article}{}
    \name{author}{3}{}{%
      {{hash=YZ}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Zhuang},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=CZ}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Zengping},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=WC}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Cheng},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier}%
    }
    \strng{namehash}{YZCZWC1}
    \strng{fullhash}{YZCZWC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{157\bibrangedash 173}
    \field{title}{Accelerating mini-batch {SARAH} by step size rules}
    \field{volume}{558}
    \field{journaltitle}{Information Sciences}
    \field{year}{2021}
  \endentry

  \entry{mcmahan2010adaptive}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=MHB}{%
         family={McMahan},
         familyi={M\bibinitperiod},
         given={H\bibnamedelima Brendan},
         giveni={H\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Streeter},
         familyi={S\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{MHBSM1}
    \strng{fullhash}{MHBSM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 23rd Conference on Learning Theory
  (COLT)}
    \field{pages}{244}
    \field{title}{Adaptive bound optimization for online convex optimization}
    \field{year}{2010}
  \endentry

  \entry{Adagrad_article}{article}{}
    \name{author}{3}{}{%
      {{hash=DJ}{%
         family={Duchi},
         familyi={D\bibinitperiod},
         given={John},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HE}{%
         family={Hazan},
         familyi={H\bibinitperiod},
         given={Elad},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SY}{%
         family={Singer},
         familyi={S\bibinitperiod},
         given={Yoram},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{DJHESY1}
    \strng{fullhash}{DJHESY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Adaptive subgradient methods for online learning and
  stochastic optimization}
    \field{volume}{12}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2011}
  \endentry

  \entry{kingma2015adam}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=KDP}{%
         family={Kingma},
         familyi={K\bibinitperiod},
         given={Diederik\bibnamedelima P.},
         giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=BJ}{%
         family={Ba},
         familyi={B\bibinitperiod},
         given={Jimmy},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{KDPBJ1}
    \strng{fullhash}{KDPBJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Learning Representations
  (ICLR)}
    \field{title}{Adam: A Method for Stochastic Optimization}
    \field{year}{2015}
  \endentry

  \entry{levy2017online}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=LK}{%
         family={Levy},
         familyi={L\bibinitperiod},
         given={Kfir},
         giveni={K\bibinitperiod},
      }}%
    }
    \strng{namehash}{LK1}
    \strng{fullhash}{LK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems}
    \field{pages}{1613\bibrangedash 1622}
    \field{title}{Online to offline conversions, universality and adaptive
  minibatch sizes}
    \field{year}{2017}
  \endentry

  \entry{levy2018online}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LYK}{%
         family={Levy},
         familyi={L\bibinitperiod},
         given={Yehuda\bibnamedelima Kfir},
         giveni={Y\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
      {{hash=YA}{%
         family={Yurtsever},
         familyi={Y\bibinitperiod},
         given={Alp},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Cevher},
         familyi={C\bibinitperiod},
         given={Volkan},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{LYKYACV1}
    \strng{fullhash}{LYKYACV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems}
    \field{pages}{6500\bibrangedash 6509}
    \field{title}{Online adaptive methods, universality and acceleration}
    \field{year}{2018}
  \endentry

  \entry{kavis2019unixgrad}{article}{}
    \name{author}{4}{}{%
      {{hash=KA}{%
         family={Kavis},
         familyi={K\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LKY}{%
         family={Levy},
         familyi={L\bibinitperiod},
         given={Kfir\bibnamedelima Y},
         giveni={K\bibinitperiod\bibinitdelim Y\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Cevher},
         familyi={C\bibinitperiod},
         given={Volkan},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{KALKYBF+1}
    \strng{fullhash}{KALKYBFCV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Unix{G}rad: {A} universal, adaptive algorithm with optimal
  guarantees for constrained optimization}
    \field{volume}{32}
    \field{journaltitle}{Advances in neural information processing systems}
    \field{year}{2019}
  \endentry

  \entry{wu2018wngrad}{article}{}
    \name{author}{3}{}{%
      {{hash=WX}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Xiaoxia},
         giveni={X\bibinitperiod},
      }}%
      {{hash=WR}{%
         family={Ward},
         familyi={W\bibinitperiod},
         given={Rachel},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={L{\'e}on},
         giveni={L\bibinitperiod},
      }}%
    }
    \strng{namehash}{WXWRBL1}
    \strng{fullhash}{WXWRBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{{WNG}rad: {L}earn the learning rate in gradient descent}
    \field{journaltitle}{arXiv preprint arXiv:1803.02865}
    \field{year}{2018}
  \endentry

  \entry{cutkosky2019anytime}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=CA}{%
         family={Cutkosky},
         familyi={C\bibinitperiod},
         given={Ashok},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{CA1}
    \strng{fullhash}{CA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{1446\bibrangedash 1454}
    \field{title}{Anytime online-to-batch, optimism and acceleration}
    \field{year}{2019}
  \endentry

  \entry{ene2021adaptive}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=EA}{%
         family={Ene},
         familyi={E\bibinitperiod},
         given={Alina},
         giveni={A\bibinitperiod},
      }}%
      {{hash=NHL}{%
         family={Nguyen},
         familyi={N\bibinitperiod},
         given={Huy\bibnamedelima L},
         giveni={H\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
      {{hash=VA}{%
         family={Vladu},
         familyi={V\bibinitperiod},
         given={Adrian},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{EANHLVA1}
    \strng{fullhash}{EANHLVA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the AAAI Conference on Artificial
  Intelligence}
    \field{number}{8}
    \field{pages}{7314\bibrangedash 7321}
    \field{title}{Adaptive gradient methods for constrained convex optimization
  and variational inequalities}
    \field{year}{2021}
  \endentry

  \entry{ene2022adaptive}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=EA}{%
         family={Ene},
         familyi={E\bibinitperiod},
         given={Alina},
         giveni={A\bibinitperiod},
      }}%
      {{hash=NH}{%
         family={Nguyen},
         familyi={N\bibinitperiod},
         given={Huy},
         giveni={H\bibinitperiod},
      }}%
    }
    \strng{namehash}{EANH1}
    \strng{fullhash}{EANH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the AAAI Conference on Artificial
  Intelligence}
    \field{number}{6}
    \field{pages}{6559\bibrangedash 6567}
    \field{title}{Adaptive and universal algorithms for variational
  inequalities with optimal convergence}
    \field{year}{2022}
  \endentry

  \entry{ward2020adagrad}{article}{}
    \name{author}{3}{}{%
      {{hash=WR}{%
         family={Ward},
         familyi={W\bibinitperiod},
         given={Rachel},
         giveni={R\bibinitperiod},
      }}%
      {{hash=WX}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Xiaoxia},
         giveni={X\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={L\'eon},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {JMLRORG}%
    }
    \strng{namehash}{WRWXBL1}
    \strng{fullhash}{WRWXBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{1}
    \field{pages}{9047\bibrangedash 9076}
    \field{title}{Ada{G}rad stepsizes: {S}harp convergence over nonconvex
  landscapes}
    \field{volume}{21}
    \field{journaltitle}{The Journal of Machine Learning Research}
    \field{year}{2020}
  \endentry

  \entry{levy2021storm+}{article}{}
    \name{author}{3}{}{%
      {{hash=LK}{%
         family={Levy},
         familyi={L\bibinitperiod},
         given={Kfir},
         giveni={K\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Kavis},
         familyi={K\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Cevher},
         familyi={C\bibinitperiod},
         given={Volkan},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{LKKACV1}
    \strng{fullhash}{LKKACV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{20571\bibrangedash 20582}
    \field{title}{{STORM}+: {F}ully adaptive {SGD} with recursive momentum for
  nonconvex optimization}
    \field{volume}{34}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2021}
  \endentry

  \entry{kavis2022high}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KA}{%
         family={Kavis},
         familyi={K\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Levy},
         familyi={L\bibinitperiod},
         given={{Kfir Y.}},
         giveni={K\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Cevher},
         familyi={C\bibinitperiod},
         given={Volkan},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{KALKCV1}
    \strng{fullhash}{KALKCV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this paper, we propose a new, simplified high probability analysis of
  AdaGrad for smooth, non-convex problems. More specifically, we focus on a
  particular accelerated gradient (AGD) template (Lan, 2020), through which we
  recover the original AdaGrad and its variant with averaging, and prove a
  convergence rate of O(1/âˆšT) with high probability without the knowledge of
  smoothness and variance. We use a particular version of Freedman's
  concentration bound for martingale difference sequences (Kakade & Tewari,
  2008) which enables us to achieve the best-known dependence of log(1/Î´) on
  the probability margin Î´. We present our analysis in a modular way and
  obtain a complementary O(1/T) convergence rate in the deterministic setting.
  To the best of our knowledge, this is the first high probability result for
  AdaGrad with a truly adaptive scheme, i.e., completely oblivious to the
  knowledge of smoothness and uniform variance bound, which simultaneously has
  best-known dependence of log(1/Î´). We further prove noise adaptation
  property of AdaGrad under additional noise assumptions.%
    }
    \field{note}{Publisher Copyright: {\textcopyright} 2022 ICLR 2022 - 10th
  International Conference on Learning Representationss.}
    \field{title}{High probability bounds for a class of nonconvex algorithms
  with AdaGrad stepsize}
    \field{year}{2022}
  \endentry

  \entry{faw2022power}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=FM}{%
         family={Faw},
         familyi={F\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
      {{hash=TI}{%
         family={Tziotis},
         familyi={T\bibinitperiod},
         given={Isidoros},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CC}{%
         family={Caramanis},
         familyi={C\bibinitperiod},
         given={Constantine},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mokhtari},
         familyi={M\bibinitperiod},
         given={Aryan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Shakkottai},
         familyi={S\bibinitperiod},
         given={Sanjay},
         giveni={S\bibinitperiod},
      }}%
      {{hash=WR}{%
         family={Ward},
         familyi={W\bibinitperiod},
         given={Rachel},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{FMTICC+1}
    \strng{fullhash}{FMTICCMASSWR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Conference on Learning Theory}
    \field{pages}{313\bibrangedash 355}
    \field{title}{The power of adaptivity in {SGD}: {S}elf-tuning step sizes
  with unbounded gradients and affine variance}
    \field{year}{2022}
  \endentry

  \entry{attia2023sgd}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=AA}{%
         family={Attia},
         familyi={A\bibinitperiod},
         given={Amit},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KT}{%
         family={Koren},
         familyi={K\bibinitperiod},
         given={Tomer},
         giveni={T\bibinitperiod},
      }}%
    }
    \name{editor}{6}{}{%
      {{hash=KA}{%
         family={Krause},
         familyi={K\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BE}{%
         family={Brunskill},
         familyi={B\bibinitperiod},
         given={Emma},
         giveni={E\bibinitperiod},
      }}%
      {{hash=CK}{%
         family={Cho},
         familyi={C\bibinitperiod},
         given={Kyunghyun},
         giveni={K\bibinitperiod},
      }}%
      {{hash=EB}{%
         family={Engelhardt},
         familyi={E\bibinitperiod},
         given={Barbara},
         giveni={B\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Sabato},
         familyi={S\bibinitperiod},
         given={Sivan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Scarlett},
         familyi={S\bibinitperiod},
         given={Jonathan},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \strng{namehash}{AAKT1}
    \strng{fullhash}{AAKT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We study Stochastic Gradient Descent with AdaGrad stepsizes: a popular
  adaptive (self-tuning) method for first-order stochastic optimization.
  Despite being well studied, existing analyses of this method suffer from
  various shortcomings: they either assume some knowledge of the problem
  parameters, impose strong global Lipschitz conditions, or fail to give bounds
  that hold with high probability. We provide a comprehensive analysis of this
  basic method without any of these limitations, in both the convex and
  non-convex (smooth) cases, that additionally supports a general â€œaffine
  varianceâ€ noise model and provides sharp rates of convergence in both the
  low-noise and high-noise regimes.%
    }
    \field{booktitle}{Proceedings of the 40th International Conference on
  Machine Learning}
    \field{pages}{1147\bibrangedash 1171}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{{SGD} with {A}da{G}rad stepsizes: full adaptivity with high
  probability to unknown parameters, unbounded gradients and affine variance}
    \verb{url}
    \verb https://proceedings.mlr.press/v202/attia23a.html
    \endverb
    \field{volume}{202}
    \verb{file}
    \verb https://proceedings.mlr.press/v202/attia23a/attia23a.pdf
    \endverb
    \field{year}{2023}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{liu2022adaptive}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=LZ}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Zijian},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=NTD}{%
         family={Nguyen},
         familyi={N\bibinitperiod},
         given={Ta\bibnamedelima Duy},
         giveni={T\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=EA}{%
         family={Ene},
         familyi={E\bibinitperiod},
         given={Alina},
         giveni={A\bibinitperiod},
      }}%
      {{hash=NH}{%
         family={Nguyen},
         familyi={N\bibinitperiod},
         given={Huy},
         giveni={H\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{LZNTDEA+1}
    \strng{fullhash}{LZNTDEANH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{13947\bibrangedash 13994}
    \field{title}{Adaptive accelerated (extra-) gradient methods with variance
  reduction}
    \field{year}{2022}
  \endentry

  \entry{dubois2022svrg}{article}{}
    \name{author}{5}{}{%
      {{hash=DTB}{%
         family={Dubois-Taine},
         familyi={D\bibinithyphendelim T\bibinitperiod},
         given={Benjamin},
         giveni={B\bibinitperiod},
      }}%
      {{hash=VS}{%
         family={Vaswani},
         familyi={V\bibinitperiod},
         given={Sharan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BR}{%
         family={Babanezhad},
         familyi={B\bibinitperiod},
         given={Reza},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Schmidt},
         familyi={S\bibinitperiod},
         given={Mark},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LJS}{%
         family={Lacoste-Julien},
         familyi={L\bibinithyphendelim J\bibinitperiod},
         given={Simon},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{DTBVSBR+1}
    \strng{fullhash}{DTBVSBRSMLJS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{1\bibrangedash 51}
    \field{title}{{SVRG} meets {A}da{G}rad: {P}ainless variance reduction}
    \field{journaltitle}{Machine Learning}
    \field{year}{2022}
  \endentry

  \entry{li2022variance}{article}{}
    \name{author}{4}{}{%
      {{hash=LW}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Wenjie},
         giveni={W\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Zhanyu},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Yichen},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=CG}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={Guang},
         giveni={G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{LWWZZY+1}
    \strng{fullhash}{LWWZZYCG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{1\bibrangedash 39}
    \field{title}{Variance reduction on general adaptive stochastic mirror
  descent}
    \field{journaltitle}{Machine Learning}
    \field{year}{2022}
  \endentry

  \entry{wang2022divergence}{article}{}
    \name{author}{2}{}{%
      {{hash=WR}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Ruiqi},
         giveni={R\bibinitperiod},
      }}%
      {{hash=KD}{%
         family={Klabjan},
         familyi={K\bibinitperiod},
         given={Diego},
         giveni={D\bibinitperiod},
      }}%
    }
    \strng{namehash}{WRKD1}
    \strng{fullhash}{WRKD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Divergence results and convergence of a variance reduced
  Version of {ADAM}}
    \field{journaltitle}{arXiv preprint arXiv:2210.05607}
    \field{year}{2022}
  \endentry

  \entry{kavis2022adaptive}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=KA}{%
         family={Kavis},
         familyi={K\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SEP}{%
         family={SKOULAKIS},
         familyi={S\bibinitperiod},
         given={EFSTRATIOS\bibnamedelima PANTELEIMON},
         giveni={E\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=AK}{%
         family={Antonakopoulos},
         familyi={A\bibinitperiod},
         given={Kimon},
         giveni={K\bibinitperiod},
      }}%
      {{hash=DLT}{%
         family={Dadi},
         familyi={D\bibinitperiod},
         given={Leello\bibnamedelima Tadesse},
         giveni={L\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Cevher},
         familyi={C\bibinitperiod},
         given={Volkan},
         giveni={V\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=OAH}{%
         family={Oh},
         familyi={O\bibinitperiod},
         given={Alice\bibnamedelima H.},
         giveni={A\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Agarwal},
         familyi={A\bibinitperiod},
         given={Alekh},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BD}{%
         family={Belgrave},
         familyi={B\bibinitperiod},
         given={Danielle},
         giveni={D\bibinitperiod},
      }}%
      {{hash=CK}{%
         family={Cho},
         familyi={C\bibinitperiod},
         given={Kyunghyun},
         giveni={K\bibinitperiod},
      }}%
    }
    \strng{namehash}{KASEPAK+1}
    \strng{fullhash}{KASEPAKDLTCV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems}
    \field{title}{Adaptive stochastic variance reduction for non-convex
  finite-sum minimization}
    \verb{url}
    \verb https://openreview.net/forum?id=k98U0cb0Ig
    \endverb
    \field{year}{2022}
  \endentry

  \entry{cutkosky2018distributed}{article}{}
    \name{author}{2}{}{%
      {{hash=CA}{%
         family={Cutkosky},
         familyi={C\bibinitperiod},
         given={Ashok},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BFR}{%
         family={Busa-Fekete},
         familyi={B\bibinithyphendelim F\bibinitperiod},
         given={R{\'o}bert},
         giveni={R\bibinitperiod},
      }}%
    }
    \strng{namehash}{CABFR1}
    \strng{fullhash}{CABFR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Distributed stochastic optimization via adaptive SGD}
    \field{volume}{31}
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{year}{2018}
  \endentry

  \entry{li2023convergence}{article}{}
    \name{author}{3}{}{%
      {{hash=LH}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Haochuan},
         giveni={H\bibinitperiod},
      }}%
      {{hash=JA}{%
         family={Jadbabaie},
         familyi={J\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
      {{hash=RA}{%
         family={Rakhlin},
         familyi={R\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{LHJARA1}
    \strng{fullhash}{LHJARA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Convergence of {A}dam under relaxed assumptions}
    \field{journaltitle}{arXiv preprint arXiv:2304.13972}
    \field{year}{2023}
  \endentry

  \entry{xie2022adaptive}{article}{}
    \name{author}{5}{}{%
      {{hash=XB}{%
         family={Xie},
         familyi={X\bibinitperiod},
         given={Binghui},
         giveni={B\bibinitperiod},
      }}%
      {{hash=JC}{%
         family={Jin},
         familyi={J\bibinitperiod},
         given={Chenhan},
         giveni={C\bibinitperiod},
      }}%
      {{hash=ZK}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Kaiwen},
         giveni={K\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={James},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MW}{%
         family={Meng},
         familyi={M\bibinitperiod},
         given={Wei},
         giveni={W\bibinitperiod},
      }}%
    }
    \strng{namehash}{XBJCZK+1}
    \strng{fullhash}{XBJCZKCJMW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{An adaptive incremental gradient method with support for
  non-euclidean norms}
    \field{journaltitle}{arXiv preprint arXiv:2205.02273}
    \field{year}{2022}
  \endentry

  \entry{shi2021ai}{article}{}
    \name{author}{5}{}{%
      {{hash=SZ}{%
         family={Shi},
         familyi={S\bibinitperiod},
         given={Zheng},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Sadiev},
         familyi={S\bibinitperiod},
         given={Abdurakhmon},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LN}{%
         family={Loizou},
         familyi={L\bibinitperiod},
         given={Nicolas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Richt{\'a}rik},
         familyi={R\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=TM}{%
         family={Tak{\'a}{\v{c}}},
         familyi={T\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{SZSALN+1}
    \strng{fullhash}{SZSALNRPTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{{AI-SARAH}: {A}daptive and implicit stochastic recursive
  gradient methods}
    \field{journaltitle}{arXiv preprint arXiv:2102.09700}
    \field{year}{2021}
  \endentry

  \entry{gower2016stochastic}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=GR}{%
         family={Gower},
         familyi={G\bibinitperiod},
         given={Robert},
         giveni={R\bibinitperiod},
      }}%
      {{hash=GD}{%
         family={Goldfarb},
         familyi={G\bibinitperiod},
         given={Donald},
         giveni={D\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Richt{\'a}rik},
         familyi={R\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {PMLR}%
    }
    \strng{namehash}{GRGDRP1}
    \strng{fullhash}{GRGDRP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Machine Learning}
    \field{pages}{1869\bibrangedash 1878}
    \field{title}{Stochastic block {BFGS}: {S}queezing more curvature out of
  data}
    \field{year}{2016}
  \endentry

  \entry{gorbunov2020unified}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=GE}{%
         family={Gorbunov},
         familyi={G\bibinitperiod},
         given={Eduard},
         giveni={E\bibinitperiod},
      }}%
      {{hash=HF}{%
         family={Hanzely},
         familyi={H\bibinitperiod},
         given={Filip},
         giveni={F\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Richtarik},
         familyi={R\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=CS}{%
         family={Chiappa},
         familyi={C\bibinitperiod},
         given={Silvia},
         giveni={S\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Calandra},
         familyi={C\bibinitperiod},
         given={Roberto},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \strng{namehash}{GEHFRP1}
    \strng{fullhash}{GEHFRP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this paper we introduce a unified analysis of a large family of variants
  of proximal stochastic gradient descent (SGD) which so far have required
  different intuitions, convergence analyses, have different applications, and
  which have been developed separately in various communities. We show that our
  framework includes methods with and without the following tricks, and their
  combinations: variance reduction, importance sampling, mini-batch sampling,
  quantization, and coordinate sub-sampling. As a by-product, we obtain the
  first unified theory of SGD and randomized coordinate descent (RCD) methods,
  the first unified theory of variance reduced and non-variance-reduced SGD
  methods, and the first unified theory of quantized and non-quantized methods.
  A key to our approach is a parametric assumption on the iterates and
  stochastic gradients. In a single theorem we establish a linear convergence
  result under this assumption and strong-quasi convexity of the loss function.
  Whenever we recover an existing method as a special case, our theorem gives
  the best known complexity result. Our approach can be used to motivate the
  development of new useful methods, and offers pre-proved convergence
  guarantees. To illustrate the strength of our approach, we develop five new
  variants of SGD, and through numerical experiments demonstrate some of their
  properties.%
    }
    \field{booktitle}{Proceedings of the Twenty Third International Conference
  on Artificial Intelligence and Statistics}
    \field{pages}{680\bibrangedash 690}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{A unified theory of SGD: Variance reduction, sampling,
  quantization and coordinate descent}
    \verb{url}
    \verb https://proceedings.mlr.press/v108/gorbunov20a.html
    \endverb
    \field{volume}{108}
    \verb{file}
    \verb http://proceedings.mlr.press/v108/gorbunov20a/gorbunov20a.pdf
    \endverb
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{condat2022murana}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=CL}{%
         family={Condat},
         familyi={C\bibinitperiod},
         given={Laurent},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Richtarik},
         familyi={R\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=DB}{%
         family={Dong},
         familyi={D\bibinitperiod},
         given={Bin},
         giveni={B\bibinitperiod},
      }}%
      {{hash=LQ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Qianxiao},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=WL}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Lei},
         giveni={L\bibinitperiod},
      }}%
      {{hash=XZQJ}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={Zhi-Qin\bibnamedelima John},
         giveni={Z\bibinithyphendelim Q\bibinitperiod\bibinitdelim
  J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \strng{namehash}{CLRP1}
    \strng{fullhash}{CLRP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We propose a generic variance-reduced algorithm, which we call MUltiple
  RANdomized Algorithm (MURANA), for minimizing a sum of several smooth
  functions plus a regularizer, in a sequential or distributed manner. Our
  method is formulated with general stochastic operators, which allow us to
  model various strategies for reducing the computational complexity. For
  example, MURANA supports sparse activation of the gradients, and also
  reduction of the communication load via compression of the update vectors.
  This versatility allows MURANA to cover many existing randomization
  mechanisms within a unified framework, which also makes it possible to design
  new methods as special cases.%
    }
    \field{booktitle}{Proceedings of Mathematical and Scientific Machine
  Learning}
    \field{pages}{81\bibrangedash 96}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{MURANA: A generic framework for stochastic variance-reduced
  optimization}
    \verb{url}
    \verb https://proceedings.mlr.press/v190/condat22a.html
    \endverb
    \field{volume}{190}
    \verb{file}
    \verb https://proceedings.mlr.press/v190/condat22a/condat22a.pdf
    \endverb
    \field{year}{2022}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{misc_covertype_31}{misc}{}
    \name{author}{1}{}{%
      {{hash=BJ}{%
         family={Blackard},
         familyi={B\bibinitperiod},
         given={Jock},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{BJ1}
    \strng{fullhash}{BJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{howpublished}{UCI Machine Learning Repository}
    \field{note}{{DOI}: https://doi.org/10.24432/C50K5N}
    \field{title}{{Covertype}}
    \field{year}{1998}
  \endentry

  \entry{scMark}{misc}{}
    \name{author}{1}{}{%
      {{hash=DMJ}{%
         family={Diaz-Mejia},
         familyi={D\bibinithyphendelim M\bibinitperiod},
         given={Javier},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Zenodo}%
    }
    \strng{namehash}{DMJ1}
    \strng{fullhash}{DMJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.5281/zenodo.5765804
    \endverb
    \field{title}{scMARK an 'MNIST' like benchmark to evaluate and optimize
  models for unifying scRNA data}
    \verb{url}
    \verb https://doi.org/10.5281/zenodo.5765804
    \endverb
    \field{version}{1.0}
    \field{year}{2021}
  \endentry

  \entry{ruder2017overview}{misc}{}
    \name{author}{1}{}{%
      {{hash=RS}{%
         family={Ruder},
         familyi={R\bibinitperiod},
         given={Sebastian},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{RS1}
    \strng{fullhash}{RS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1609.04747
    \endverb
    \field{title}{An overview of gradient descent optimization algorithms}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{year}{2017}
  \endentry

  \entry{Nesterov}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=NY}{%
         family={Nesterov},
         familyi={N\bibinitperiod},
         given={Yurii},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{NY1}
    \strng{fullhash}{NY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Applied Optimization}
    \field{title}{Introductory lectures on convex optimization - A basic
  course}
    \field{year}{2004}
  \endentry

  \entry{DefazioThese}{thesis}{}
    \name{author}{1}{}{%
      {{hash=DA}{%
         family={Defazio},
         familyi={D\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {arXiv}%
    }
    \keyw{Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer
  and information sciences, FOS: Computer and information sciences}
    \strng{namehash}{DA1}
    \strng{fullhash}{DA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.48550/ARXIV.1510.02533
    \endverb
    \field{title}{New optimisation methods for machine learning}
    \verb{url}
    \verb https://arxiv.org/abs/1510.02533
    \endverb
    \field{type}{phdthesis}
    \field{year}{2015}
  \endentry
\enddatalist
\endinput
