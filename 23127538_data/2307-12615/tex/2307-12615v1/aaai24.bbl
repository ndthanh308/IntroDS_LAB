\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Cauchy(1847)]{cauchy1847methode}
Augustin-Louis Cauchy.
\newblock M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes
  dâ€™{\'e}quations simultan{\'e}es.
\newblock \emph{Comptes rendus de l'Acad\'{e}mie des sciences}, 25\penalty0
  (1847):\penalty0 536--538, 1847.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{roux2012stochastic}
Nicolas Le~Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1):\penalty0 83--112,
  2017.

\bibitem[Blatt et~al.(2007)Blatt, Hero, and Gauchman]{blatt2007convergent}
Doron Blatt, Alfred~O. Hero, and Hillel Gauchman.
\newblock A convergent incremental gradient method with a constant step size.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (1):\penalty0
  29--51, 2007.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem[Defazio et~al.(2014{\natexlab{a}})Defazio, Bach, and
  Lacoste-Julien]{SAGA_article}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pages
  1646--1654, 2014{\natexlab{a}}.

\bibitem[Allen-Zhu and Yuan(2016)]{allen2016improved}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved {SVRG} for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In \emph{International conference on machine learning}, pages
  1080--1089. PMLR, 2016.

\bibitem[Hofmann et~al.(2015)Hofmann, Lucchi, Lacoste-Julien, and
  McWilliams]{hofmann2015variance}
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Gower et~al.(2021)Gower, Richt{\'a}rik, and Bach]{gower2021stochastic}
Robert~M Gower, Peter Richt{\'a}rik, and Francis Bach.
\newblock Stochastic quasi-gradient methods: {V}ariance reduction via
  {J}acobian sketching.
\newblock \emph{Mathematical Programming}, 188:\penalty0 135--192, 2021.

\bibitem[Defazio et~al.(2014{\natexlab{b}})Defazio, Domke,
  et~al.]{defazio2014finito}
Aaron Defazio, Justin Domke, et~al.
\newblock Finito: {A} faster, permutable incremental gradient method for big
  data problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  1125--1133. PMLR, 2014{\natexlab{b}}.

\bibitem[Mairal(2013)]{mairal2013optimization}
Julien Mairal.
\newblock Optimization with first-order surrogate functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  783--791. PMLR, 2013.

\bibitem[Kone{\v{c}}n{\`y} and Richt{\'a}rik(2017)]{konevcny2017semi}
Jakub Kone{\v{c}}n{\`y} and Peter Richt{\'a}rik.
\newblock Semi-stochastic gradient descent methods.
\newblock \emph{Frontiers in Applied Mathematics and Statistics}, 3:\penalty0
  9, 2017.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Ma, and Giannakis]{li2020convergence}
Bingcong Li, Meng Ma, and Georgios~B Giannakis.
\newblock On the convergence of {SARAH} and beyond.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 223--233. PMLR, 2020{\natexlab{a}}.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0 (1), 2013.

\bibitem[Lin et~al.(2014)Lin, Lu, and Xiao]{lin2014accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated proximal coordinate gradient method.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Shalev-Shwartz and Zhang(2014)]{shalev2014accelerated}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In \emph{International conference on machine learning}, pages 64--72.
  PMLR, 2014.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{International conference on machine learning}, pages
  314--323. PMLR, 2016.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Allen-Zhu(2017{\natexlab{a}})]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: {T}he first direct acceleration of stochastic gradient
  methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8194--8244, 2017{\natexlab{a}}.

\bibitem[Lan and Zhou(2018)]{lan2018optimal}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{Mathematical programming}, 171\penalty0 (1):\penalty0 167--215,
  2018.

\bibitem[Lan et~al.(2019)Lan, Li, and Zhou]{lan2019unified}
Guanghui Lan, Zhize Li, and Yi~Zhou.
\newblock A unified variance-reduced accelerated gradient method for convex
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Joulani et~al.(2020)Joulani, Raj, Gyorgy, and
  Szepesv{\'a}ri]{joulani2020simpler}
Pooria Joulani, Anant Raj, Andras Gyorgy, and Csaba Szepesv{\'a}ri.
\newblock A simpler approach to accelerated optimization: {I}terative averaging
  meets optimism.
\newblock In \emph{International Conference on Machine Learning}, pages
  4984--4993. PMLR, 2020.

\bibitem[Song et~al.(2020)Song, Jiang, and Ma]{song2020variance}
Chaobing Song, Yong Jiang, and Yi~Ma.
\newblock Variance reduction via accelerated dual averaging for finite-sum
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 833--844, 2020.

\bibitem[Li(2021)]{li2021anita}
Zhize Li.
\newblock {ANITA: A}n optimal loopless accelerated variance-reduced gradient
  method.
\newblock \emph{arXiv preprint arXiv:2103.11333}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Shang, An, Liu, and
  Lin]{liu2022kill}
Yuanyuan Liu, Fanhua Shang, Weixin An, Hongying Liu, and Zhouchen Lin.
\newblock Kill a bird with two stones: {C}losing the convergence gaps in
  non-strongly convex optimization by directly accelerated {SVRG} with double
  compensation and snapshots.
\newblock In \emph{International Conference on Machine Learning}, pages
  14008--14035. PMLR, 2022{\natexlab{a}}.

\bibitem[Woodworth and Srebro(2016)]{woodworth2016tight}
Blake~E Woodworth and Nati Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: {A} novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{International Conference on Machine Learning}, pages
  2613--2621. PMLR, 2017.

\bibitem[Allen-Zhu(2017{\natexlab{b}})]{allen2017natasha}
Zeyuan Allen-Zhu.
\newblock Natasha: {F}aster non-convex stochastic optimization via strongly
  non-convex parameter.
\newblock In \emph{International Conference on Machine Learning}, pages 89--97.
  PMLR, 2017{\natexlab{b}}.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock {SPIDER}: {N}ear-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Latorre et~al.(2020)Latorre, Rolland, and
  Cevher]{latorre2020lipschitz}
Fabian Latorre, Paul Rolland, and Volkan Cevher.
\newblock Lipschitz constant estimation of neural networks via sparse
  polynomial optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Armijo(1966)]{armijo1966minimization}
Larry Armijo.
\newblock Minimization of functions having {L}ipschitz continuous first partial
  derivatives.
\newblock \emph{Pacific Journal of mathematics}, 16\penalty0 (1):\penalty0
  1--3, 1966.

\bibitem[Nesterov(2015)]{nesterov2015universal}
Yurii Nesterov.
\newblock Universal gradient methods for convex optimization problems.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1):\penalty0 381--404,
  2015.

\bibitem[Lemar{\'e}chal et~al.(1995)Lemar{\'e}chal, Nemirovskii, and
  Nesterov]{lemarechal1995new}
Claude Lemar{\'e}chal, Arkadii Nemirovskii, and Yurii Nesterov.
\newblock New variants of bundle methods.
\newblock \emph{Mathematical programming}, 69:\penalty0 111--147, 1995.

\bibitem[Lan(2015)]{lan2015bundle}
Guanghui Lan.
\newblock Bundle-level type methods uniformly optimal for smooth and nonsmooth
  convex optimization.
\newblock \emph{Mathematical Programming}, 149\penalty0 (1-2):\penalty0 1--45,
  2015.

\bibitem[Barzilai and Borwein(1988)]{barzilai1988two}
Jonathan Barzilai and Jonathan~M Borwein.
\newblock Two-point step size gradient methods.
\newblock \emph{IMA journal of numerical analysis}, 8\penalty0 (1):\penalty0
  141--148, 1988.

\bibitem[Vrahatis et~al.(2000)Vrahatis, Androulakis, Lambrinos, and
  Magoulas]{vrahatis2000class}
Michael~N Vrahatis, George~S Androulakis, John~N Lambrinos, and George~D
  Magoulas.
\newblock A class of gradient unconstrained minimization algorithms with
  adaptive stepsize.
\newblock \emph{Journal of Computational and Applied Mathematics}, 114\penalty0
  (2):\penalty0 367--386, 2000.

\bibitem[Malitsky and Mishchenko(2020)]{malitsky2020adaptive}
Yura Malitsky and Konstantin Mishchenko.
\newblock Adaptive gradient descent without descent.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 6702--6712, 2020.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste-Julien]{vaswani2019painless}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
  Simon Lacoste-Julien.
\newblock Painless stochastic gradient: {I}nterpolation, line-search, and
  convergence rates.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Dvinskikh et~al.(2019)Dvinskikh, Ogaltsov, Gasnikov, Dvurechensky,
  Tyurin, and Spokoiny]{dvinskikh2019adaptive}
Darina Dvinskikh, Aleksandr Ogaltsov, Alexander Gasnikov, Pavel Dvurechensky,
  Alexander Tyurin, and Vladimir Spokoiny.
\newblock Adaptive gradient descent for convex and non-convex stochastic
  optimization.
\newblock \emph{arXiv preprint arXiv:1911.08380}, 2019.

\bibitem[Tan et~al.(2016)Tan, Ma, Dai, and Qian]{tan2016barzilai}
Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian.
\newblock Barzilai-{B}orwein step size for stochastic gradient descent.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Liu et~al.(2019)Liu, Han, and Guo]{liu2019class}
Yan Liu, Congying Han, and Tiande Guo.
\newblock A class of stochastic variance reduced methods with an adaptive
  stepsize.
\newblock \emph{URL http://www. optimization-online. org/DB\_FILE/2019/04/7170.
  pdf}, 2019.

\bibitem[Li and Giannakis(2019)]{li2019adaptive}
Bingcong Li and Georgios~B Giannakis.
\newblock Adaptive step sizes in variance reduction via regularization.
\newblock \emph{arXiv preprint arXiv:1910.06532}, 2019.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Wang, and Giannakis]{li2020almost}
Bingcong Li, Lingda Wang, and Georgios~B Giannakis.
\newblock Almost tune-free variance reduction.
\newblock In \emph{International conference on machine learning}, pages
  5969--5978. PMLR, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2021)Yang, Chen, and Wang]{yang2021accelerating}
Zhuang Yang, Zengping Chen, and Cheng Wang.
\newblock Accelerating mini-batch {SARAH} by step size rules.
\newblock \emph{Information Sciences}, 558:\penalty0 157--173, 2021.

\bibitem[McMahan and Streeter(2010)]{mcmahan2010adaptive}
H~Brendan McMahan and Matthew Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{Proceedings of the 23rd Conference on Learning Theory
  (COLT)}, page 244, 2010.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Adagrad_article}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12, 2011.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Levy(2017)]{levy2017online}
Kfir Levy.
\newblock Online to offline conversions, universality and adaptive minibatch
  sizes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1613--1622, 2017.

\bibitem[Levy et~al.(2018)Levy, Yurtsever, and Cevher]{levy2018online}
Yehuda~Kfir Levy, Alp Yurtsever, and Volkan Cevher.
\newblock Online adaptive methods, universality and acceleration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6500--6509, 2018.

\bibitem[Kavis et~al.(2019)Kavis, Levy, Bach, and Cevher]{kavis2019unixgrad}
Ali Kavis, Kfir~Y Levy, Francis Bach, and Volkan Cevher.
\newblock Unix{G}rad: {A} universal, adaptive algorithm with optimal guarantees
  for constrained optimization.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Wu et~al.(2018)Wu, Ward, and Bottou]{wu2018wngrad}
Xiaoxia Wu, Rachel Ward, and L{\'e}on Bottou.
\newblock {WNG}rad: {L}earn the learning rate in gradient descent.
\newblock \emph{arXiv preprint arXiv:1803.02865}, 2018.

\bibitem[Cutkosky(2019)]{cutkosky2019anytime}
Ashok Cutkosky.
\newblock Anytime online-to-batch, optimism and acceleration.
\newblock In \emph{International Conference on Machine Learning}, pages
  1446--1454. PMLR, 2019.

\bibitem[Ene et~al.(2021)Ene, Nguyen, and Vladu]{ene2021adaptive}
Alina Ene, Huy~L Nguyen, and Adrian Vladu.
\newblock Adaptive gradient methods for constrained convex optimization and
  variational inequalities.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, number~8, pages 7314--7321, 2021.

\bibitem[Ene and Nguyen(2022)]{ene2022adaptive}
Alina Ene and Huy Nguyen.
\newblock Adaptive and universal algorithms for variational inequalities with
  optimal convergence.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, number~6, pages 6559--6567, 2022.

\bibitem[Ward et~al.(2020)Ward, Wu, and Bottou]{ward2020adagrad}
Rachel Ward, Xiaoxia Wu, and L\'eon Bottou.
\newblock Ada{G}rad stepsizes: {S}harp convergence over nonconvex landscapes.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 9047--9076, 2020.

\bibitem[Levy et~al.(2021)Levy, Kavis, and Cevher]{levy2021storm+}
Kfir Levy, Ali Kavis, and Volkan Cevher.
\newblock {STORM}+: {F}ully adaptive {SGD} with recursive momentum for
  nonconvex optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20571--20582, 2021.

\bibitem[Kavis et~al.(2022{\natexlab{a}})Kavis, Levy, and
  Cevher]{kavis2022high}
Ali Kavis, {Kfir Y.} Levy, and Volkan Cevher.
\newblock High probability bounds for a class of nonconvex algorithms with
  adagrad stepsize.
\newblock 2022{\natexlab{a}}.
\newblock Publisher Copyright: {\textcopyright} 2022 ICLR 2022 - 10th
  International Conference on Learning Representationss.

\bibitem[Faw et~al.(2022)Faw, Tziotis, Caramanis, Mokhtari, Shakkottai, and
  Ward]{faw2022power}
Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay
  Shakkottai, and Rachel Ward.
\newblock The power of adaptivity in {SGD}: {S}elf-tuning step sizes with
  unbounded gradients and affine variance.
\newblock In \emph{Conference on Learning Theory}, pages 313--355. PMLR, 2022.

\bibitem[Attia and Koren(2023)]{attia2023sgd}
Amit Attia and Tomer Koren.
\newblock {SGD} with {A}da{G}rad stepsizes: full adaptivity with high
  probability to unknown parameters, unbounded gradients and affine variance.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of
  \emph{Proceedings of Machine Learning Research}, pages 1147--1171. PMLR,
  23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/attia23a.html}.

\bibitem[Dubois-Taine et~al.(2022)Dubois-Taine, Vaswani, Babanezhad, Schmidt,
  and Lacoste-Julien]{dubois2022svrg}
Benjamin Dubois-Taine, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and Simon
  Lacoste-Julien.
\newblock {SVRG} meets {A}da{G}rad: {P}ainless variance reduction.
\newblock \emph{Machine Learning}, pages 1--51, 2022.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Nguyen, Ene, and
  Nguyen]{liu2022adaptive}
Zijian Liu, Ta~Duy Nguyen, Alina Ene, and Huy Nguyen.
\newblock Adaptive accelerated (extra-) gradient methods with variance
  reduction.
\newblock In \emph{International Conference on Machine Learning}, pages
  13947--13994. PMLR, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Wang, Zhang, and Cheng]{li2022variance}
Wenjie Li, Zhanyu Wang, Yichen Zhang, and Guang Cheng.
\newblock Variance reduction on general adaptive stochastic mirror descent.
\newblock \emph{Machine Learning}, pages 1--39, 2022.

\bibitem[Wang and Klabjan(2022)]{wang2022divergence}
Ruiqi Wang and Diego Klabjan.
\newblock Divergence results and convergence of a variance reduced version of
  {ADAM}.
\newblock \emph{arXiv preprint arXiv:2210.05607}, 2022.

\bibitem[Kavis et~al.(2022{\natexlab{b}})Kavis, SKOULAKIS, Antonakopoulos,
  Dadi, and Cevher]{kavis2022adaptive}
Ali Kavis, EFSTRATIOS~PANTELEIMON SKOULAKIS, Kimon Antonakopoulos,
  Leello~Tadesse Dadi, and Volkan Cevher.
\newblock Adaptive stochastic variance reduction for non-convex finite-sum
  minimization.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=k98U0cb0Ig}.

\bibitem[Cutkosky and Busa-Fekete(2018)]{cutkosky2018distributed}
Ashok Cutkosky and R{\'o}bert Busa-Fekete.
\newblock Distributed stochastic optimization via adaptive sgd.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Li et~al.(2023)Li, Jadbabaie, and Rakhlin]{li2023convergence}
Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin.
\newblock Convergence of {A}dam under relaxed assumptions.
\newblock \emph{arXiv preprint arXiv:2304.13972}, 2023.

\bibitem[Xie et~al.(2022)Xie, Jin, Zhou, Cheng, and Meng]{xie2022adaptive}
Binghui Xie, Chenhan Jin, Kaiwen Zhou, James Cheng, and Wei Meng.
\newblock An adaptive incremental gradient method with support for
  non-euclidean norms.
\newblock \emph{arXiv preprint arXiv:2205.02273}, 2022.

\bibitem[Shi et~al.(2021)Shi, Sadiev, Loizou, Richt{\'a}rik, and
  Tak{\'a}{\v{c}}]{shi2021ai}
Zheng Shi, Abdurakhmon Sadiev, Nicolas Loizou, Peter Richt{\'a}rik, and Martin
  Tak{\'a}{\v{c}}.
\newblock {AI-SARAH}: {A}daptive and implicit stochastic recursive gradient
  methods.
\newblock \emph{arXiv preprint arXiv:2102.09700}, 2021.

\bibitem[Gower et~al.(2016)Gower, Goldfarb, and
  Richt{\'a}rik]{gower2016stochastic}
Robert Gower, Donald Goldfarb, and Peter Richt{\'a}rik.
\newblock Stochastic block {BFGS}: {S}queezing more curvature out of data.
\newblock In \emph{International Conference on Machine Learning}, pages
  1869--1878. PMLR, 2016.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Hanzely, and
  Richtarik]{gorbunov2020unified}
Eduard Gorbunov, Filip Hanzely, and Peter Richtarik.
\newblock A unified theory of sgd: Variance reduction, sampling, quantization
  and coordinate descent.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 680--690. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/gorbunov20a.html}.

\bibitem[Condat and Richtarik(2022)]{condat2022murana}
Laurent Condat and Peter Richtarik.
\newblock Murana: A generic framework for stochastic variance-reduced
  optimization.
\newblock In Bin Dong, Qianxiao Li, Lei Wang, and Zhi-Qin~John Xu, editors,
  \emph{Proceedings of Mathematical and Scientific Machine Learning}, volume
  190 of \emph{Proceedings of Machine Learning Research}, pages 81--96. PMLR,
  15--17 Aug 2022.
\newblock URL \url{https://proceedings.mlr.press/v190/condat22a.html}.

\bibitem[Diaz-Mejia(2021)]{scMark}
Javier Diaz-Mejia.
\newblock scmark an 'mnist' like benchmark to evaluate and optimize models for
  unifying scrna data, 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5765804}.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms, 2017.

\bibitem[Nesterov(2004)]{Nesterov}
Yurii Nesterov.
\newblock Introductory lectures on convex optimization - a basic course.
\newblock In \emph{Applied Optimization}, 2004.

\bibitem[Defazio(2015)]{DefazioThese}
Aaron Defazio.
\newblock \emph{New optimisation methods for machine learning}.
\newblock PhD thesis, 2015.
\newblock URL \url{https://arxiv.org/abs/1510.02533}.

\end{thebibliography}
