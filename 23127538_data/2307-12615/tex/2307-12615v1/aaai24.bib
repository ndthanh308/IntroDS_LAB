% ### stopped at kavis
@inproceedings{SAGA_article,
  title={SAGA: {A} fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}
@phdthesis{DefazioThese,
  doi = {10.48550/ARXIV.1510.02533},

  url = {https://arxiv.org/abs/1510.02533},

  author = {Defazio, Aaron},

  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {New optimisation methods for machine learning},

  publisher = {arXiv},

  year = {2015},

  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{defazio2014finito,
  title={Finito: {A} faster, permutable incremental gradient method for big data problems},
  author={Defazio, Aaron and Domke, Justin and others},
  booktitle={International Conference on Machine Learning},
  pages={1125--1133},
  year={2014},
  organization={PMLR}
}

@inproceedings{allen2016improved,
  title={Improved {SVRG} for non-strongly-convex or sum-of-non-convex objectives},
  author={Allen-Zhu, Zeyuan and Yuan, Yang},
  booktitle={International conference on machine learning},
  pages={1080--1089},
  year={2016},
  organization={PMLR}
}




@inproceedings{nguyen2017sarah,
  title={{SARAH}: {A} novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}


@inproceedings{mairal2013optimization,
  title={Optimization with first-order surrogate functions},
  author={Mairal, Julien},
  booktitle={International Conference on Machine Learning},
  pages={783--791},
  year={2013},
  organization={PMLR}
}


@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}


@article{Adagrad_article,
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of Machine Learning Research},
  title = {Adaptive subgradient methods for online learning and stochastic optimization},
  volume = 12,
  year = 2011
}


@inproceedings{Nesterov,
  title={Introductory lectures on convex optimization - A basic course},
  author={Yurii Nesterov},
  booktitle={Applied Optimization},
  year={2004}
}





##ici
@misc{MiG,
title={A {S}imple {S}tochastic {V}ariance {R}educed {A}lgorithm with {F}ast {C}onvergence {R}ates},
author={Kaiwen Zhou and Fanhua Shang and James Cheng},
year={2018},
eprint={1806.11027},
archivePrefix={arXiv},
primaryClass={cs.LG}
}
@misc{Varag,
title={A unified variance-reduced accelerated gradient method for convex optimization},
author={Guanghui Lan and Zhize Li and Yi Zhou},
year={2019},
eprint={1905.12412},
archivePrefix={arXiv},
primaryClass={math.OC}
}
@article{song2020variance,
  title={Variance reduction via accelerated dual averaging for finite-sum optimization},
  author={Song, Chaobing and Jiang, Yong and Ma, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={833--844},
  year={2020}
}
@inproceedings{
rmsprop,
title={{RMS}prop converges with proper hyper-parameter},
author={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=3UDSdyIcBDA}
}
@article{LSVRG_smooth,
author  = {Xun Qian and Zheng Qu and Peter Richtárik},
title   = {L-SVRG and L-Katyusha with Arbitrary Sampling},
journal = {Journal of Machine Learning Research},
year    = {2021},
volume  = {22},
number  = {112},
pages   = {1--47},
url     = {http://jmlr.org/papers/v22/20-156.html}
}
@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin-Louis},
  journal={Comptes rendus de l'Acad\'{e}mie des sciences},
  volume={25},
  number={1847},
  pages={536--538},
  year={1847}
}
@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
@article{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence rate for finite training sets},
  author={Le Roux, Nicolas and Schmidt, Mark and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1},
  pages={83--112},
  year={2017},
  publisher={Springer}
}
@article{blatt2007convergent,
  title={A convergent incremental gradient method with a constant step size},
  author={Blatt, Doron and Hero, Alfred O. and Gauchman, Hillel},
  journal={SIAM Journal on Optimization},
  volume={18},
  number={1},
  pages={29--51},
  year={2007},
  publisher={SIAM}
}
@article{hofmann2015variance,
  title={Variance reduced stochastic gradient descent with neighbors},
  author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}
@article{gower2021stochastic,
  title={Stochastic quasi-gradient methods: {V}ariance reduction via {J}acobian sketching},
  author={Gower, Robert M and Richt{\'a}rik, Peter and Bach, Francis},
  journal={Mathematical Programming},
  volume={188},
  pages={135--192},
  year={2021},
  publisher={Springer}
}
@article{konevcny2017semi,
  title={Semi-stochastic gradient descent methods},
  author={Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter},
  journal={Frontiers in Applied Mathematics and Statistics},
  volume={3},
  pages={9},
  year={2017},
  publisher={Frontiers Media SA}
}
@inproceedings{li2020convergence,
  title={On the convergence of {SARAH} and beyond},
  author={Li, Bingcong and Ma, Meng and Giannakis, Georgios B},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={223--233},
  year={2020},
  organization={PMLR}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}
@article{lin2014accelerated,
  title={An accelerated proximal coordinate gradient method},
  author={Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}
@inproceedings{shalev2014accelerated,
  title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={64--72},
  year={2014},
  organization={PMLR}
}
@inproceedings{reddi2016stochastic,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={International conference on machine learning},
  pages={314--323},
  year={2016},
  organization={PMLR}
}
@article{lin2015universal,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{liu2022kill,
  title={Kill a bird with two stones: {C}losing the convergence gaps in non-strongly convex optimization by directly accelerated {SVRG} with double compensation and snapshots},
  author={Liu, Yuanyuan and Shang, Fanhua and An, Weixin and Liu, Hongying and Lin, Zhouchen},
  booktitle={International Conference on Machine Learning},
  pages={14008--14035},
  year={2022},
  organization={PMLR}
}
@article{woodworth2016tight,
  title={Tight complexity bounds for optimizing composite objectives},
  author={Woodworth, Blake E and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@article{lan2018optimal,
  title={An optimal randomized incremental gradient method},
  author={Lan, Guanghui and Zhou, Yi},
  journal={Mathematical programming},
  volume={171},
  number={1},
  pages={167--215},
  year={2018},
  publisher={Springer}
}
@article{lan2019unified,
  title={A unified variance-reduced accelerated gradient method for convex optimization},
  author={Lan, Guanghui and Li, Zhize and Zhou, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{joulani2020simpler,
  title={A simpler approach to accelerated optimization: {I}terative averaging meets optimism},
  author={Joulani, Pooria and Raj, Anant and Gyorgy, Andras and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Machine Learning},
  pages={4984--4993},
  year={2020},
  organization={PMLR}
}
@article{li2021anita,
  title={{ANITA: A}n optimal loopless accelerated variance-reduced gradient method},
  author={Li, Zhize},
  journal={arXiv preprint arXiv:2103.11333},
  year={2021}
}
%% Did not find something else than Arxiv.

@inproceedings{allen2017natasha,
  title={Natasha: {F}aster non-convex stochastic optimization via strongly non-convex parameter},
  author={Allen-Zhu, Zeyuan},
  booktitle={International Conference on Machine Learning},
  pages={89--97},
  year={2017},
  organization={PMLR}
}
@article{fang2018spider,
  title={{SPIDER}: {N}ear-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{allen2017katyusha,
  title={Katyusha: {T}he first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}

@misc{ruder2017overview,
      title={An overview of gradient descent optimization algorithms},
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%% No other than Arxiv.

@inproceedings{latorre2020lipschitz,
title={Lipschitz constant estimation of Neural Networks via sparse polynomial optimization},
author={Fabian Latorre and Paul Rolland and Volkan Cevher},
booktitle={International Conference on Learning Representations},
year={2020}}
@article{armijo1966minimization,
  title={Minimization of functions having {L}ipschitz continuous first partial derivatives},
  author={Armijo, Larry},
  journal={Pacific Journal of mathematics},
  volume={16},
  number={1},
  pages={1--3},
  year={1966},
  publisher={Mathematical Sciences Publishers}
}

@article{nesterov2015universal,
  title={Universal gradient methods for convex optimization problems},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={152},
  number={1},
  pages={381--404},
  year={2015},
  publisher={Springer}
}
@article{lan2015bundle,
  title={Bundle-level type methods uniformly optimal for smooth and nonsmooth convex optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={149},
  number={1-2},
  pages={1--45},
  year={2015},
  publisher={Springer}
}
@article{barzilai1988two,
  title={Two-point step size gradient methods},
  author={Barzilai, Jonathan and Borwein, Jonathan M},
  journal={IMA journal of numerical analysis},
  volume={8},
  number={1},
  pages={141--148},
  year={1988},
  publisher={Oxford University Press}
}
@article{vrahatis2000class,
  title={A class of gradient unconstrained minimization algorithms with adaptive stepsize},
  author={Vrahatis, Michael N and Androulakis, George S and Lambrinos, John N and Magoulas, George D},
  journal={Journal of Computational and Applied Mathematics},
  volume={114},
  number={2},
  pages={367--386},
  year={2000},
  publisher={Elsevier}
}
@inproceedings{malitsky2020adaptive,
  title={Adaptive gradient descent without descent},
  author={Malitsky, Yura and Mishchenko, Konstantin},
  booktitle={Proceedings of the 37th International Conference on Machine Learning},
  pages={6702--6712},
  year={2020}
}
@article{vaswani2019painless,
  title={Painless stochastic gradient: {I}nterpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{dvinskikh2019adaptive,
  title={Adaptive gradient descent for convex and non-convex stochastic optimization},
  author={Dvinskikh, Darina and Ogaltsov, Aleksandr and Gasnikov, Alexander and Dvurechensky, Pavel and Tyurin, Alexander and Spokoiny, Vladimir},
  journal={arXiv preprint arXiv:1911.08380},
  year={2019}
}
@article{tan2016barzilai,
  title={Barzilai-{B}orwein step size for stochastic gradient descent},
  author={Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@article{liu2019class,
  title={A class of stochastic variance reduced methods with an adaptive stepsize},
  author={Liu, Yan and Han, Congying and Guo, Tiande},
  journal={URL http://www. optimization-online. org/DB\_FILE/2019/04/7170. pdf},
  year={2019}
}
@article{li2019adaptive,
  title={Adaptive step sizes in variance reduction via regularization},
  author={Li, Bingcong and Giannakis, Georgios B},
  journal={arXiv preprint arXiv:1910.06532},
  year={2019}
}
% Did not find other journal, keep arxiv
@inproceedings{li2020almost,
  title={Almost tune-free variance reduction},
  author={Li, Bingcong and Wang, Lingda and Giannakis, Georgios B},
  booktitle={International conference on machine learning},
  pages={5969--5978},
  year={2020},
  organization={PMLR}
}
@article{yang2021accelerating,
  title={Accelerating mini-batch {SARAH} by step size rules},
  author={Yang, Zhuang and Chen, Zengping and Wang, Cheng},
  journal={Information Sciences},
  volume={558},
  pages={157--173},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{mcmahan2010adaptive,
  title={Adaptive bound optimization for online convex optimization},
  author={McMahan, H Brendan and Streeter, Matthew},
  pages={244},
  year={2010},
  booktitle={Proceedings of the 23rd Conference on Learning Theory (COLT)}
}
@inproceedings{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}
@inproceedings{levy2017online,
  title={Online to offline conversions, universality and adaptive minibatch sizes},
  author={Levy, Kfir},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1613--1622},
  year={2017}
}
@inproceedings{levy2018online,
  title={Online adaptive methods, universality and acceleration},
  author={Levy, Yehuda Kfir and Yurtsever, Alp and Cevher, Volkan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6500--6509},
  year={2018}
}
@article{kavis2019unixgrad,
  title={Unix{G}rad: {A} universal, adaptive algorithm with optimal guarantees for constrained optimization},
  author={Kavis, Ali and Levy, Kfir Y and Bach, Francis and Cevher, Volkan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{wu2018wngrad,
  title={{WNG}rad: {L}earn the learning rate in gradient descent},
  author={Wu, Xiaoxia and Ward, Rachel and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1803.02865},
  year={2018}
}
@inproceedings{cutkosky2019anytime,
  title={Anytime online-to-batch, optimism and acceleration},
  author={Cutkosky, Ashok},
  booktitle={International Conference on Machine Learning},
  pages={1446--1454},
  year={2019},
  organization={PMLR}
}
@inproceedings{ene2021adaptive,
  title={Adaptive gradient methods for constrained convex optimization and variational inequalities},
  author={Ene, Alina and Nguyen, Huy L and Vladu, Adrian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={8},
  pages={7314--7321},
  year={2021}
}
@inproceedings{ene2022adaptive,
  title={Adaptive and universal algorithms for variational inequalities with optimal convergence},
  author={Ene, Alina and Nguyen, Huy},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={6},
  pages={6559--6567},
  year={2022}
}
@article{ward2020adagrad,
  title={Ada{G}rad stepsizes: {S}harp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, L\'eon},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={9047--9076},
  year={2020},
  publisher={JMLRORG}
}
@article{levy2021storm+,
  title={{STORM}+: {F}ully adaptive {SGD} with recursive momentum for nonconvex optimization},
  author={Levy, Kfir and Kavis, Ali and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20571--20582},
  year={2021}
}
@conference{kavis2022high,
title = "High probability bounds for a class of nonconvex algorithms with AdaGrad stepsize",
abstract = "In this paper, we propose a new, simplified high probability analysis of AdaGrad for smooth, non-convex problems. More specifically, we focus on a particular accelerated gradient (AGD) template (Lan, 2020), through which we recover the original AdaGrad and its variant with averaging, and prove a convergence rate of O(1/√T) with high probability without the knowledge of smoothness and variance. We use a particular version of Freedman's concentration bound for martingale difference sequences (Kakade & Tewari, 2008) which enables us to achieve the best-known dependence of log(1/δ) on the probability margin δ. We present our analysis in a modular way and obtain a complementary O(1/T) convergence rate in the deterministic setting. To the best of our knowledge, this is the first high probability result for AdaGrad with a truly adaptive scheme, i.e., completely oblivious to the knowledge of smoothness and uniform variance bound, which simultaneously has best-known dependence of log(1/δ). We further prove noise adaptation property of AdaGrad under additional noise assumptions.",
author = "Ali Kavis and Levy, {Kfir Y.} and Volkan Cevher",
note = "Publisher Copyright: {\textcopyright} 2022 ICLR 2022 - 10th International Conference on Learning Representationss.",
year = "2022",
}

@inproceedings{faw2022power,
  title={The power of adaptivity in {SGD}: {S}elf-tuning step sizes with unbounded gradients and affine variance},
  author={Faw, Matthew and Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan and Shakkottai, Sanjay and Ward, Rachel},
  booktitle={Conference on Learning Theory},
  pages={313--355},
  year={2022},
  organization={PMLR}
}

@InProceedings{attia2023sgd,
  title = 	 {{SGD} with {A}da{G}rad stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance},
  author =       {Attia, Amit and Koren, Tomer},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {1147--1171},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/attia23a/attia23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/attia23a.html},
  abstract = 	 {We study Stochastic Gradient Descent with AdaGrad stepsizes: a popular adaptive (self-tuning) method for first-order stochastic optimization. Despite being well studied, existing analyses of this method suffer from various shortcomings: they either assume some knowledge of the problem parameters, impose strong global Lipschitz conditions, or fail to give bounds that hold with high probability. We provide a comprehensive analysis of this basic method without any of these limitations, in both the convex and non-convex (smooth) cases, that additionally supports a general “affine variance” noise model and provides sharp rates of convergence in both the low-noise and high-noise regimes.}
}

@article{lemarechal1995new,
  title={New variants of bundle methods},
  author={Lemar{\'e}chal, Claude and Nemirovskii, Arkadii and Nesterov, Yurii},
  journal={Mathematical programming},
  volume={69},
  pages={111--147},
  year={1995},
  publisher={Springer}
}
@dataset{scMark,
  author       = {Javier Diaz-Mejia},
  title        = {scMARK an 'MNIST' like benchmark to evaluate and
                   optimize models for unifying scRNA data},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5765804},
  url          = {https://doi.org/10.5281/zenodo.5765804}
}

@article{dubois2022svrg,
  title={{SVRG} meets {A}da{G}rad: {P}ainless variance reduction},
  author={Dubois-Taine, Benjamin and Vaswani, Sharan and Babanezhad, Reza and Schmidt, Mark and Lacoste-Julien, Simon},
  journal={Machine Learning},
  pages={1--51},
  year={2022},
  publisher={Springer}
}

@inproceedings{liu2022adaptive,
  title={Adaptive accelerated (extra-) gradient methods with variance reduction},
  author={Liu, Zijian and Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy},
  booktitle={International Conference on Machine Learning},
  pages={13947--13994},
  year={2022},
  organization={PMLR}
}

@article{li2022variance,
  title={Variance reduction on general adaptive stochastic mirror descent},
  author={Li, Wenjie and Wang, Zhanyu and Zhang, Yichen and Cheng, Guang},
  journal={Machine Learning},
  pages={1--39},
  year={2022},
  publisher={Springer}
}

@article{wang2022divergence,
  title={Divergence results and convergence of a variance reduced Version of {ADAM}},
  author={Wang, Ruiqi and Klabjan, Diego},
  journal={arXiv preprint arXiv:2210.05607},
  year={2022}
}
%% No other than Arxiv.

@inproceedings{kavis2022adaptive,
title={Adaptive stochastic variance reduction for non-convex finite-sum minimization},
author={Ali Kavis and EFSTRATIOS PANTELEIMON SKOULAKIS and Kimon Antonakopoulos and Leello Tadesse Dadi and Volkan Cevher},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=k98U0cb0Ig}
}
@article{cutkosky2018distributed,
  title={Distributed stochastic optimization via adaptive SGD},
  author={Cutkosky, Ashok and Busa-Fekete, R{\'o}bert},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{li2023convergence,
  title={Convergence of {A}dam under relaxed assumptions},
  author={Li, Haochuan and Jadbabaie, Ali and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2304.13972},
  year={2023}
}
% Did not find other than Arxiv.
@article{xie2022adaptive,
  title={An adaptive incremental gradient method with support for non-euclidean norms},
  author={Xie, Binghui and Jin, Chenhan and Zhou, Kaiwen and Cheng, James and Meng, Wei},
  journal={arXiv preprint arXiv:2205.02273},
  year={2022}
}

@article{shi2021ai,
  title={{AI-SARAH}: {A}daptive and implicit stochastic recursive gradient methods},
  author={Shi, Zheng and Sadiev, Abdurakhmon and Loizou, Nicolas and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:2102.09700},
  year={2021}
}
%% No other than Arxiv

@inproceedings{gower2016stochastic,
  title={Stochastic block {BFGS}: {S}queezing more curvature out of data},
  author={Gower, Robert and Goldfarb, Donald and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={1869--1878},
  year={2016},
  organization={PMLR}
}

@InProceedings{gorbunov2020unified,
  title = 	 {A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent},
  author =       {Gorbunov, Eduard and Hanzely, Filip and Richtarik, Peter},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {680--690},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/gorbunov20a/gorbunov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/gorbunov20a.html},
  abstract = 	 {In this paper we introduce a unified analysis of a large family of variants of proximal stochastic gradient descent (SGD) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling.  As a by-product, we obtain the first unified theory of SGD and randomized coordinate descent (RCD) methods,  the first unified theory of variance reduced and non-variance-reduced SGD methods, and the first unified theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be  used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop five new variants of SGD, and through numerical experiments demonstrate some of their properties.  }
}

@InProceedings{condat2022murana,
  title = 	 {MURANA: A generic framework for stochastic variance-reduced optimization},
  author =       {Condat, Laurent and Richtarik, Peter},
  booktitle = 	 {Proceedings of Mathematical and Scientific Machine Learning},
  pages = 	 {81--96},
  year = 	 {2022},
  editor = 	 {Dong, Bin and Li, Qianxiao and Wang, Lei and Xu, Zhi-Qin John},
  volume = 	 {190},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--17 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v190/condat22a/condat22a.pdf},
  url = 	 {https://proceedings.mlr.press/v190/condat22a.html},
  abstract = 	 {We propose a generic variance-reduced algorithm, which we call MUltiple RANdomized Algorithm (MURANA), for minimizing a sum of several smooth functions plus a regularizer, in a sequential or distributed manner. Our method is formulated with general stochastic operators, which allow us to model various strategies for reducing the computational complexity. For example, MURANA supports sparse activation of the gradients, and also reduction of the communication load via compression of the update vectors. This versatility allows MURANA to cover many existing randomization mechanisms within a unified framework, which also makes it possible to design new methods as special cases.}
}

