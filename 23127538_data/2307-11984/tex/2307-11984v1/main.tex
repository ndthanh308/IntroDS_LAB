\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\input{def}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage{bbding}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{booktabs} % fors professional tables
\newcommand{\topline}{\toprule [0.1em]}
\newcommand{\midline}{\midrule [0.05em]}
\newcommand{\bottomline}{\bottomrule [0.1em]}
\usepackage{colortbl} 
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{bigstrut}
\usepackage{dsfont}
\usepackage{pifont}
\usepackage{subcaption}


\def\todo{\textcolor{green}{TODO}}
\usepackage{array}   
\definecolor{mygray}{rgb}{0.9,0.9,0.9}
\newcommand\filledcirc[1]{{\color{#1}\bullet}\mathllap{\color{#1}\circ}}
\newcommand{\startsq}{%
  \setlength{\fboxsep}{0pt}%
  \setlength{\fboxrule}{1.2pt}%
  \raisebox{0.6pt}[0pt][0pt]{\fcolorbox{blue}{white}{\color{white}{o}}}
}
\usepackage{caption} 
\captionsetup[table]{skip=5pt}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).


\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Learning Vision-and-Language Navigation 
                from YouTube Videos}

\author{
    Kunyang Lin\textsuperscript{\rm 1 2}\thanks{Equal contribution. Email: \{imkunyanglin, phchencs\}@gmail.com} ~~ 
    Peihao Chen\textsuperscript{\rm 1}\footnotemark[1] ~~ 
    Diwei Huang\textsuperscript{\rm 1} ~ 
    Thomas H. Li\textsuperscript{\rm 6} ~
    Mingkui Tan\textsuperscript{\rm 1 \rm 5}\thanks{Corresponding author. Email: mingkuitan@scut.edu.cn} ~
    Chuang Gan\textsuperscript{\rm 3 \rm 4} \\
    \textsuperscript{\scriptsize{\rm 1}}\small{South China University of Technology,}
    \textsuperscript{\scriptsize{\rm 2}}\small{Information Technology R\&D Innovation Center of Peking University,}\\
    \textsuperscript{\scriptsize{\rm 3}}\small{UMass Amherst,}
    \textsuperscript{\rm 4}\small{MIT-IBM Watson AI Lab,}
    \textsuperscript{\rm 5}\small{Key Laboratory of Big Data and Intelligent Robot, Ministry of Education,} \\
    \textsuperscript{\scriptsize{\rm 6}}\small{Peking University Shenzhen Graduate School} 
}


\maketitle
% % Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}

Vision-and-language navigation (VLN) requires an embodied agent to navigate in realistic 3D environments using natural language instructions. 
Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to unseen environments.
There are massive house tour videos on YouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before.
In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it.
To achieve this, we have to tackle the challenges of automatically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos.
To address these, we first leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware generator for generating instructions from unlabeled trajectories. Last, we devise a trajectory judgment pretext task to encourage the agent to mine the layout knowledge. Experimental results show that our method achieves state-of-the-art performance on two popular benchmarks (R2R and REVERIE). Code is available at \small{\url{https://github.com/JeremyLinky/YouTube-VLN}}
\end{abstract}
\vspace{-2em}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
An important goal of embodied artificial intelligence is to develop agents that can interact with humans in natural language to carry out real-world tasks.
Toward this goal, vision-and-language navigation (VLN)~\cite{VLN} is a rudimentary artificial intelligence task, requiring an indoor agent to navigate in unseen environments following natural instructions.
VLN has attracted widespread attention in the fields of computer vision and robotics due to its promising applications such as in-home robots~\cite{HomeRoboticsApplication} and warehouse assistants~\cite{liang2015automated}.

One of the key challenges of VLN is the generalization ability of agents to unseen environments. 
Existing VLN methods attempt to cope with this challenge via self-supervised pre-training on vision-and-language datasets. As shown in Figure~\ref{fig:teaser}~(a), some previous works~\cite{Prompt, Transferable, Improving, AutoVLN} learn the agents on simulated navigation environments and manual-labeled data. The other works~\cite{Generic, Airbert, HOP} seek to construct path-instruction pairs by using web image data, which is shown in Figure~\ref{fig:teaser}~(b). Despite their promising performance, existing agents still suffer from the following limitations. 1) Training on simulated datasets is limited to a restricted number of environments. 2) Constructing a trajectory by simply concatenating web images leads to unreasonable room layouts, which hamper the agent to learn layout reasoning ability. As a result, VLN agents trained on such data are brittle to adapt to unseen environments.


% Figure environment removed


Fortunately, there are massive house tour videos on YouTube, providing real navigation experiences and layout information but are still under-explored. We can be naturally inspired to let an agent learn VLN ability from such videos, thereby addressing the limitations of existing methods. An intuitive way is to model the navigation experiences as path-instruction pairs to train the agent.
Motivated by this, we propose a ``\textbf{Lily}'' agent who \textbf{L}\textit{earns V}\textbf{i}\textit{sion-and-}\textbf{L}\textit{anguage}\textit{ Navigation from }\textbf{Y}\textit{ouTube Videos}. Specifically, we first develop an in-domain pre-training dataset from house tour YouTube videos, namely \textbf{YouTube-VLN}, which comprises VLN-like path-instruction pairs. Our YouTube-VLN dataset has the advantages of diverse environments, real layouts, and native actions\footnote{The execution actions that objectively exist}, reducing the domain gap with VLN datasets, as illustrated in Figure~\ref{fig:teaser} (c). Then, we pre-train the agent using these path-instruction pairs. Benefiting from in-domain pre-training on our proposed dataset, our agent thus generalizes well to unseen environments.



Constructing and utilizing such a dataset, however, is still far from trivial work and remains an open problem due to the following challenges. 1) As the nodes in a trajectory are expected to be diverse and informative, it is hard to determine the locations of trajectory nodes from massive video frames and represent the visual content in a node. 2) Real VLN instructions include various action descriptions, but obtaining corresponding instructions from navigation clips is challenging due to the actions being implicit in videos. Thus it is nontrivial to acquire matching instruction on a trajectory. 3) Layout knowledge from real navigation experience is hard to mine and model, which impedes the agent of learning layout reasoning ability. 

In this paper, we address the above challenges as follows. To conquer challenge 1), we propose an entropy-based trajectory generation method. Specifically, we first envisage that the nodes of a trajectory should contain as many types of rooms as possible to diversify trajectories. Accordingly, we group the frames with the same room types in videos and consider each group as a node in the trajectory. Then, inspired by that low classification entropy image is reliable and contains rich information relevant to a specific class (room type in our case)~\cite{EATA}, the frame with the lowest classification entropy in a group is chosen to represent the visual content in a node.
To tackle challenge 2), we introduce an action-aware instruction generation method. Specifically, we adopt an action inverse model to pseudo-label the action along trajectories and fill them in the instructions via hand-designed rules.
To grapple with challenge 3), we devise a self-supervised pretext task. As we all know, humans often judge whether a navigation trajectory is reasonable based on the layout of the environment. Therefore, it is believed that an agent equipped with layout reasoning ability should be able to make similar judgment. To this end, we propose \textbf{trajectory judgment} pretext task to ask the agent to identify reasonable navigation trajectories, which further equips the model with the ability to reason environment layouts.
%

We empirically show that the diverse entropy-based  trajectory generation method and action-aware instruction generator allow us to harvest high-quality path-instruction pairs from YouTube house tour videos, resulting in the YouTube-VLN dataset. By integrating the self-supervised trajectory judgment task in pre-training a VLN agent, our Lily agent presents state-of-the-art performance on two mature and solid benchmarks (R2R~\cite{VLN}, REVERIE~\cite{REVERIE}). The proposed Lily agent reaches the first place on the R2R leaderboard in terms of success rate and outperforms the SOTA method under discriminative setting and generative setting by 2\% and 3\% \textit{w.r.t.} success rate, respectively.

Our main contributions are as follows:

$\bullet$ We unleash the huge potential of house tour videos for VLN. By leveraging these videos, we introduce a large-scale dataset containing real navigation path-instruction pairs for promoting VLN pre-training and a self-supervised pretext task for the learning of layout reasoning.

$\bullet$ Our diverse trajectory generation method, together with the action-aware instruction generator, creates informative and diverse trajectory nodes and produces matching instructions, both of which make the path-instruction pairs authentic and of high quality for training a VLN agent.

$\bullet$ The proposed trajectory judgment pretext task allows the model to build up an awareness of learning and reasoning the layout knowledge, which is crucial in the VLN task of indoor environments. We also empirically substantiate that the agent indeed learns the layout learning ability. 
    


\section{Related Work}

\subsection{Vision-and-Language Navigation}
Vision-and-Language Navigation (VLN)~\cite{VLN} is a challenging task and has received continuous and intense attention from the academic community in recent years~\cite{Weakly-Supervised, Room-and-Object, Cross-Modal, Contrastive-PI, One-Step, CLEAR, DUET, Outdoor, ding2022embodied}. Early methods attempt to learn the agent from sequence-to-sequence models~\cite{VLN, Speaker-Follower, EnvDrop}. However, these methods can not model the cross-modal relation between language and visual observation well. To address this issue, transformer~\cite{Transformer} architecture is adopted to the agents followed by vision-and-language pre-training~\cite{SMNA, RCMS, AuxRN, VLNBert, Airbert, HOP, HAMT, SOAT, SEA}. PREVALENT~\cite{image-text-action} pre-trains transformer-based agent via masked language modeling and action prediction tasks. Inspired by BERT~\cite{BERT}, several works propose to use different variants of BERT for VLN pre-training. VLN-BERT~\cite{VLNBert} utilizes image-text data~\cite{ViLBERT} to perform path-instruction matching pretext task. Airbert~\cite{Airbert} proposes the shuffle loss to improve the ability of the model to learn the order of image-caption pairs. Recently, HOP~\cite{HOP} introduces the history and order-aware pretext tasks. However, these existing pretext tasks do not consider the learning of environment layout reasoning ability and bring limited performance for VLN tasks. In this work, we propose a trajectory judgment task to teach the agent to distinguish reasonable navigation trajectories. By proficiently accomplishing this task, the agent can acquire the ability to reason about environment layout, which enhances its generalization capability to unseen environments.

\subsection{Datasets for VLN}
The major difficulty of generalizing a VLN agent to unseen environments is the scarcity of VLN training data. Well-labeled VLN datasets built from the simulators~\cite{MP3d, HMP3D, Gibson} allow the agent to obtain a promising performance, such as R2R~\cite{VLN}, R4R~\cite{R4R}, RxR~\cite{RxR} and SOON~\cite{SOON}. While the data built from simulators are laborious, Wang \textit{et al.}~\cite{Less-is-More} and ProbES~\cite{Prompt} enrich the navigation instructions via self-exploration in the simulation environments. Some other endeavors~\cite{EnvDrop, Envedit, Speaker-Follower, APS, Pathdreamer, SIVLN} seek to augment the data from existing datasets. AutoVLN~\cite{AutoVLN} and Kamath \textit{et al.}~\cite{ANewPath} enlarges the VLN data from simulations with a larger number of environments. However, these datasets are limited by the number of scenes in simulators. To ease this problem, VLN-BERT~\cite{VLNBert} leverages the abundant web image-captions pairs as VLN pre-training data. Airbert~\cite{Airbert} further exploits indoor house images and captions from the web to construct path-instruction pairs for VLN pre-training. However, the trajectories constructed by simply splicing pictures may be confusing and ambiguous. In our work, we address these problems by proposing a large-scale in-domain VLN-like pre-training dataset, providing the agent with diverse visual environments and reasonable layouts.



\section{Building VLN Dataset from YouTube Videos}
\label{sec:dataset}

Our first step is to develop a large-scale VLN-like dataset that comprises reasonable path-instruction pairs from house tour videos on YouTube, termed YouTube-VLN. YouTube-VLN serves as a cornerstone for facilitating the acquisition of VLN capabilities for Lily, featuring diverse environments, real layouts and native actions. To achieve this goal, we present an entropy-based trajectory generation technique~(Section~\ref{sec:path_gen}) and an action-aware generator~(Section~\ref{sec:ins_gen}) to tackle the arduous tasks of trajectory and instruction generation, respectively.


\subsection{Diverse Trajectory Generation.}
\label{sec:path_gen}
We seek to construct discrete navigation trajectories from YouTube videos. Similar to discrete navigation datasets (\eg R2R~\cite{VLN}), each trajectory comprises $K$ navigation nodes, representing different locations of a navigation path. 
This entails addressing two major challenges:
1) how to determine the locations composing a trajectory to make the trajectory more diverse and
2) how to represent the visual content at each node location.
To tackle these challenges, we first collect large-scale consecutive indoor frames from YouTube videos. Then, we group the adjacent frames according to their room types and consider each group as a node. Last, we present an entropy-based technique to select the most informative frames in a group for representing the visual content in a node.

% Figure environment removed

\textbf{Collecting Navigation Data from YouTube.}
\label{sec:Collecting_In-door_Images}
Real estate agents typically tour a house in each video. To satisfy the visual diversity and dataset scale, we create the YouTube-VLN dataset from 4078 videos collected from various uploaders, with a total duration of 433 hours. In contrast to prior work~\cite{YTb}, which relied on a limited set of videos from a single uploader, our dataset features greater diversity and volume. We also employ sparse sampling and off-the-shelf image classifiers~\cite{Mask_RCNN, CLIP} to pre-process the videos, filtering out redundant or noisy frames (those featuring people or outdoor scenes), resulting in a final set of 587k indoor frames suitable for constructing trajectories.

\textbf{Determining the Locations of Trajectory Nodes.}
\label{sec:Grouping-Nodes}
A real robot often needs to go through different locations for navigating to a goal. To mimic the real navigation process, we expect that our constructed trajectories also contain diverse visual content within the limited navigation nodes. To achieve this, we first utilize the powerful large model CLIP~\cite{CLIP} to recognize the room type of each indoor image. Then, we gather temporally adjacent frames with the same room type as a group and consider this group as one of the navigation nodes. In this sense, the navigation nodes are diversely spread in different rooms and the constructed trajectories are able to mimic the real navigation process. We also call this kind of node a \textit{room node}. In practice, to increase the visual diversity of trajectories, we also randomly insert \textit{transition nodes} that are composed of video frames captured during the transition from one room to another one, between two adjacent room nodes.


\textbf{Representing Visual Content in a Node.}
\label{sec:landmarks}
A node consists of a group of images and sometimes the number of images may exceed 100 as the photographer could stay in the same room for a long time. Hence, we have to select the most informative images for representing node features. Inspired by EATA~\cite{EATA}, an image with lower classification entropy is more reliable, containing more information relevant to a specific class (room type in our case). We thus propose to select an image with the lowest classification entropy to represent the current view of a node. In order to mimic the panoramic visual context, we then merge $M$ adjacent consecutive images of the current view. It is worth noting that our node features better represent a panoramic view compared with Airbert~\cite{Airbert} because we merge adjacent frames that belong to the same place as the current view.

Ultimately, we randomly chose $K$ continuous nodes to construct a trajectory. An example of the constructed trajectory is shown in Figure~\ref{fig:merge_strategy}.


\subsection{Action-Aware Instruction Generation}
\label{sec:ins_gen}


In addition to constructing navigation trajectories, one more important step for building a VLN dataset is to create the corresponding instructions without manual annotation.
The main challenge for this step is how to correctly describe visual content and actions along navigation paths.
To conquer this challenge, we first generate instruction templates with verb and noun phrase blanks. Then, we describe each node in trajectories using the CLIP~\cite{CLIP} model and infer the native actions using an action inverse model~\cite{YTb}. To generate the final instructions, we fill the instruction templates with these visual descriptions and actions.

Specifically, we first generate templates with verb and noun blanks from instructions in the R2R dataset following Airbert~\cite{Airbert}.
For noun blanks, we fill them with visual content descriptions about each node. We select the frame with the lowest classification entropy (as described in Sec.~\ref{sec:landmarks}) and use the CLIP model to infer the objects it contains, together with the room type to populate a noun blank.
For verb blanks, the existing instruction generation method~\cite{Airbert} is unable to fill them with the correct action words because it cannot figure out the actions taken for navigating from one image to another. This makes the agent confused when it observes similar viewpoints transition but is given different action descriptions.
To tackle this problem, we propose an action-aware strategy to fill instruction templates with native actions instead of random inconsistent actions. To be specific, we follow~\cite{YTb} to train an action inverse model, which has 96\% prediction accuracy for predicting native actions, to pseudo-label the trajectory with action labels from one location node to another. The predicted actions are then converted into actionable verbs,~\ie ``go forward'', ``turn left'' and ``turn right''. For each noun blank that has been filled with the description of one node, we find its closest verb phrase blank and fill it with the pseudo-labeled action which is executed to reach the next node. This eventually enables us to create action-aware instructions.

 % Figure environment removed

\section{Learning VLN from YouTube Videos}
\label{sec:approach}

Given the VLN-like and reasonable path-instruction pairs generated from YouTube videos, we then describe how to learn the Lily agent from these data. As shown in Figure~\ref{fig:scheme}, our VLN model consists of two components: a vision-and-language backbone (\ie Multi-Layer Transformer) that models the relationship between trajectories and instructions and a decision-making module that predicts the next action or a matching score for a  path-instruction pair. The vision-and-language backbone can be any type of cross-modal network. We chose ViLBERT~\cite{ViLBERT} for a fair comparison with Airbert~\cite{Airbert}. As a common practice, pretext tasks are utilized for pre-training the backbone. We next describe how to pre-train the backbone on our Youtube-VLN dataset using the proposed trajectory judgment pretext task. 

\subsection{Model Architecture}
\label{sec:Architecture}
We follow Airbert~\cite{Airbert} to leverage a ViLBERT-like~\cite{ViLBERT} architecture as the model backbone. The model encodes the sequential visual region features and the text token via two separate transformers respectively. More formally, the  path-instruction pair consists of $K$ nodes $\left\{V_k\right\}_{k=1}^K$ and $L$ text tokens $\left\{w_l\right\}_{l=1}^L$. Each node $V_k$ is composed of $R_k$ visual region features $\left\{v_i^k\right\}_{i=1}^{R_k}$. In this way, we represent the visual and text inputs respectively as follows:
% \vspace{-2mm}
\begin{small}
\begin{equation}
X_V=\left[[\texttt{IMG}], v_1^1, \ldots, v_{R_1}^1, \ldots,[\texttt{IMG}], v_1^K, \ldots, v_{R_K}^K\right],
\label{eq:xv}
\end{equation}
\end{small}
\vspace{-4mm}
\begin{small}
\begin{equation}
X_W=\left[[\texttt{CLS}], w_1, \ldots, w_l, \ldots, w_L,[\texttt {SEP}]\right],
\label{eq:xw}
\end{equation}
\end{small}
% \vspace{0.5mm}

\noindent where [\texttt{IMG}], [\texttt{CLS}] and [\texttt{SEP}] are special tokens.
% The visual and text inputs are then sent into two separate transformer encoders respectively and
The encoded visual and text tokens finally interact via a cross-modal transformer encoder. We represent the whole model architecture as ``Multi-Layer Transformer'' in Figure~\ref{fig:scheme}.
 
\subsection{Learning Layout from Trajectory Judgment}
\label{sec:Pre-training}

Given the aforementioned model architecture, we propose to train a VLN agent with a trajectory judgment (TJ) task, enabling it to reason about layouts. Herein, we elaborate on the proposed trajectory judgment task.


\textbf{Formulation.} 
The trajectory judgment task aims to judge the reasonableness of trajectories. We consider the trajectories generated in the way described in Section~\ref{sec:path_gen} as positive (reasonable) samples and the shuffled trajectories as negative (unreasonable) ones. To finish this task, the agent is required to reason about the visual information and identify the room types, then infer whether the trajectory matches the real environment layout distribution. 
% It is imperative that the agent correctly identifies the true navigation trajectory, since the agent can then implicitly learn about what the real environment layout is like. 
Specifically, we first calculate the dot product of the output features of [\texttt{IMG}] and [\texttt{CLS}] tokens. Then, we feed this vector feature to a linear layer to predict the probability that indicates whether the trajectory is reasonable.
The model aims to minimize the binary cross-entropy loss:

\vspace{-4mm}
\begin{small}
\begin{equation}
% L_{T J}=\frac{1}{N} \sum_{n=1}^N-w\left[y_n \cdot \log \left(\operatorname{Sigmoid}\left(\mathbf{P}_\theta^n\right)\right)\right],
L=-\frac{1}{N} \sum_{n=1}^N\left[w \cdot y_n \log \left(p_n\right) + (1-y_n) \log \left({1-p_n}\right)\right],
\label{eq:tj_loss}
\end{equation}
\end{small}
\vspace{-4mm}

\noindent where $y_n = 1$ if the $n^{th}$ trajectory is reasonable, otherwise $y_n = 0$. $p_n$ represents the probability that the $n^{th}$ trajectory is predicted as reasonable. $N$ is the number of trajectories in a batch.
$w$ is a factor to mitigate the imbalance of positive and negative samples, which equals the ratio of the number of negative samples to the number of positive samples.


% \vspace{2mm}
\textbf{Sample Generation.}
 We propose to shuffle the positive sample to generate the negative samples: 1) shuffle only the transition nodes; 2) shuffle all the nodes; 3) keep the order of the room nodes, and randomly insert nodes from other videos. In this way, we create rich and hard negative samples, which increases the task difficulty, helping the agent understand the real layout in a more complex manner.

\textbf{Combining with Existing Pre-Training Tasks.}
As depicted in Figure~\ref{fig:scheme}, we follow Airbert~\cite{Airbert} to pre-train the model backbone using our proposed trajectory judgment task, additionally combining with three other existing pretext tasks, namely masked language modeling (MLM), masked vision modeling (MVM) and path ranking (PR) on YouTube-VLN dataset. For MLM, we randomly mask out the words in instruction and the goal is to recover the masked words. Similar to MLM, MVM is designed to predict masked image regions. PR is a ranking task, which aims to decide the most matching path-instruction pair among a few pairs. 


\begin{table*}[t!]
{
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{clccclclccccclccccc}
\topline
\multirow{2}{*}{\#} &  & \multicolumn{3}{c}{Dataset}  &  & Pre-training Task &  & \multicolumn{5}{c}{Val Seen} &  & \multicolumn{5}{c}{Val Unseen}\\ \cmidrule{3-5} \cmidrule{7-7} \cmidrule{9-13} \cmidrule{15-19} 
 &  & Source  & \begin{tabular}[c]{@{}c@{}}Reasonable \\Navigation Path\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pseudo-labeled \\ Action\end{tabular} &  & \begin{tabular}[c]{@{}c@{}}Trajectory\\ Judgment\end{tabular} 
                                                              &   & TL           & NE↓          & OSR↑          & SR↑           & SPL↑          &  & TL          & NE↓          & OSR↑          & SR↑           & SPL↑          \\ 
\midline
1    &  & Airbnb Images    &\XSolidBrush &\XSolidBrush &  &\XSolidBrush &   & 10.21 & 3.41 & 79.02 & 74.12 & 0.70 &  & 9.63 & 3.95 & 70.97 & 62.84 & 0.58 \\ 
\midline
2    &  & YouTube Videos &\XSolidBrush &\XSolidBrush &  &\XSolidBrush &   & 10.12 & 3.40 & 79.90 & 74.31 & 0.70 &  & 9.81 & 3.72 & 74.24 & 63.73 & 0.59 \\
3    &  & YouTube Videos & \Checkmark  &\XSolidBrush &  &\XSolidBrush &   & 10.30 & 3.40 & 78.60 & 75.10 & 0.71 &  & 9.60 & 3.70 & 73.50 & 65.00 & 0.61 \\
4    &  & YouTube Videos & \Checkmark  & \Checkmark  &  &\XSolidBrush &   & 10.20 & 3.30 & 79.80 & 75.40 & 0.71 &  & 9.30 & 3.60 & 72.70 & 66.10 & 0.62 \\
5    &  & YouTube Videos & \Checkmark  & \Checkmark &  & \Checkmark   &   & 9.99  & \textbf{3.12} & \textbf{80.88} & \textbf{77.45} & \textbf{0.74} &  & 9.64 & \textbf{3.37} & \textbf{74.93} & \textbf{66.70} & \textbf{0.62} \\ 
\bottomline
\end{tabular}
}
}
\caption{Ablation study on YouTube-VLN dataset and trajectory judgment pretext task for pre-training.}
\label{tab:ablation}
\vspace{-4mm}
\end{table*}


% \vspace{-1.5mm}
\subsection{Adapting Pre-trained Backbone for VLN}
\label{sec:Downstream}
We adapt the pre-trained model to both goal-oriented navigation task and object-oriented navigation task. All the tasks are based on the Matterport3D simulator~\cite{MP3d}. We utilize R2R~\cite{VLN} as the benchmark for the goal-oriented navigation task, which is divided into discriminative setting and generative setting. As for the object-oriented task, we evaluate our model on REVERIE~\cite{REVERIE} in generative setting. 

The discriminative setting formulates VLN as a path-selection problem, requiring the agent to choose the path that best matches the instruction from multiple candidate paths. Under the discriminative setting, we utilize the classifier used in the path ranking pretext task for decision-making and fine-tune the Lily agent on the R2R dataset. 

In the generative setting, the agent needs to predict actions sequentially to reach the goal (R2R) or simultaneously find the object (REVERIE). We adopt DUET~\cite{DUET} as the architecture for fine-tuning, which feeds the cross-modal feature into a feed-forward network for decision-making. We initialize the text transformer encoder and cross-modal transformer encoder of the generative model with our Lily agent. 
Note that our Lily agent can apply to any generative model. More details are available in the supplementary.


\section{Experiments}
\subsection{Experimental Setup}

\textbf{Dataset and Evaluation Metrics.}
We conduct our experiments on two VLN benchmarks,~\ie R2R~\cite{VLN} and REVERIE~\cite{REVERIE}. These two datasets consist of 21,567 path-instruction pairs from 90 scenes in Matterport3D~\cite{MP3d}. REVERIE follows the same train/val/test splits as the R2R, while requiring an agent to select the bounding box 
 of the target object bounding box additionally. Following standard settings~\cite{Airbert}, we adopt five metrics for evaluating R2R, namely success rate \textbf{(SR)}, oracle success rate \textbf{(OSR)}, success rate weighted by the ratio between the length of the shortest path and the predicted path \textbf{(SPL)}, trajectory length \textbf{(TL)} and navigation error \textbf{(NE)}. As for REVERIE, we leverage four metrics for evaluating navigation performance, namely \textbf{SR}, \textbf{OSR}, \textbf{SPL} and \textbf{TL}, and two for object grounding performance, namely remote grounding success (\textbf{RGS}) and RGS weighted by path length (\textbf{RGSPL}).




\textbf{Implementation Details.} 
We implement our method based on Pytorch framework~\cite{pytorch} and Matterport3D simulator~\cite{MP3d}. Specifically, we divide our training process into two stages,~\ie pre-training and fine-tuning. For the pre-training stage, we distribute training over 4 NVIDIA 3090 GPUs for 500 epochs to convergence.  
The pre-trained model with the highest accuracy for the path ranking pretext task is selected for fine-tuning.
During the fine-tuning stage, we distribute training over 8 NVIDIA 3090 GPUs for 30 epochs to convergence. 
Following Airbert~\cite{Airbert}, we use augmented data from EnvDrop~\cite{EnvDrop} for fine-tuning by default.
More details are provided in the supplementary.



\subsection{Ablation Studies on Pre-Training}
\label{sec:ablstion}
We ablate our approach under discriminative setting on R2R benchmark. Considering the time efficiency, we do not use augmented data for fine-tuning on these experiments.


\textbf{Data Source: Airbnb Images~\vs YouTube Videos.}
One of the main differences between YouTube-VLN dataset and Airbnb dataset~\cite{Airbert} is the data source. Airbnb consists of 713k image-caption pairs while YouTube-VLN consists of 587k images extracted from 433 hours of house tour videos.
YouTube-VLN has fewer images but provides more information about a room from different camera angles by merging, which better simulates a panorama for downstream VLN tasks. 
To evaluate the effect of data source, we follow Airbert~\cite{Airbert} to randomly select images in the same house
to build a trajectory and its corresponding instruction.
We keep the number of instructions the same in pre-training for a fair comparison.
In Table~\ref{tab:ablation} (\# 1 \vs \# 2), YouTube data performs slightly better than Airbnb data, surpassing the SR by 0.89\% on the val unseen split.  We speculate this is because data quality is more important than quantity.


\textbf{Effectiveness of Reasonable Navigation Trajectory.}
YouTube-VLN dataset is collected from real house tour videos and is thus able to extract frames in chronological order to build reasonable navigation trajectories instead of combining multiple randomly chosen images.
We use the generated reasonable navigation trajectories and shuffled trajectories to train two agents, respectively. In Table~\ref{tab:ablation}, the agent trained with reasonable navigation trajectories (\# 3) achieves significantly better performance than the shuffled navigation trajectories variant (\# 2), with 1.27\% gains on SR under the val unseen split. This suggests that the agent can not well understand and ground the instruction to the trajectory without reasonable navigation paths for learning. 


\textbf{Effectiveness of Pseudo-Labeled Action.}
The core of the proposed action-aware instruction generator is the pseudo-labeled actions for the instructions. To evaluate the effectiveness of the pseudo-labeled actions, we construct a variant that replaces random action words with the pseudo-labeled actions which are filled in the instructions. The results are shown in Table~\ref{tab:ablation}, \# 4. Compared to the variant (\# 3) that fills the instructions with random action words, this variant boosts the SR metric on both the val unseen (+1.10$\%$) and seen (+0.30$\%$) splits, showing the effectiveness of pseudo-labeled actions. It indicates that with pseudo-labeled actions, the agent can effectively ground the action to the visual observation and recognizes the correct transition from one location to another.



\textbf{Effectiveness of Trajectory Judgment Pretext Task.}
To explore the layout knowledge of the YouTube-VLN dataset and equip the agent with layout reasoning ability, we propose a self-supervised trajectory judgment pretext task. In Table~\ref{tab:ablation}, the variant with the proposed pretext task improves the performance (\# 5 \vs \# 4, +2.05\% on val seen and +0.60\% on val unseen \textit{w.r.t.} SR). Moreover, given a room type and visual information of the current node as inputs, this variant predicts the relative orientation of nodes for that room type with a 30\% increase in accuracy (see supplementary for more details). These substantiate the claim of the importance of the proposed trajectory judgment task, which helps the agent learn the layout reasoning ability.

\begin{table}[!t]
{
\centering
\resizebox{1.0 \linewidth}{!}{
\begin{tabular}{clllccccc}
\topline
% \multirow{2}{*}{\#} &  & \multirow{2}{*}{Method} &  & \multicolumn{5}{c}{Val Unseen}                      \\ \cmidrule{5-9} 
        \#          &  &  Methods                 &  & TL            & NE↓           & OSR↑          & SR↑             & SPL↑      \\ \midline
1                   &  & Random Sample           &  & 9.37 & 3.52 & 72.88 & 64.41 & 0.61 \\
2                   &  & Temporal Difference     &  & 9.55 & 3.61 & 73.35 & 65.30 & 0.61 \\
3                   &  & Entropy-Based           &  & 9.64 & \textbf{3.37} & \textbf{74.93} & \textbf{66.70} & \textbf{0.62}\\ \bottomline
\end{tabular}
}
}
\caption{Comparison between different strategies of selecting frames to represent nodes under val unseen split.}
\label{tab:KeyFrames}
\vspace{-4mm}
\end{table}


\textbf{Effectiveness of Entropy-Based Trajectory Generation Strategy.}
In order to acquire an informative frame to represent a node, we propose an entropy-based technique as mentioned in Section~\ref{sec:path_gen}. To demonstrate the effectiveness of our strategy, we construct two variants,~\ie one randomly chooses a frame, and one decides the frame by temporal difference~\cite{TD}. The second variant computes the temporal pixel difference between every two consecutive frames and picks the frames where the peaks are located as the frames to represent the nodes. In Table~\ref{tab:KeyFrames}, our entropy-based method significantly outperforms these two variants, increasing the SR on the val unseen split from 64.41\% and 65.30\% to 66.70\%, respectively. We speculate this is because 1) randomly sampling frames can generate consecutive redundant frames or meaningless frames (\eg a wall takes up most of the frame); 2) the temporal difference method selects a frame that 
represents the junction of two different rooms, which is ambiguous and thus confuses the agent. In comparison, our entropy-based method is able to find a reliable frame to represent a node and ensure two adjacent nodes belong to different room types.


\subsection{Comparison with State-of-the-Arts}

\textbf{Results on R2R Dataset.}
We first compare our Lily agent with current methods under the discriminative setting. In Table~\ref{tab:discriminative}, compared with VLN-BERT that uses image-caption pairs from the web for pre-training, our Lily agent significantly increases the SR by 10.74\% on the val unseen split. This highlights the importance of providing the in-domain indoor data for pre-training. Moreover, compared with Airbert which uses in-domain image-caption pairs from online rental marketplaces, Lily still increases the SR from 73.85\% to 79.31\% on val seen and from 68.67\% to 70.00\% on val unseen. We attribute the improvement to our proposed VLN-like YouTube-VLN dataset and trajectory judgment pretext task, which have been thoughtfully evaluated in Section~\ref{sec:ablstion}. When ensembled with the speaker-follower~\cite{Speaker-Follower}, all three methods increase the performance and our Lily agent performs the best.
\begin{table}[!t]
{
\resizebox{1.0\linewidth}{!}{

    \begin{tabular}{lllcccccccccccc}
    \topline
    \multicolumn{1}{l}{\multirow{2}{*}{Methods}} & \multicolumn{4}{c}{Val Seen}    & \multicolumn{4}{c}{Val Unseen}                                          \\ \cmidrule{2-9}
                       \multicolumn{1}{c}{}     & TL            & NE↓           & SR↑            & SPL↑           & TL         & NE↓           & SR↑            & SPL↑           \\ \midline
    Follower~\cite{Speaker-Follower}          & 10.40          & 3.68                    & 65.10          & 0.62          & 9.57       & 5.20                    & 52.36          & 0.49          \\
    Speaker~\cite{Speaker-Follower}           & 11.19          & 3.80                    & 60.69          & 0.56          & 10.71      & 4.25                    & 54.66          & 0.49          \\
    \footnotesize{Speaker-Follower}~\cite{Speaker-Follower}  & 10.69          & 2.72                    & 74.22          & 0.70          & 10.10      & 3.32                    & 67.90          & 0.63          \\
    ProbES~\cite{Prompt}                      & -              & -                       & -              & -             & 9.50       & 4.05                    & 60.28          & 0.56          \\
    VLN-BERT~\cite{Improving}                 & 10.28          & 3.73                    & 70.20          & 0.66          & 9.60       & 4.10                    & 59.26          & 0.55          \\
    Airbert~\cite{Airbert}                    & 10.59          & 3.21                    & 73.85          & 0.69          & 10.03      & 3.24                    & 68.67          & 0.63          \\
    Lily                                      & 10.21          & 2.89                    & \textbf{79.31}  & \textbf{0.76}     & 10.03      & \textbf{3.19}             & \textbf{70.00}  & \textbf{0.65}     \\ \midline
    VLN-BERT*~\cite{Improving}                & 10.61          & 2.35                    & 81.86          & 0.78          & 10.00      & 2.76                   & 73.61          & 0.68          \\
    Airbert*~\cite{Airbert}~                  & 10.63          & 2.13                    & 81.40          & 0.77          & 9.99       & 2.69                    & 75.01          & 0.70          \\
    Lily*                                    & 10.51           & \textbf{2.06}           & \textbf{83.29}          & \textbf{0.80}   & 9.78  & \textbf{2.48}             & \textbf{76.88}  & \textbf{0.72} \\ 
    \bottomline
    \end{tabular}
}
}
\centering
\caption{Comparison with state-of-the-arts on R2R dataset under discriminative setting. * means results of ensembling with the speaker-follower~\cite{Speaker-Follower} model.}
\label{tab:discriminative}
\vspace{-2mm}
\end{table}

\begin{table}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccccc}
\topline
           Methods              & TL     & NE↓  & SPL↑ & OSR↑ & SR↑  \\ \midline

Speaker-Follower~\cite{Speaker-Follower}         & 1257    & 4.87 & 0.01  & 96   & 53   \\
% PreSS           ~\cite{Robust}                   & 10.52   & 4.53 & 0.53  & 63   & 57   \\
% PREVALENT       ~\cite{Generic}                  & 10.21   & 4.52 & 0.56  & 64   & 59   \\
Self-Monitoring ~\cite{SMNA}                     & 373     & 4.48 & 0.02  & 97   & 61   \\
Reinforced CM   ~\cite{RCM}                      & 358     & 4.03 & 0.02  & 96   & 63   \\
EnvDrop         ~\cite{EnvDrop}                  & 687     & 3.26 & 0.01  & 99   & 69   \\
AuxRN           ~\cite{AuxRN}                    & 41      & 3.24 & 0.21  & 81   & 71   \\
VLN-BERT~\cite{Improving}                        & 686.82  & 2.99 & 0.01  & 99   & 73   \\
Global Normalization~\cite{GN}                   & 686.86  & 2.99 & 0.01  & 99   & 74   \\
Airbert~\cite{Airbert}                           & 686.54  & 2.58 & 0.01  & 99   & 77 \\ \midline
LiLy                                             & 686.45 & \textbf{2.50}       & 0.01   &  99    & \textbf{79}     \\ \bottomline
\end{tabular}
}
\setlength{\abovecaptionskip}{0.15cm}
\caption{Results under discriminative setting on the test unseen split as indicated on the R2R leaderboard~\protect\footnotemark.}
\label{tab:leaderboard}
\vspace{-4mm}
\end{table}

\footnotetext{\fontsize{5.75pt}{1pt}{\url{https://eval.ai/web/challenges/challenge-page/97/leaderboard/270}}}

\begin{table*}[t!]
\centering
\resizebox{0.8\linewidth}{!}{\begin{tabular}{lp{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}cp{0.8cm}<{\centering}p{1.0cm}<{\centering}cp{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}cp{0.8cm}<{\centering}p{1.0cm}<{\centering}cp{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}cp{0.8cm}<{\centering}p{1.0cm}<{\centering}}
\topline
\multirow{3}{*}{Methods} & \multicolumn{7}{c}{Val Unseen}                                                     &  & \multicolumn{7}{c}{Test Unseen}             \\ 
                           \cmidrule{2-8} \cmidrule{10-16}
                         & \multicolumn{4}{c}{Navigation}                  &  & \multicolumn{2}{c}{Grounding} &  & \multicolumn{4}{c}{Navigation} &  & \multicolumn{2}{c}{Grounding} \\ \cmidrule{2-5} \cmidrule{7-8} \cmidrule{10-13} \cmidrule{15-16}
                         & TL   & OSR↑  & SR↑            & SPL↑           &  & RGS↑          & RGSPL↑        &  & TL    & OSR↑  & SR↑   & SPL↑  &  & RGS↑          & RGSPL↑         \\ \midline
Human                    & -     & -     & -              & -              &  & -             & -            &  & 21.18  & 86.83 & 81.53 & 83.66 &  & 77.84         & 51.44          \\ \midline
Seq2Seq~\cite{VLN}       & 11.07 & 8.07  & 4.20           & 2.84           &  & 2.16          & 1.63          &  & 10.89  & 6.88  & 3.99  & 3.09  &  & 2.00          & 1.58           \\
RCM~\cite{RCM}           & 11.98 & 14.23 & 9.29           & 6.97           &  & 4.89          & 3.89          &  & 10.60  & 11.68 & 7.84  & 6.67  &  & 3.67          & 3.14           \\
SMNA~\cite{SMNA}         & 9.07  & 11.28 & 8.15           & 6.44           &  & 4.54          & 3.61          &  & 9.23   & 8.39  & 5.80  & 4.53  &  & 3.10          & 2.39           \\
FM~\cite{REVERIE}        & 45.28 & 28.20 & 14.40          & 7.19           &  & 7.84          & 4.67          &  & 39.05  & 30.63 & 19.88 & 11.61 &  & 11.28         & 6.08           \\
SIA~\cite{SIA}           & 41.53 & 44.67 & 31.53          & 16.28          &  & 22.41         & 11.56         &  & 48.61  & 44.56 & 30.80 & 14.85 &  & 19.02         & 9.20           \\
HAMT~\cite{HAMT}         & 14.08 & 36.84 & 32.95          & 30.20          &  & 18.92         & 17.28         &  & 13.62  & 33.41 & 30.40 & 26.67 &  & 14.88         & 13.08           \\
RecBERT~\cite{RecVLN}    & 16.78 & 35.02 & 30.67          & 24.90          &  & 18.77         & 15.27         &  & 15.86  & 32.91 & 29.61 & 23.99 &  & 16.50         & 13.51          \\
ProbES~\cite{Prompt}     & 18.00 & 33.23 & 27.63          & 22.75          &  & 16.84         & 13.94         &  & 16.84  & 28.23 & 24.97 & 20.12 &  & 15.11         & 12.32        \\ 
Airbert~\cite{Airbert}   & 18.71 & 34.51 & 27.89          & 21.88          &  & 18.23         & 14.18         &  & 17.91  & 34.20 & 30.28 & 23.61 &  & 16.83        & 13.28          \\\cmidrule{1-16}
DUET~\cite{DUET}         & 22.11 & 51.07 & 46.98          & 33.73          &  & 32.15         & 23.03         &  & 21.30  & 56.91 & 52.51 & 36.06 &  & 31.88         & 22.06         \\
% DUET~(Lily)              & 20.62 & \textbf{53.28} & \textbf{47.06} & \textbf{33.87} &  &\textbf{32.80} & \textbf{23.71}         &  & 21.00 & \textbf{58.18} & \textbf{52.94} & \gc\textbf{37.76} &  & \textbf{32.52} & \textbf{22.93}            \\ \bottomline
DUET~(Lily)              & 21.87 & \textbf{53.71} & \textbf{48.11} & \textbf{34.43} &  &\textbf{32.15} & \textbf{23.43}         &  & 21.94 & \textbf{60.51} & \textbf{54.32} & \textbf{37.34} &  & \textbf{32.02} & 21.94           \\ \bottomline
\end{tabular}
}
\setlength{\abovecaptionskip}{0.15cm}
\caption{Comparison with state-of-the-arts  on REVERIE. Lily agent achieves the state-of-the-art performance on all splits.}
\label{tab:REVERIE_SOTA}
\vspace{-4mm}
\end{table*}


\begin{table}[htb]
\centering
\resizebox{1.0\linewidth}{!}{\begin{tabular}{lcccccccc}
\topline
\multirow{2}{*}{Methods}                     & \multicolumn{4}{c}{Val Unseen}              &\multicolumn{4}{c}{Test Unseen}          \\ \cmidrule{2-6} \cmidrule{7-9}
                            & TL     & NE↓         & SR↑      & SPL↑      &TL     & NE↓        & SR↑     & SPL↑     \\ \cmidrule{1-9}
Seq2Seq~\cite{VLN}          & 8.39   & 7.81        & 22       & -         &8.13   & 7.85       & 20      & -        \\
EnvDrop~\cite{EnvDrop}      & 10.70  & 5.22        & 52       & 48        &11.66  & 5.23       & 51      & 47       \\
AuxRN~\cite{AuxRN}          & -      & 5.28        & 55       & 50        &-      & 5.15       & 55      & 51       \\
\footnotesize{PREVALENT}~\cite{Generic}    & 10.19  & 4.71        & 58       & 53        &10.51  & 5.30       & 54      & 51       \\
RelGraph~\cite{RelGraph}    & 9.99   & 4.73        & 57       & 53        &10.29  & 4.75       & 55      & 52       \\
RecBERT~\cite{RecVLN}       & 12.01  & 3.93        & 63       & 57        &12.35  & 4.09       & 63      & 57       \\
ProbES~\cite{Prompt}        & 11.58  & 4.00        & 61       & 55        &12.43  & 4.20       & 62      & 56       \\
ADAPT~\cite{ADAPT}          & 12.33  & 3.66        & 66       & 59        &13.16  & 4.11       & 63      & 57       \\
HOP~\cite{HOP}              & 12.27  & 3.80        & 64       & 57        &12.65  & 3.83       & 64      & 59       \\
HAMT~\cite{HAMT}            & 11.46  & 2.29        & 66       & 61        &12.27  & 3.93       & 65      & 60       \\
Airbert~\cite{Airbert}      & 11.78  & 4.01        & 62       & 56        &12.41  & 4.13       & 62      & 57       \\ \cmidrule{1-9}
DUET~\cite{DUET}            & 13.94  & 3.31        & 72       & 60        &14.73  & 3.65       & 69      & 59       \\ 
DUET (Lily)                 & 14.58  & \textbf{2.90}  & \textbf{74}  & \textbf{62} &16.13  & \textbf{3.44}  &\textbf{72}  & \textbf{60}  \\ \bottomline
\end{tabular}
}
\caption{Comparison with state-of-the-arts on R2R dataset under generative setting.}
\label{tab:generative}
\vspace{-3mm}
\end{table}
% \vspace{-6mm}

In Table~\ref{tab:leaderboard}, we evaluate on R2R test split and our Lily ranks first on the VLN challenge leaderboard compared with the results whose manuscripts are publicly available, achieving the highest SR of 79\%. As we follow Airbert to use 30 candidate trajectories from EnvDrop~\cite{EnvDrop} and the leaderboard considers that our agent has walked through all these paths, the SPL metric is low for both Lily and Airbert.

Besides, our Lily agent also helps to increase the performance on R2R under the generative setting. We enhance the state-of-the-art DUET~\cite{DUET} method by pre-training the agent using our method as mentioned in Section~\ref{sec:approach}. In Table~\ref{tab:generative}, the agent incorporating Lily achieves 2\% and 3\% improvements \textit{w.r.t.} SR on the val unseen split and test unseen split, respectively, compared to DUET. Notably, our method increases the SPL by 2\% on the val unseen split, indicating the agent is able to reach the goal more efficiently. We attribute this to the agent's acquisition of layout prior knowledge via the proposed trajectory judgment task on our YouTube-VLN dataset, enabling it to plan more efficient routes to the goals in new environments.



\textbf{Results on REVERIE Dataset.}
Compared with R2R, REVERIE is more challenging as its instructions only describe the destinations without detailed path descriptions. This requires the agent to be equipped with common knowledge about the room layouts and to reason the possible paths that lead to the destinations. In Table~\ref{tab:REVERIE_SOTA}, we outperform the SOTA agent (\ie DUET) by 1.13\% on SR and increase the SPL from 46.98\% to 48.11\% on the val unseen split. It is worth noting that our method achieves a higher OSR with a shorter TL, indicating that our agent finds the destination more quickly. We attribute this to the better layout reason ability learned from the large-scale diverse reasonable trajectories in the proposed YouTube-VLN dataset. A similar performance is obtained on the test unseen split, where the Lily agent improves the SR by 1.81\% and the SPL by 1.28\%. Although our objective is solely navigation, we still achieve comparable performance on grounding metrics and even slightly outstrip DUET on most metrics.


\subsection{Learning Navigation from One Environment}

Our intuition is that pre-training on the YouTube-VLN dataset can mitigate the domain gap of training from scratch and is able to achieve excellent performance with only a few training environments. To verify this, we conduct a one-shot learning study, where we fine-tune our model on only one environment of the original training environments. Note that the candidate paths are generated from all of the possible paths from the start viewpoints to all navigable points, instead of from the expert model in EnvDrop~\cite{EnvDrop}. All the candidate paths are the shortest paths in the navigation graphs. To reduce the bias, we randomly select 5 sets from the entire environments and report the average results.

In Table~\ref{tab:few-shot}, our agent outperforms all the existing pre-training methods. On the val seen split using one-shot fine-tuning, compared to VLN-BERT and AirBERT, our Lily agent achieves 3.60\% and 1.43\% improvements, respectively. In the val unseen split using one-shot fine-tuning, we achieve 28.43\% improvement compared to VLN-BERT. All these results suggest the effectiveness of our method.


 \begin{table}[t!]
\centering
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{lcccccc}
\topline
Methods                   &         & Val Seen      & Val Unseen                   \\ \midline
VLN BERT~\cite{Improving} &         & 45.71         & 22.43                      \\
AirBERT~\cite{Airbert}    &         & 47.88         & 50.00                      \\
Lily                      &         & \textbf{49.31}& \textbf{50.86}             \\ \bottomline
\end{tabular}
}
\caption{SR on val seen and val unseen splits of R2R. All the agents access only one environment.}
\label{tab:few-shot}
\vspace{-4mm}
\end{table}


\section{Conclusion}

In this work, we propose a new approach Lily to address the limitations of existing vision-and-language navigation (VLN) methods by creating a large-scale VLN-like dataset from real house tour videos to train our embodied agent. We overcome the challenges of automatically generating path-instruction pairs to construct the dataset from raw and unlabeled videos by leveraging an entropy-based method for trajectory construction and an action-aware generator for instruction generation. Additionally, we train the agent to judge the reasonableness of trajectories, improving its layout reasoning ability. Our method achieves state-of-the-art performance on two popular benchmarks (R2R and REVERIE), demonstrating the efficacy. Overall, we hope our work can provide valuable insight into the VLN community by learning embodied VLN from passive videos.


\section*{Acknowledgement}
Prof. Tan and his students were partially supported by the 
National Natural Science Foundation of China (NSFC) (62072190), National Natural Science Foundation of China (NSFC) 61836003 (key project), Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183.



%%%%%%%%% REFERENCES
\bibliographystyle{abbrv}
{
	\small
	\bibliography{ref}
}

\onecolumn 
\newpage
\appendix
\input{supp}

\end{document}

