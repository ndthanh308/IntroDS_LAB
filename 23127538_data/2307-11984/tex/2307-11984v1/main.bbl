\begin{thebibliography}{10}

\bibitem{bottom-up}
P.~Anderson, X.~He, C.~Buehler, D.~Teney, M.~Johnson, S.~Gould, and L.~Zhang.
\newblock Bottom-up and top-down attention for image captioning and visual
  question answering.
\newblock In {\em {CVPR}}, pages 6077--6086, 2018.

\bibitem{VLN}
P.~Anderson, Q.~Wu, D.~Teney, J.~Bruce, M.~Johnson, N.~S{\"{u}}nderhauf, I.~D.
  Reid, S.~Gould, and A.~van~den Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In {\em {CVPR}}, pages 3674--3683, 2018.

\bibitem{MP3d}
A.~X. Chang, A.~Dai, T.~A. Funkhouser, M.~Halber, M.~Nie{\ss}ner, M.~Savva,
  S.~Song, A.~Zeng, and Y.~Zhang.
\newblock Matterport3d: Learning from {RGB-D} data in indoor environments.
\newblock In {\em {3DV}}, pages 667--676, 2017.

\bibitem{YTb}
M.~Chang, A.~Gupta, and S.~Gupta.
\newblock Semantic visual navigation by watching youtube videos.
\newblock In {\em {NeurIPS}}, pages 4283--4294, 2020.

\bibitem{Weakly-Supervised}
P.~Chen, D.~Ji, K.~Lin, R.~Zeng, T.~H. Li, M.~Tan, and C.~Gan.
\newblock Weakly-supervised multi-granularity map learning for
  vision-and-language navigation.
\newblock In {\em {NeurIPS}}, 2022.

\bibitem{HAMT}
S.~Chen, P.~Guhur, C.~Schmid, and I.~Laptev.
\newblock History aware multimodal transformer for vision-and-language
  navigation.
\newblock In {\em {NeurIPS}}, pages 5834--5847, 2021.

\bibitem{DUET}
S.~Chen, P.~Guhur, M.~Tapaswi, C.~Schmid, and I.~Laptev.
\newblock Think global, act local: Dual-scale graph transformer for
  vision-and-language navigation.
\newblock In {\em {CVPR}}, pages 16516--16526, 2022.

\bibitem{AutoVLN}
S.~Chen, P.-L. Guhur, M.~Tapaswi, C.~Schmid, and I.~Laptev.
\newblock Learning from unlabeled 3d environments for vision-and-language
  navigation.
\newblock In {\em {ECCV}}, 2022.

\bibitem{BERT}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em {NAACL}}, pages 4171--4186, 2019.

\bibitem{ding2022embodied}
M.~Ding, Y.~Xu, Z.~Chen, D.~D. Cox, P.~Luo, J.~B. Tenenbaum, and C.~Gan.
\newblock Embodied concept learner: Self-supervised learning of concepts and
  mapping through instruction following.
\newblock In {\em CoRL}, 2022.

\bibitem{Speaker-Follower}
D.~Fried, R.~Hu, V.~Cirik, A.~Rohrbach, J.~Andreas, L.~Morency,
  T.~Berg{-}Kirkpatrick, K.~Saenko, D.~Klein, and T.~Darrell.
\newblock Speaker-follower models for vision-and-language navigation.
\newblock In {\em {NeurIPS}}, pages 3318--3329, 2018.

\bibitem{APS}
T.~Fu, X.~E. Wang, M.~F. Peterson, S.~T. Grafton, M.~P. Eckstein, and W.~Y.
  Wang.
\newblock Counterfactual vision-and-language navigation via adversarial path
  sampler.
\newblock In {\em {ECCV}}, pages 71--86, 2020.

\bibitem{Room-and-Object}
C.~Gao, J.~Chen, S.~Liu, L.~Wang, Q.~Zhang, and Q.~Wu.
\newblock Room-and-object aware knowledge reasoning for remote embodied
  referring expression.
\newblock In {\em {CVPR}}, pages 3064--3073, 2021.

\bibitem{Airbert}
P.~Guhur, M.~Tapaswi, S.~Chen, I.~Laptev, and C.~Schmid.
\newblock Airbert: In-domain pretraining for vision-and-language navigation.
\newblock In {\em {ICCV}}, pages 1614--1623, 2021.

\bibitem{Generic}
W.~Hao, C.~Li, X.~Li, L.~Carin, and J.~Gao.
\newblock Towards learning a generic agent for vision-and-language navigation
  via pre-training.
\newblock In {\em {CVPR}}, pages 13134--13143, 2020.

\bibitem{image-text-action}
W.~Hao, C.~Li, X.~Li, L.~Carin, and J.~Gao.
\newblock Towards learning a generic agent for vision-and-language navigation
  via pre-training.
\newblock In {\em {CVPR}}, pages 13134--13143, 2020.

\bibitem{Mask_RCNN}
K.~He, G.~Gkioxari, P.~Doll{\'{a}}r, and R.~B. Girshick.
\newblock Mask {R-CNN}.
\newblock In {\em {ICCV}}, pages 2980--2988, 2017.

\bibitem{ResNet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {CVPR}}, pages 770--778, 2016.

\bibitem{RelGraph}
Y.~Hong, C.~R. Opazo, Y.~Qi, Q.~Wu, and S.~Gould.
\newblock Language and visual entity relationship graph for agent navigation.
\newblock In {\em {NeurIPS}}, pages 7685--7696, 2020.

\bibitem{RecVLN}
Y.~Hong, Q.~Wu, Y.~Qi, C.~R. Opazo, and S.~Gould.
\newblock {VLN} {BERT:} {A} recurrent vision-and-language {BERT} for
  navigation.
\newblock In {\em {CVPR}}, pages 1643--1653, 2021.

\bibitem{Transferable}
H.~Huang, V.~Jain, H.~Mehta, A.~Ku, G.~Magalh{\~{a}}es, J.~Baldridge, and
  E.~Ie.
\newblock Transferable representation learning in vision-and-language
  navigation.
\newblock In {\em {ICCV}}, pages 7403--7412, 2019.

\bibitem{R4R}
V.~Jain, G.~Magalh{\~{a}}es, A.~Ku, A.~Vaswani, E.~Ie, and J.~Baldridge.
\newblock Stay on the path: Instruction fidelity in vision-and-language
  navigation.
\newblock In {\em {ACL}}, pages 1862--1872, 2019.

\bibitem{ANewPath}
A.~Kamath, P.~Anderson, S.~Wang, J.~Y. Koh, A.~Ku, A.~Waters, Y.~Yang,
  J.~Baldridge, and Z.~Parekh.
\newblock A new path: Scaling vision-and-language navigation with synthetic
  instructions and imitation learning.
\newblock {\em CoRR}, abs/2210.03112, 2022.

\bibitem{Pathdreamer}
J.~Y. Koh, H.~Lee, Y.~Yang, J.~Baldridge, and P.~Anderson.
\newblock Pathdreamer: {A} world model for indoor navigation.
\newblock In {\em {ICCV}}, pages 14718--14728, 2021.

\bibitem{VG}
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.~Li, D.~A. Shamma, M.~S. Bernstein, and L.~Fei{-}Fei.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock {\em {IJCV}}, 123:32--73, 2017.

\bibitem{RxR}
A.~Ku, P.~Anderson, R.~Patel, E.~Ie, and J.~Baldridge.
\newblock Room-across-room: Multilingual vision-and-language navigation with
  dense spatiotemporal grounding.
\newblock In {\em {EMNLP}}, pages 4392--4412, 2020.

\bibitem{SEA}
C.~Kuo, C.~Ma, J.~Hoffman, and Z.~Kira.
\newblock Structure-encoding auxiliary tasks for improved visual representation
  in vision-and-language navigation.
\newblock In {\em {WACV}}, pages 1104--1113, 2023.

\bibitem{BLIP}
J.~Li, D.~Li, C.~Xiong, and S.~C.~H. Hoi.
\newblock {BLIP:} bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em {ICML}}, pages 12888--12900.

\bibitem{Cross-Modal}
J.~Li, H.~Tan, and M.~Bansal.
\newblock Improving cross-modal alignment in vision language navigation via
  syntactic information.
\newblock In {\em {NAACL}}, pages 1041--1050, 2021.

\bibitem{SIVLN}
J.~Li, H.~Tan, and M.~Bansal.
\newblock Improving cross-modal alignment in vision language navigation via
  syntactic information.
\newblock In K.~Toutanova, A.~Rumshisky, L.~Zettlemoyer,
  D.~Hakkani{-}T{\"{u}}r, I.~Beltagy, S.~Bethard, R.~Cotterell, T.~Chakraborty,
  and Y.~Zhou, editors, {\em {NAACL-HLT}}, pages 1041--1050, 2021.

\bibitem{CLEAR}
J.~Li, H.~Tan, and M.~Bansal.
\newblock {CLEAR:} improving vision-language navigation with cross-lingual,
  environment-agnostic representations.
\newblock In {\em {NAACL}}, pages 633--649, 2022.

\bibitem{Envedit}
J.~Li, H.~Tan, and M.~Bansal.
\newblock Envedit: Environment editing for vision-and-language navigation.
\newblock In {\em {CVPR}}, pages 15386--15396, 2022.

\bibitem{liang2015automated}
C.~Liang, K.~Chee, Y.~Zou, H.~Zhu, A.~Causo, S.~Vidas, T.~Teng, I.~Chen,
  K.~Low, and C.~Cheah.
\newblock Automated robot picking system for e-commerce fulfillment warehouse
  application.
\newblock In {\em {IFToMM}}, 2015.

\bibitem{Prompt}
X.~Liang, F.~Zhu, L.~Li, H.~Xu, and X.~Liang.
\newblock Visual-language navigation pretraining via prompt-based environmental
  self-exploration.
\newblock In {\em {ACL}}, pages 4837--4851, 2022.

\bibitem{Contrastive-PI}
X.~Liang, F.~Zhu, Y.~Zhu, B.~Lin, B.~Wang, and X.~Liang.
\newblock Contrastive instruction-trajectory learning for vision-language
  navigation.
\newblock In {\em {AAAI}}, pages 1592--1600, 2022.

\bibitem{ADAPT}
B.~Lin, Y.~Zhu, Z.~Chen, X.~Liang, J.~Liu, and X.~Liang.
\newblock {ADAPT:} vision-language navigation with modality-aligned action
  prompts.
\newblock In {\em {CVPR}}, pages 15375--15385, 2022.

\bibitem{COCO}
T.~Lin, M.~Maire, S.~J. Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'{a}}r, and C.~L. Zitnick.
\newblock Microsoft {COCO:} common objects in context.
\newblock In {\em {ECCV}}, pages 740--755, 2014.

\bibitem{SIA}
X.~Lin, G.~Li, and Y.~Yu.
\newblock Scene-intuitive agent for remote embodied visual grounding.
\newblock In {\em {CVPR}}, pages 7036--7045, 2021.

\bibitem{ViLBERT}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In {\em {NeurIPS}}, pages 13--23, 2019.

\bibitem{SMNA}
C.~Ma, J.~Lu, Z.~Wu, G.~AlRegib, Z.~Kira, R.~Socher, and C.~Xiong.
\newblock Self-monitoring navigation agent via auxiliary progress estimation.
\newblock In {\em {ICLR}}, 2019.

\bibitem{cat}
S.~Ma, Y.~Wang, Y.~Wei, J.~Fan, T.~H. Li, H.~Liu, and F.~Lv.
\newblock Cat: Localization and identification cascade detection transformer
  for open-world object detection.
\newblock In {\em {CVPR}}, pages 19681--19690, 2023.

\bibitem{Improving}
A.~Majumdar, A.~Shrivastava, S.~Lee, P.~Anderson, D.~Parikh, and D.~Batra.
\newblock Improving vision-and-language navigation with image-text pairs from
  the web.
\newblock In {\em {ECCV}}, pages 259--274, 2020.

\bibitem{VLNBert}
A.~Majumdar, A.~Shrivastava, S.~Lee, P.~Anderson, D.~Parikh, and D.~Batra.
\newblock Improving vision-and-language navigation with image-text pairs from
  the web.
\newblock In {\em {ECCV}}, pages 259--274, 2020.

\bibitem{SOAT}
A.~Moudgil, A.~Majumdar, H.~Agrawal, S.~Lee, and D.~Batra.
\newblock {SOAT:} {A} scene- and object-aware transformer for
  vision-and-language navigation.
\newblock In {\em {NeurIPS}}, pages 7357--7367, 2021.

\bibitem{EATA}
S.~Niu, J.~Wu, Y.~Zhang, Y.~Chen, S.~Zheng, P.~Zhao, and M.~Tan.
\newblock Efficient test-time model adaptation without forgetting.
\newblock In {\em {ICML}}, pages 16888--16905, 2022.

\bibitem{GPT4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock 2023.

\bibitem{pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~K{\"{o}}pf, E.~Z. Yang,
  Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang,
  J.~Bai, and S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em {NeurIPS}}, pages 8024--8035, 2019.

\bibitem{REVERIE}
Y.~Qi, Q.~Wu, P.~Anderson, X.~Wang, W.~Y. Wang, C.~Shen, and A.~van~den Hengel.
\newblock {REVERIE:} remote embodied visual referring expression in real indoor
  environments.
\newblock In {\em {CVPR}}, pages 9979--9988, 2020.

\bibitem{HOP}
Y.~Qiao, Y.~Qi, Y.~Hong, Z.~Yu, P.~Wang, and Q.~Wu.
\newblock Hop: History-and-order aware pre-training for vision-and-language
  navigation.
\newblock In {\em {CVPR}}, pages 15418--15427, 2022.

\bibitem{CLIP}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em {ICML}}, pages 8748--8763, 2021.

\bibitem{HMP3D}
S.~K. Ramakrishnan, A.~Gokaslan, E.~Wijmans, O.~Maksymets, A.~Clegg, J.~Turner,
  E.~Undersander, W.~Galuba, A.~Westbury, A.~X. Chang, M.~Savva, Y.~Zhao, and
  D.~Batra.
\newblock Habitat-matterport 3d dataset {(HM3D):} 1000 large-scale 3d
  environments for embodied {AI}.
\newblock In {\em NeurIPS Datasets and Benchmarks}, 2021.

\bibitem{Outdoor}
R.~Schumann and S.~Riezler.
\newblock Analyzing generalization of vision and language navigation to unseen
  outdoor areas.
\newblock In S.~Muresan, P.~Nakov, and A.~Villavicencio, editors, {\em {ACL}},
  pages 7519--7532, 2022.

\bibitem{One-Step}
C.~H. Song, J.~Kil, T.~Pan, B.~M. Sadler, W.~Chao, and Y.~Su.
\newblock One step at a time: Long-horizon vision-and-language navigation with
  milestones.
\newblock In {\em {CVPR}}, pages 15461--15470, 2022.

\bibitem{EnvDrop}
H.~Tan, L.~Yu, and M.~Bansal.
\newblock Learning to navigate unseen environments: Back translation with
  environmental dropout.
\newblock In J.~Burstein, C.~Doran, and T.~Solorio, editors, {\em {NAACL}},
  pages 2610--2621, 2019.

\bibitem{Transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In {\em {NeurIPS}}, pages 5998--6008, 2017.

\bibitem{Less-is-More}
S.~Wang, C.~Montgomery, J.~Orbay, V.~Birodkar, A.~Faust, I.~Gur, N.~Jaques,
  A.~Waters, J.~Baldridge, and P.~Anderson.
\newblock Less is more: Generating grounded navigation instructions from
  landmarks.
\newblock In {\em {CVPR}}, pages 15407--15417, 2022.

\bibitem{RCMS}
X.~Wang, Q.~Huang, A.~Celikyilmaz, J.~Gao, D.~Shen, Y.~Wang, W.~Y. Wang, and
  L.~Zhang.
\newblock Reinforced cross-modal matching and self-supervised imitation
  learning for vision-language navigation.
\newblock In {\em {CVPR}}, pages 6629--6638, 2019.

\bibitem{RCM}
X.~Wang, Q.~Huang, A.~Celikyilmaz, J.~Gao, D.~Shen, Y.~Wang, W.~Y. Wang, and
  L.~Zhang.
\newblock Reinforced cross-modal matching and self-supervised imitation
  learning for vision-language navigation.
\newblock In {\em {CVPR}}, pages 6629--6638, 2019.

\bibitem{Gibson}
F.~Xia, A.~R. Zamir, Z.~He, A.~Sax, J.~Malik, and S.~Savarese.
\newblock Gibson env: Real-world perception for embodied agents.
\newblock In {\em {CVPR}}, pages 9068--9079, 2018.

\bibitem{GN}
L.~Xie, M.~Zhang, Y.~Li, W.~Qin, Y.~Yan, and E.~Yin.
\newblock Vision--language navigation with beam-constrained global
  normalization.
\newblock {\em {TNNLS}}, 2022.

\bibitem{HomeRoboticsApplication}
G.~A. Zachiotis, G.~Andrikopoulos, R.~Gornez, K.~Nakamura, and
  G.~Nikolakopoulos.
\newblock A survey on the application trends of home service robotics.
\newblock In {\em {ROBIO}}, pages 1999--2006, 2018.

\bibitem{TD}
Y.~Zheng and L.~Fan.
\newblock Moving object detection based on running average background and
  temporal difference.
\newblock In {\em {ISKE}}, pages 270--272, 2010.

\bibitem{Place_365}
B.~Zhou, {\`{A}}.~Lapedriza, A.~Khosla, A.~Oliva, and A.~Torralba.
\newblock Places: {A} 10 million image database for scene recognition.
\newblock {\em {TIPAMI}}, 40:1452--1464, 2018.

\bibitem{SOON}
F.~Zhu, X.~Liang, Y.~Zhu, Q.~Yu, X.~Chang, and X.~Liang.
\newblock {SOON:} scenario oriented object navigation with graph-based
  exploration.
\newblock In {\em {CVPR}}, pages 12689--12699, 2021.

\bibitem{AuxRN}
F.~Zhu, Y.~Zhu, X.~Chang, and X.~Liang.
\newblock Vision-language navigation with self-supervised auxiliary reasoning
  tasks.
\newblock In {\em {CVPR}}, pages 10009--10019, 2020.

\end{thebibliography}
