% \title{Supplementary Materials for \\  
%         ``Learning Vision-and-Language Navigation from YouTube Videos''}

% \author{
%     Kunyang Lin\textsuperscript{\rm 1 2}\thanks{Equal contribution. Email: \{imkunyanglin, phchencs\}@gmail.com} ~~ 
%     Peihao Chen\textsuperscript{\rm 1}\footnotemark[1] ~~ 
%     Diwei Huang\textsuperscript{\rm 1} ~ 
%     Thomas H. Li\textsuperscript{\rm 6} ~
%     Mingkui Tan\textsuperscript{\rm 1 \rm 5}\thanks{Corresponding author. Email: mingkuitan@scut.edu.cn} ~
%     Chuang Gan\textsuperscript{\rm 3 \rm 4} \\
%     \textsuperscript{\scriptsize{\rm 1}}\small{South China University of Technology,}
%     \textsuperscript{\scriptsize{\rm 2}}\small{Information Technology R\&D Innovation Center of Peking University,}\\
%     \textsuperscript{\scriptsize{\rm 3}}\small{UMass Amherst,}
%     \textsuperscript{\rm 4}\small{MIT-IBM Watson AI Lab,}
%     \textsuperscript{\rm 5}\small{Key Laboratory of Big Data and Intelligent Robot, Ministry of Education,} \\
%     \textsuperscript{\scriptsize{\rm 6}}\small{Peking University Shenzhen Graduate School} 
% }
% % \maketitle

% \vspace{-4mm}
\begin{center}
	{
		\LARGE{\textsc{Appendix}}
	}
\end{center}

% \begin{leftline}
% 	{
% 		\LARGE{\textsc{Appendix}}
% 	}
% \end{leftline}
In the supplementary, we provide more details of our method. We organize the supplementary as follows.

\begin{itemize}[leftmargin=*]
    \item In Section~\ref{sec:supp-collection}, we present more details on video collection.
    \item In Section~\ref{sec:supp-filter}, we present more details on frame filtering.
    \item In Section~\ref{sec:supp-trajctory}, we present more details on trajectory generation.
    \item In Section~\ref{sec:supp-instruction}, we present more details on instruction generation.
    \item In Section~\ref{sec:statistic-examples}, we present statistics and visualization examples of our YouTube-VLN dataset.
    \item In Section~\ref{sec:imple}, we provide more implementation details of experiments.
    \item In Section~\ref{sec:supp-layout_effectives}, we provide more details of the effectiveness of trajectory judgment task on layout reasoning ability.
    \item In Section~\ref{sec:supp-trans}, we provide the transferability results of our method.
    \item In Section~\ref{sec:supp-qualitive}, we present qualitative results of our method.
    \item In Section~\ref{sec:supp-future}, we discuss potential future research and social impact.
\end{itemize}


\section{More Details on YouTube Video Collection}
\label{sec:supp-collection}
We collect real estate tour videos from YouTube\footnote{\url{https://www.youtube.com}}. Specifically, we restrict the videos to real estate tour videos. It is unrealistic to play all categories of videos one by one to check whether they meet our requirements. On the contrary, we look for several well-known YouTubers whose playlists have been well-categorized for house tour videos, and each list has a consistent style. To find such YouTubers, we only spend less than an hour of mannual search time. In YouTube, each video has its corresponding video id (~\eg $C99YjG\_JBsg$\footnote{\url{https://www.youtube.com/watch?v=C99YjG_JBsg}}), we obtain all the house tour videos according to the video ids and regard the video ids as house ids.



\section{More Details on Frame Filtering}
\label{sec:supp-filter}
Before generating a navigation trajectory, we first pre-process these videos by sparse sampling and using off-the-shelf image classifiers~\cite{Mask_RCNN, ResNet} to filter out redundant frames and noisy frames (\ie frames with persons or outdoor scenes). A video can be sampled at up to 60 frames per second. Generally, there is almost no obvious change between screens within 2s intervals in a video. Therefore, we sparsely sample the videos with 0.5 frames per second. Considering that in the downstream indoor VLN task, persons and outdoor images are not allowed to occur in the observations, we discard such noisy frames in the house tour videos. Specifically, we employ a Resnet~\cite{ResNet} model pre-trained on Place 365~\cite{Place_365} and Mask RCNN~\cite{Mask_RCNN} model pre-trained on COCO~\cite{COCO} to detect the outdoor images~(as shown in Figure~\ref{fig:outdoor_example}) and images with persons~(as shown in Figure~\ref{fig:person_example}), respectively. In addition, some images are filtered since they do not contain any objects and can not be extracted to region features (as shown in Figure~\ref{fig:feature_example}).

% Figure environment removed

% Figure environment removed

\section{More Details on Trajectory Generation}
\label{sec:supp-trajctory}
To mimic the sequential navigation path in R2R, we typically choose $K \in [4,7]$ as the length of a trajectory. As mentioned in Section 3.1, a trajectory consists of room nodes and transition nodes. We randomly sample $R \in [2,7]$ room nodes in temporal order for a trajectory. Considering that 1) navigation is a continuous problem in both temporal dimension and spatial dimension, and 2) instruction does not necessarily describe all observations on a trajectory, the remaining $(K - R)$ nodes are filled with transition nodes. Each image is encoded into region features by Faster R-CNN bottom-up top-down attention~\cite{bottom-up} model pre-trained on Visual Genome~\cite{VG}. In order to approximate the panoramic visual context, we merge the region features from similar room types per image. The images we merged are from consecutive frames and in the same group, usually taken by the real estate agent around similar locations in order to better introduce the room. 


% Figure environment removed


% Figure environment removed




% Figure environment removed


\section{More Details on Instruction Generation}
\label{sec:supp-instruction}
As for instruction generation, we first harvest 14,031 fill-in-the-blank templates from the R2R training set. Specifically, we extract the noun phrases and verb phrases for each human-annotated navigation instruction in the R2R training set. We then randomly select a template that has $R$ noun phrase blanks and $(R-1)$ verb phrase blanks for a trajectory with $R$ room nodes. We use CLIP~\cite{CLIP} model to caption the room nodes with a template ``[room] with [object]'' following~\cite{Prompt}, where the ``[room]'' and ``[object]'' represent the room category and object category of a room node, respectively. To better include the details of the rooms, we fill a noun blank with ``[room] with [object]'' or ``[room]'' or ``[object]''.  A CNN inverse action model~\cite{YTb} infers the transition action from one room node to another. Finally, we obtain $R$ captions and $(R-1)$ action words for a template. The captions are used to fill the noun phrase blanks in the template sequentially. For each noun phrase blank filled with the captions of one room node, we find its closest verb phrase blank and fill it with the action which is executed to reach the next room node. Our instruction generation strategy fills the verb phrase blanks with pseudo-labeled actions, providing a natural transition between two nodes to the created instruction.


\section{Statistics and Visualizations of YouTube-VLN Dataset}
\label{sec:statistic-examples}
In Figure~\ref{fig:statistics}, we show some key statistics about our YouTube-VLN dataset. We construct the YouTube-VLN from the collected 4078 videos. After filtering the noisy frames, we harvest 568K images in total. In Figure~\ref{fig:frames_video}, we present the number of frames per video via a histogram. It shows that most of the videos contain more than 25 effective frames, indicating that each video can provide sufficient image samples for an agent to learn and reason about this house. Figure~\ref{fig:room_types} presents the predicted room types of the frames. We used CLIP~\cite{CLIP} to categorize each frame into one of the 12 labeled room types in Matterport dataset~\cite{VLN}. It can be observed that most of the images in the proposed YouTube-VLN dataset cover the core part of a house (~\eg family room and bedroom). This enables the agent to learn the layout prior knowledge more efficiently. Moreover, these labels are further used for instruction generation and image merging. In addition, we also show the distribution of the pseudo-labeled actions in Figure~\ref{fig:actions}. Each action is the predicted native action from one room node to another, representing the direction that the agent should follow. As shown in the histogram, the pseudo-labeled actions are evenly distributed into three types of actions, endowing the agent to understand the actions efficiently. We also show some visualization examples of the generated path-instruction pairs in YouTube-VLN as in Figure~\ref{fig:YTb_1}, Figure~\ref{fig:YTb_2}, and Figure~\ref{fig:YTb_3}.



% \newpage

\section{More Implementation Details}
\label{sec:imple}
The model architecture details are shown in Figure~\ref{fig:models}. The meanings of each layer are as follows:

\begin{itemize}
    \item \textbf{Embed-Lang}: language token embeddings, which consist of word embeddings, position embeddings, and token type embeddings.
    \item \textbf{Embed-Vis}: vision token embeddings, which consist of visual feature embeddings and position embeddings of the current node.
    \item \textbf{Embed-Node}: node embeddings, which consist of visual feature embeddings, position embeddings, and navigation step embeddings of all nodes in the navigation graph.
    \item \textbf{Self-Att-Lang}: self-attention layers for language input.
    \item \textbf{Self-Att-Vis}: self-attention layers for vision input.
    \item \textbf{Cross-Att-Vis}: cross-modal attention layers for vision branch.
    \item \textbf{Cross-Att-Lang}: cross-modal attention layers for language branch.
    \item \textbf{FFN}: feed-forward network, which consists of two linear layers.
\end{itemize}

% Figure environment removed


\subsection{Pre-training Details}
As described in Section~4.1, we adopt a ViLBERT-like architecture as the same as Airbert~\cite{Airbert}. The model architecture of pre-training is shown in Figure~\ref{fig:vilbert_disc_model}. For a fair comparison, we set both $L_1$ and $L_2$ to 6, consistent with Airbert in the discriminative setting. For generative adaption, the number of layers $L_1$ is equal to 9 and $L_2$ is equal to 5 for a fair comparison with DUET~\cite{DUET}. For both settings, we distribute training over 4 NVIDIA 3090 GPUs~(24GB each) for 500 epochs to convergence. The batch size is 8 (2 for each GPU) and the learning rate is $2 \times 10^{-5}$. We randomly selected 95\% videos per epoch as the training set and 5\% videos as the test set.

\subsection{Fine-tuning Details}

% \noindent\textbf{Discriminative Setting.}
\paragraph{Discriminative Setting.}
In our experiments, discriminative evaluation is conducted on R2R dataset. In this setting, VLN is formulated as a path selection problem. As shown in Figure~\ref{fig:vilbert_disc_model}, the model architecture is the same as the pre-trained model whose classifier used in the path ranking pretext task can be directly for path selection. We follow a two-stage fine-tuning as Airbert, which fine-tunes the agent with MLM task and MVM task in stage one and PR task in stage two. In stage one, the batch size is 12 on 4 VIDIA 3090 GPUs~(24GB each) and the learning rate is $4 \times 10^{-5}$. In stage two, we set the batch size as 16 on 8 NVIDIA 3090 GPUs~(24GB each) and the learning rate as $1 \times 10^{-5}$. The agent is fine-tuned for 30 epochs in both stages. The visual features are also encoded by a bottom-up top-down attention~\cite{bottom-up} model. We select the model checkpoint with the highest success rate on the val unseen validation split for the test set evaluation and leaderboard submission.

\vspace{-3mm}
% \noindent\textbf{Generative Setting.}
\paragraph{Generative Setting.}
In the generative setting, the agent needs to predict actions sequentially in order to reach the goal (R2R) or find the object (REVERIE). We adopt DUET~\cite{DUET} as the architecture for fine-tuning as it is the state-of-the-art model. As illustrated in Figure~\ref{fig:vilbert_gen_model}, DUET is a three-stream  architecture which is fed with $P$ node features $\left\{n_i\right\}_{i=1}^P$, word tokens $\{[\texttt {CLS}],\left\{w_i\right\}_{i=1}^L, [\texttt {SEP}]\}$ and the current panorama encoding with image features $\left\{r_i\right\}_{i=1}^n$ together with object features $\left\{o_i\right\}_{i=1}^m$. The BERT-like architecture is used to determine which node should the agent go to or what the object goal id is. We initialize the language stream and the cross-modal streams using the corresponding modules in our pre-trained model, highlighted as the orange dotted line in Figure~\ref{fig:vilbert_gen_model}. The other settings remain the same as DUET for a fair comparison.


% Figure environment removed

\section{Effectiveness of Trajectory Judgment Task on Layout Reasoning Ability}
\label{sec:supp-layout_effectives}
To evaluate the effectiveness of the proposed trajectory judgment task, we conduct an experiment to evaluate whether the agent trained with this task is able to figure out the direction of an unexplored room based on current observation. Since the common room layout knowledge is required for figuring out this question, the accuracy of this question reveals the layout reasoning ability of the agent.

We conduct this experiment on the R2R~\cite{VLN} dataset, which provides a navigation graph for each environment. Specifically, we randomly initialize an agent on a navigation node. Then, we sample another node from the candidate node list of the current node and use the CLIP~\cite{CLIP} model to identify the room type of this node. Given this room type and the panorama visual feature of the current node as text input and visual input, respectively, the agent is asked to predict the relative orientation of the node for that room type. We define the angle between the matching node of the room type and the current orientation as $s \in [-180^\circ, 180^\circ]$. We then divide $[-180^\circ, 180^\circ]$ uniformly into twelve intervals and compute the interval that $s$ belongs to. This task thus becomes a twelve-category problem and is optimized by minimizing the cross-entropy loss:
% [TODO: describe cross-entropy loss here using equation]. 
% $\hat{y}=\frac{e^{-x_i}}{\sum_{k=1}^12 e^{-x_k}}$
\vspace{-2mm}
\begin{equation}
L_{C E}=-\sum_{i=1}^{12} y_i \ln \hat{y_i}
\vspace{-2mm}
\end{equation}
where $\hat{y_i}=\frac{e^{-x_i}}{\sum_{k=1}^{12} e^{-x_k}}$ represents the predicted probability of $s$ belonging to $i^{th}$ interval, $x_i$ represents $i^{th}$ output logit of the model and $y_i \in\{0,1\}$ indicates whether $s$ belongs to $i^{th}$ interval.
We train two agents for 500 epochs, one with the proposed trajectory judgment task and one without it. 

As shown in Figure \ref{fig:layout}, the training cross-entropy loss almost does not decrease without being pre-trained with the trajectory judgment task. This indicates that this variant has not learned the layout reasoning ability at all. Pre-trained with the trajectory judgment task, the agent model drops the training cross-entropy loss rapidly and the test accuracy increases stably. Finally, the highest accuracy of the variant pre-trained with the trajectory judgment task reaches around 40\%, while the other variant is 10\% (nearly equal to $ \frac{1}{12}$).  These results verify that the trajectory judgment task facilitates learning layout reasoning ability, achieving substantial improvement. 



% Figure environment removed


% Figure environment removed


\section{Transferability Results to Other VLN Benchmarks}
\label{sec:supp-trans}
To better confirm the effectiveness of the our method, we evaluate the transferability of Lily on the other three VLN benchmarks, \ie RxR~\cite{RxR}, R4R~\cite{R4R} and SOON~\cite{SOON}. Specifically, we transfer the model trained on R2R to RxR and R4R and the model trained on REVERIE to SOON without fine-tuning. In Table~\ref{tab:other}, most of the results share the same trend as the results on the R2R and REVERIE dataset, \ie consistently surpassing the SOTA with large margins on val unseen split. These results demonstrate that our method can generalize well to different domains with varying complexity.


\begin{table}[h]
\centering
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{llcclcclcl}
\topline
\multirow{2}{*}{Methods} &  & \multicolumn{2}{c}{RxR} &  & \multicolumn{2}{c}{R4R} &  & \multicolumn{2}{c}{SOON}       \\ \cmidrule{3-4} \cmidrule{6-7} \cmidrule{9-10} 
                         &  & SR         & SPL        &  & SR         & SPL        &  & SR    & \multicolumn{1}{c}{SPL} \\ \midline
DUET~\cite{DUET}                    &  & 23.05      & 18.05       &  & 16.01       &13.20        &  & 2.83    & 2.09                    \\ Lily~(ours)                     &  & \textbf{27.20}      & \textbf{20.51}      &  & \textbf{20.76}      & \textbf{17.34}      &  & \textbf{5.72}       & \textbf{4.10}                    \\
\bottomline
\end{tabular}
}
\caption{Transfer results on RxR, R4R and SOON under val unseen split.}
\label{tab:other}
\end{table}
\vspace{-5mm}


\section{Qualitative Results}
\label{sec:supp-qualitive}
We also present some visualization examples of our Lily agent and the state-of-the-art agent Airbert on the R2R dataset. As shown in Figure~\ref{fig:layout_win}, given the instruction that asks the agent to go to a dining room, our Lily agent is able to arrive at the office more quickly than Airbert. We speculate that our Lily agent can be aware of the layout knowledge that an office is usually located in a room on either side of a hallway. Hence, our Lily agent goes straight to the hallway (the red arrow in step 4), while Airbert goes to an entryway connecting the door to the outside(the red arrow in step 4).
Our method also improves the understanding of the actions in an instruction. In Figure~\ref{fig:action_win}, our Lily agent can easily understand the instruction which asks it to  turn left~(the red arrow in step 4). However, when pre-trained with incorrect actions, Airbert feels confused about the action words and does not execute the ``Turn left'' in the instruction, and keeps going forward~(the red arrow in step 4), eventually not finding the kitchen.

\section{Potential Future Research and Social Impact}
\label{sec:supp-future}
Our method is still restricted to graph-based environments. In a real-world application, we may expect the agent actuates the action continuously. This requires us to build more continuous navigation trajectories for the pre-training dataset. Besides, with more powerful vision and language foundation models~\cite{BLIP, GPT4, cat}, the models used to construct the proposed dataset can be further improved as more precise and open-world. We can also increase the data diversity by adding richer video and instruction templates. 

In the future, agents can be able to actively learn some helpful skills by watching videos like us humans and then assist people with their jobs~(\eg delivering and cleaning), thereby reducing high training costs. However, data security can be an important issue. For some videos that humans keep secret or do not want agents to see, countermeasures should be taken to prevent agents from accessing such videos, otherwise, it may affect human survival one day.