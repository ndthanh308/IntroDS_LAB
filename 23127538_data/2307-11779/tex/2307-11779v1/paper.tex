%%%%%%%% GenLaw '23 SUBMISSION FILE; adpated from ICML '23 file %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{natbib}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[inline]{enumitem}
\usepackage{dirtytalk}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[inkscapearea=page]{svg}


\definecolor{orange}{rgb}{1,0.5,0}
\newcommand{\js}[1]{{\color{red}{[#1]}}} % COLOR NOTE (don't remove)

\newcommand{\nt}[1]{{\color{blue}{[#1]}}} % COLOR NOTE (don't remove)

% Green checkmark
\newcommand{\greencheck}{\color{green}\checkmark}

% Red cross
\newcommand{\redcross}{\color{red}\text{\ding{55}}}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{The Extractive-Abstractive Axis}

\begin{document}

\twocolumn[
\icmltitle{The Extractive-Abstractive Axis: \\ Measuring Content ``Borrowing" in Generative Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nedelina Teneva}{yyy} 
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Megagon Labs, Mountain View, CA, USA}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Nedelina Teneva}{nedteneva@gmail.com}
% $\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Generative language models produce highly \textit{abstractive} outputs by design, in contrast to \textit{extractive} responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing \& Attribution, we propose the the so-called \textit{Extractive-Abstractive axis} for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality. 
\looseness=-1
\end{abstract}
\section {Introduction}

The widespread adoption of Large Language Models (LLMs) has created many practical data governance challenges, among which Licensing \& Attribution has emerged  as a key one \cite {jernite2022data}. The interplay between generative language models and copyright law, the fair use doctrine and licensing requirements is of broad research and practical interest to legal practitioners, and increasingly, developers and users of LLMs. This topic is not new: content owners' rights have been of interest to the legal community since the inception of the web and the subsequent wide spread use of search engines \cite{travis2008opting}. Traditionally, search engines have been powered by information retrieval techniques, which take as input a user query and output a query answer by parsing out relevant paragraphs, sentences or phrases from a web-scale corpus of documents to produce an \textit{attributable extractive answer} to the query.

The advent of LLMs -- which \citet{liu2023evaluating} call \say{generative search engines} -- is leading to a paradigm shift from \textit{attributable extractive} question answering and summarization methodologies to increasingly \textit{abstractive} ones. To produce these abstractive responses, generative models \cite{bart, t5} synthesize information from multiple sources/text documents using sequence-to-sequence LLMs such that the generated answers may be highly abstractive or otherwise not readily attributable -- as they are in search engines -- to a specific content source such as a document on the web with a unique URI identifier \footnote{https://datatracker.ietf.org/doc/html/rfc3986} . Given this shift, we propose the \textit{Extractive--Abstractive axis} for quantifying the propensity of LLMs for content borrowing.
We highlight the need for relevant metrics, benchmarks and annotations and list some practical challenges in Section \ref{challenges}.

%as well as the current lack of scalable tools or standards for assessing the licensing terms of web content.
\vspace{-8pt}
\section {The Extractive--Abstractive Axis}
Being able to quantify a generative language model's extractiveness/abstractiveness level -- in other words where the model lies on what we call the \textit{Extractive-Abstractive axis} -- with respect to one or several sources (e.g., a text snippet, a web page or social media post), is necessary for evaluating whether (and how much) a generative AI application is using content from copyrighted or licensed sources.  Intuitively, LLM answers with high levels of content borrowing in the absence of proper attribution constitute a higher risk for copyright infringement. By way of a practical example: a news publisher would like to determine if their article was used for training a LLM without their permission. 
If the publisher had access to the LLM pre-training and fine tuning corpus they can examine each training document and compare it to their article. If the LLM is only commercially accessible through APIs (e.g., ChatGPT \cite{bubeck2023sparks}), the publisher may query it in an attempt to examine if its responses contain snippets from their article. Depending on the abstractiveness level of the responses, the publisher may be facing a rather complex prompting task while at the same time providing additional training information to an already potentially copyright-infringing LLM operator.

%The shift from search engines-based extractive answers to LLM-based abstractive ones poses new technical challenges related to the quantification of LLMs' \say{language/content borrowing}. 
Quantification along the Extractive-Abstractive axis is of practical use to content owners, the developers of generative language applications and third parties for several reasons: 
\begin{enumerate*}[label=\textbf{(\arabic*)}]
\item Being able to quantify the level of language/content borrowing will allows content owners or third parties such as   algorithmic auditors \cite{raji2019actionable} to quantify how prone a trained LLM  is to content borrowing. 
\item Such metrics will enable the designers of generative language-based applications to minimize their legal risks (e.g. copyright infringing)  by identifying highly extractive responses at inference/run time.
\item More generally, such tools  will also help organizations with the assessment of LLM related Licensing \& Attribution risks pre- and post-deployment or liability assessment of off-the-shelf tools such as black box LLMs.
\item In courts these metrics can potentially be used for quantifying if a LLM-generated text is substantially similar to copyrighted content (or derivative of such). %copyright infringement matters.
We may even imagine cases in which the empirical  propensity (measured on benchmarking datasets) of generative models for borrowing large amounts of content may also play a role in Licensing \&  Attribution matters.
% TODO Footnote: publishers: the 4th factor of copyright: the effect of the use upon the potential market for or value of the copyrighted work. 

%Being able to quantify a generative language model's extractiveness/abstractiveness level with respect to multiple documents or a specific source (e.g. a specific web page, content from a particular web domain, a platform such as Reddit, or other material), is necessary for evaluating whether a generative AI application is using content from copyrighted or licensed sources. By way of a practical example: a news publisher would like to determine if their article was used for training a LLM (the LLM caonly accessed through APIs) without their permission. 
%If the publisher had access to the LLM pre-training and fine tuning corpus they may be able to examine each training document and compare it to their article (e.g. comparing their unique URI identifier). Since the LLM does not allow such access, the publisher may query it in an attempt to examine if its responses contain snippets from their article. \textbf{Depending on how abstractive or extractive the LLM's responses are, the publisher's query task may be easier or harder.}
\end{enumerate*}
% https://haveibeentrained.com
\vspace{-8pt}
\section{Metrics, Datasets and Annotation Tasks}\label{metrics}
\textbf{Metrics}. Existing Natural Language Processing (NLP) tasks such as question answering, machine translation, extractive and abstractive summarization (see Appx. \ref{nlptasks} for definitions) use various automatic  metrics 
to measure the similarity between generated answers and the true \say{gold} answers. Some of these include (see  \citet{liu2023towards, fabbri2021summeval} for additional ones): 
%\vspace{-6pt}
%\begin{enumerate}[label=\textbf{(\alph*)}]\setlength\itemsep{0.1pt}
\textbf{(1)} token overlap metrics (e.g., ROUGE \cite{lin2004rouge} and BLEU \cite{papineni2002bleu}) compare the similarity between two texts based on the $n$-grams (contiguous sequence of tokens) overlap between them; 
\textbf{(2)} vector-based metrics such as BERTScore \cite{zhang2019bertscore} and BARTScore \cite{yuan2021bartscore} measure text sequence similarity based on text representations learnt by neural models; 
\textbf{(3)} metrics relying on the assumption that if two texts are similar, they should be able to address the same set of questions -- one example is QAEval \cite{qaeval}; 
    % \item \textbf{two stage methods} \nt{add}
\textbf{(4)} additional metrics such as $n$-grams ratio \cite{narayan2018don} or coverage \cite{newsroom} are used for measuring summarization quality.
     % ngram ratio: proportion of novel $n$-grapms in the reference summaries/gold  answers.
     % coverage: proportion of words in a summary that are part of an extractive fragment in the input document 
%\end{enumerate}
%\vspace{-6pt}

In principle, while some of the above mentioned NLP metrics can be repurposed to measure LLMs along the Extractive-Abstractive axis in matters related to Licensing \& Attribution, no empirical studies exist on this topic and there are no evaluation benchmarks to guide such analysis. Since these automatic metrics have not been previously applied in the context of Licensing\& Attribution, there are no empirical studies of whether  they correlate with content owners'  perception of Licensing \& Attribution.

\textbf{Datasets and Human Annotations}.
Like all NLP models, LLMs are evaluated with respect to the downstream \textit{user perception} of the answer quality -- see \citet  {rogers2023qa} for a review and taxonomy of the vast number of NLP datasets.
They are, however, not evaluated with respect to the \textit{content owners' perception} of how well their content is used for answering users' questions. Given LLM's propensity for content borrowing,  it is critical that the experience and rights of content owners are balanced with those of application users.

A simple approach for benchmarking content owners' perception of Licensing \& Attribution quality is by re-purposing existing NLP datasets.  For example, summarization tasks, which already rely heavily on human annotation, may be particularly well suited for benchmarking generative models along the Extractive-Abstractive axis.
% and described several datasets that have not yet been created. We higlight the need for an additional set of datasets geared towards  
%% Figure environment removed
% Figure environment removed
Currently, summarization tasks are benchmarked using human annotators who rate the generated summaries along dimensions such as summary
%\begin{enumerate*}[label=\textbf{(\arabic*)}] answer 
 relevance, fluency, coherence \cite{fabbri2020summeval} or the level of factual alignment between the summary and the underlying source text being summarized (e.g., answer  consistency \cite{fabbri2020summeval}, faithfulness \cite{ladhak2021faithful} and  factuality \cite {dreyer-tradeoff}).
%\end{enumerate*}
As an example, Figure \ref{extrabstr}A shows summaries with their example coherence scores.
These summarization benchmarks can be augmented with (legal) expert annotation of Licensing \& Attribution quality assessing whether, e.g. (1) the similarity between the input text and summary is acceptable, (2) there are copyright concerns, (3) any extractive snippets are properly attributed to the source.
%or the similarity with the original source is ) taking into account the absrtactive/extractive levels of the generated text, as well the attribution quality in cases where citations and references are integrated in the generated text. 
%We propose the development of guidelines geared towards assessing LLM's output from legal perspective. 
Some of these dimensions may be more easy to assess in a comparative manner, rather than individually as long as proper annotator agreement is established. Figure \ref{extrabstr}B illustrates this point with an example annotation question which ranks the 3 summaries in order of decreasing perceived similarity.

\section{Practical Challenges and Limitations}\label{challenges}

There are several practical challenges associated with measuring generative models along the Extractive-Abstractive axis which we categorize below.

\textbf {Evaluation Challenges}: Human evaluation, especially of longer answers, is a hard and actively studied research problem \cite{rogers2023qa}. Content Licensing \& Attribution nuances and expertise required can pose challenges to the human evaluation of the generated responses described in Section \ref{metrics} and Figure \ref{extrabstr}. Additionally, while this study focuses mainly on English language and we note that  content borrowing may be different in other languages.

\textbf{Usability Challenges and Conflicting Interests}: 
(1) Correlation between faithfulness/factuality and extractiveness  \cite {dreyer-tradeoff,ladhak2021faithful} observed in summarization tasks implies that a certain level of extractiveness may be needed in the generated answers in order to balance mis/disinformation concerns. This observation may heighten the necessity of measuring content borrowing in LLMs along the Extractive-Abstractive axis. 
(2) Interactions with LLMs can be used as additional training signals for the underlying LLM system so a practical challenge is how generative models can be audited for Licensing \& Attribution purposes without further aiding their development.

\textbf{Ethical Challenges}: Content borrowing poses numerous ethical challenges in addition to legal ones. In order to mitigate such challenges, incentives and broader policies may be needed in order to alleviate the concerns of both content owners and LLM end users. Adversarial scenarios also needs to be considered: LLMs can be  tuned for specific abstraction levels which means that copyrighted content used for pre-training and fine tuning can be intentionally obfuscated. In such cases, developing methodologies for identifying copyright infringement in black box LLMs becomes even more critical. 

%\textbf{In conclusion, we stress the need for appropriate metrics, datasets and annotation guidelines for studying the effects of generative models along the Extractive-Abstractive axis.}
% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
We would like to thank John Santerre for their input in the early stages of the project; Estevam Hruschka, Sajjadur Rahman and the anonymous reviewers for their draft feedback and suggestions. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}\label{appx}

\subsection{Figure Details}\label{figdetails}
The figure shows three summaries of the input text snippet shown at the top of the figure; each summary is of different degree of extractives/abstractiveness (measured by ROUGE score \cite{lin2004rouge}). Summaries $1$ and $3$ are reproduced with permission from \citet{dreyer-tradeoff}'s study. Summary $2$ was obtained by prompting ChatGPT (using the free plan) in May 2023 as follows: \say{Summarize this snippet: \say{The National Zoo's giant panda cub made his debut Wednesday in a five-minute explosion of cuteness confined to a live stream because of the coronavirus pandemic. The zoo is closed because of the pandemic and has not said when it will reopen.}}

For each summary in (A), we show 1) an example scoring of the coherence; and 2) automatically computed ROUGE-L score (recall) between the summary and the input using the \texttt{py-rouge} library (\href{https://pypi.org/project/py-rouge/}{https://pypi.org/project/py-rouge/}). 
Following  \citet{dreyer-tradeoff}, fragments extracted from the input are marked from red (longer fragments) to yellow (shorter fragments). 

\subsection{NLP Task Definitions}\label{nlptasks}
\textbf{Question Answering} refers to the task of answering asked by humans in natural language using either a pre-structured database or a collection of text documents (see \cite{soares2020literature} for a review). There are various  subtypes of question answering such as factoid question answering or  multiple choice question answering.

\textbf{Summarization} tasks aim produce summaries of single or multiple documents to answer questions that require longer responses. The goal is to convey the key information in the input text. In \textit{extractive} summarization, the summarizers identify the most important sentences in the input, which can be either a single document or a cluster of related documents, and string them together to form a summary \cite{nenkova2012survey}. In \textit{abstractive} summarization, the summary contains synthesized text which may not be explicitly present in the input text. 



% All of these tasks require large corpi of training documents and/or text snippets. As highlighted by \nt{ref} the underlying corpus of documents used is some combination of internet data (CommonCrawl), semi-curated sources such as Wikipedia and Reddit datasets, as well as proprietary data. 


% \subsection{Practical Challenges}\label{challenges}
% There are several practical challenges associated with measuring generative models along the Extractive-Abstractive axis:
% \begin{itemize}
% \item Human evaluation, especially of longer answers, is a hard and actively studied research problem \cite{rogers2023qa}. Content Licensing \& Attribution nuances may make the human evaluation of the generated responses described in Section \ref{metrics} and Figure \ref{extrabstr} even harder.
% 
% \item Correlation between faithfulness/factuality and extractiveness  \cite {dreyer-tradeoff,ladhak2021faithful} observed in summarization tasks implies that a certain level of extractiveness may be needed in the generated answers in order to balance mis/disinformation concerns. This observation may heighten the necessity of measuring content borrowing in LLMs along the Extractive-Abstractive axis. 
% \item Adversarial scenarios also needs to be considered: since the abstraction level of the generated answers can be tuned to intentionally, adversarial LLM operators may choose to hide the use of copyrighted content used for pre-training and fine tuning. In such cases, developing methodologies for identifying copyright infringement in black box LLMs becomes even more critical.
% 
% \item Interactions with LLMs can be used as additional training signals for the underlying system. Hence, a practical challenge is how generative models can be audited for Licensing \& Attribution purposes without further aiding their development.
% 
%\item Some practical challenges for the is the presence of blackbox training data (content owners's unique identifiers such as URIs are often removed in different pre-cleaning steps of large trainig corpi making it harder to etablish if their content was used at any step of the LLM pipeline. 

% Another challenge is that content owners need to access generative AI APIs that are behind paywalls to determine whether LLM agents are infringing on copyright in the process potentially providing additional training information to the copyright-infringing LLM agent. 

%\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
