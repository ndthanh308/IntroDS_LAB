\section{Introduction}
% Why would you research this (coarse-to-fine start with the general goal/problem and get more specific)?
The use of tools to achieve desired changes to the environment is a hallmark feature of human intelligence~\cite{Jamone2022, Wenke2019}.
This includes various behaviors, from using a vessel to carry water, to driving a nail with a hammer, to operating a power drill. 
While humans routinely use specialized tools for construction and assembly tasks, this behavior has been challenging to automate because of the high-dimensional action space of humanoids robots and the intra-class variability of the tools made for human hands.

% What is the open problem? (Classical vs RL, This level of complexity is to challenging for RL on its own.)
Despite these challenges, tool use remains a central task in robot learning, due to its overwhelming practical utility.
Classical approaches for tool use include affordance learning~\cite{Stoytchev2005, Montesano2007} and dynamic motion primitives~\cite{Kober2008, Muelling2010}.
These rely on predefined exploration primitives or trajectories and lack the interactive manipulation capabilities that humans so effortlessly exhibit.
Reinforcement learning (RL)~\cite{Rajeswaran2018,Wenke2019} has recently been used to generate interactive control policies, but suffers from the high-dimensional action space of human-like robotic hands. 
This leads to excessive sample complexity, if convergences can be achieved at all.
To enable RL to handle manipulation tasks in the intricacy of interactive tool use, auxiliary guidance such as demonstration datasets or precise reward engineering is needed~\cite{Rajeswaran2018}.

Using demonstrations to communicate the desired behavior to a robot is a intuitive approach since humans can provide competent demonstrations for anthropomorphic end-effectors. 
However, existing methods make only limited use of demonstrations.
Consider the task of grasping a hammer to drive a nail. 
To derive this general skill, regular imitation learning would necessitate a vast number of demonstrations spanning various tool instances.
Instead, we want our robot to use tools as flexibly as humans do, relating demonstrated behaviors to different instances without the need for repeated demonstrations.
While prior work considers intra-class variation of object instances~\cite{Rodriguez2020, Rodriguez2018}, the grasping of tools is framed as reaching a desired grasping position derived from an initial observation of the tool. 
This is in stark contrast to the way humans interact with tools and objects, where perception and action are continuously interleaved, making adaptive behaviors and operation in unstructured environments possible. 
Humans have the ability to effortlessly generalize prior knowledge and interactively adapt their behavior, enabling them to operate unfamiliar tools with ease.
While generalization to new tools and their interactive operation have been demonstrated individually, to the best of our knowledge, no prior method realizes both.

% What did we do to resolve it?
In this work, we present a system that learns a continuous control policy to operate a variety of tools under the guidance of only a single human demonstration. 
To this end, we introduce a procedure that utilizes non-rigid registration to generalize a canonical grasping demonstration to novel instances and use these demonstrations to guide policy search, without imposing rigid behaviors. 
This is achieved by initializing episodes in pre-grasp poses to enable efficient exploration and by inducing prior knowledge about how to grasp a tool through a shaped reward function.
Teh effectiveness of our proposed approach is experimentally evaluated on three simulated tool-use tasks.
Specifically, we make the following contributions:
\begin{itemize}
        \item We present a novel method that uses non-rigid registration to generalize grasp-configurations to unknown instances of a class.
        \item We examine how grasping demonstrations can be used to guide the learning of an RL policy.
        \item We demonstrate, in simulation, that interactive operation of different tools can be learned with model-free RL using only a single demonstration.
\end{itemize}