\documentclass[11pt]{article}
%\usepackage[cp1251]{inputenc}
\usepackage{amsmath,amsthm,mathrsfs,bm}
%\usepackage[english]{babel}

\usepackage[initials,alphabetic]{amsrefs}
\usepackage[utf8]{inputenc}
%\usepackage{amsrefs}


\usepackage[dvipsnames]{xcolor}

\usepackage{amsfonts,amstext,amssymb,verbatim,epsfig,psfrag,stmaryrd,extarrows}
%\usepackage{fourier}
\usepackage{pgf,tikz}
\usepackage{float}
\usetikzlibrary{arrows}
\usepackage{hyperref}
%\usepackage{cite}
%\usepackage{appendix}
%\usepackage{epstopdf}
\usepackage{color}
\usepackage{transparent}
\usepackage{subfigure}
\usepackage{prettyref}
%\usepackage{fancyref}
%\usepackage{cleveref}
\newrefformat{pr}{Proposition \ref{#1}}
\newrefformat{co}{Corollary \ref{#1}}
\newrefformat{le}{Lemma \ref{#1}}
\newrefformat{th}{Theorem \ref{#1}}
\newrefformat{ex}{Example \ref{#1}}
\newrefformat{re}{Remark \ref{#1}}
\newrefformat{se}{Section \ref{#1}}

\usepackage{esint}
%\usepackage[notcite,notref]{showkeys}
%\usepackage{showkeys}


\DeclareMathOperator*{\esssup}{ess\,sup}

\usepackage{bbm,mathrsfs}

\usepackage[normalem]{ulem}

%\hypersetup{colorlinks,urlcolor=blue}
\definecolor{forestgreen}{rgb}{0.1333,0.5451,0.1333}
\definecolor{navyblue}{rgb}{0,0,0.5}
\definecolor{darkgreen}{rgb}{0,0.3922,0}

%\hypersetup{colorlinks=true}
%\hypersetup{colorlinks=true,pdfborder=000,  citecolor  = teal, citebordercolor= {magenta}}
\hypersetup{colorlinks=true, citecolor  = teal, linkcolor=MidnightBlue}
%\hypersetup{colorlinks,urlcolor=blue}
\makeatletter
\let\reftagform@=\tagform@
\def\tagform@#1{\maketag@@@{(\ignorespaces\textcolor{black}{#1}\unskip\@@italiccorr)}}
\renewcommand{\eqref}[1]{\textup{\reftagform@{\ref{#1}}}}
\makeatother



% hack into hyperref
\makeatletter
%\xpatchcmd{\@lbibitem}
% {\item[\hfil}
% {\item[\hfil\color{red}}
% {}{}
%\makeatother


\sloppy %\addtolength\textwidth{9ex}
\def\R{{\mathbb{R}}}
\def\N{{\mathbb{N}}}
\def\Z{{\mathbb{Z}}}

\textheight=225mm \textwidth=167mm
\topmargin=-1.3cm
\oddsidemargin=-0.04cm
\evensidemargin=-0.0cm
\allowdisplaybreaks



% \DeclareUrlCommand\ULurl@@{%
%   \def\UrlFont{\ttfamily\color{teal}}%
%   \def\UrlLeft{\uline\bgroup}%
%   \def\UrlRight{\egroup}}
% \def\ULurl@#1{\hyper@linkurl{\ULurl@@{#1}}{#1}}
% \DeclareRobustCommand*\ULurl{\hyper@normalise\ULurl@}
% \makeatother
% \renewcommand{\citeleft}{\textcolor{purple}{[}}
% \renewcommand{\citeright}{\textcolor{purple}{]}}
% hack into hyperref

%\xpatchcmd{\@lbibitem}
% {\item[\hfil}
% {\item[\hfil\color{red}}
% {}{}
%\makeatother

%\makeatletter
%\DeclareUrlCommand\ULurl@@{%
%  \def\UrlFont{\ttfamily\color{purple}}%
%  \def\UrlLeft{\uline\bgroup}%
%  \def\UrlRight{\egroup}}
%\def\ULurl@#1{\hyper@linkurl{\ULurl@@{#1}}{#1}}
%\DeclareRobustCommand*\ULurl{\hyper@normalise\ULurl@}
%\makeatother



\def\betaox{\quad\hbox{\vrule \vbox{\hrule \vskip2pt \hbox{\hskip2pt
				\vbox{\hsize=1pt}\hskip2pt} \vskip2pt\hrule}\vrule}}
\def\lessim{\ \lower4pt\hbox{$
		\buildrel{\displaystyle <}\over\sim$}\ }
\def\gessim{\ \lower4pt\hbox{$\buildrel{\displaystyle >}
		\over\sim$}\ }
\def\n{\noindent}
\def\P{{\cal P}}
%\def\la{\langle}
%\def\ra{\rangle}
\def\si{\sigma}
\def\E{{\cal E}}
\def\LL{{\cal L}}
\def\FF{{\cal F}}
\def\SS{{\cal S}}
\def\C{{\cal C}}
\def\H{{\cal H}}
\def\X{{\cal X}}
\def\Y{{\cal Y}}
\def\A{{\cal A}}
\def\B{{\cal B}}
\def\vX{\boldsymbol{X}}
\def\vY{\boldsymbol{Y}}
\def\vZ{\boldsymbol{Z}}
\def\O{{\cal O}}
\def\M{{\cal M}}
\def\I{{\cal I}}
\def\eps{{\varepsilon}}
\def\ch{{\mbox{\rm ch}\hspace{0.4mm}}}
\def\sh{{\mbox{\rm sh}}}
\def\th{{\mbox{\rm th}}}
\def\Av{{\mbox{\rm{Av}}}}
\def\n{\noindent}
\def\betae{\vskip .15pc\hangindent=.5pc\hangafter=1}
\def\Bla{\Big{\langle}}
\def\Bra{\Big{\rangle}}
\def\bla{\langle}
\def\bra{\rangle}
\def\goin{\to\infty}
%\def\qed{\hfill\break\rightline{$\betaox$}}

\newcommand{\hp}{\hat{\mathcal{P}}}
\newcommand{\hg}{\Gamma}
\newcommand{\ow}{\overline{w}}
\newcommand{\ou}{\overline{u}}
\newcommand{\ov}{\overline{v}}
\newcommand{\e}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Natural}{\mathbb{N}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\F}{{\cal F}}
\newcommand{\veps}{\vec{\varepsilon}}
\newcommand{\vsi}{\boldsymbol{\sigma}}
\newcommand{\vrho}{{\boldsymbol{\rho}}}
\newcommand{\vbeta}{\vec{\beta}}
\newcommand{\vtau}{\boldsymbol{\tau}}
\newcommand{\veta}{\vec{\beta}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{z}}
\newcommand{\vw}{\boldsymbol{w}}
\newcommand{\vW}{\boldsymbol{W}}
\newcommand{\Exval}[1]{\mathbb{E}#1}
\newcommand{\prob}[1]{\mathbb{P}(#1)}
\newcommand{\Prob}[1]{\mathbb{P}\{#1\}}
\newcommand{\bigProb}[1]{\mathbb{P}\Bigl\{#1\Bigr\}}
\newcommand{\bigprob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\showcite}[1]{\ensuremath{\stackrel{\textnormal{#1}} {\textnormal{\cite{#1}}}}}
\newcommand{\indi}{\ensuremath{\mathbbm{1}}}

\newcommand{\Pp}{\mathcal{P}}
\newcommand{\s}{\boldsymbol{\sigma}}
\newcommand{\ii}{\iota}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bg}{\boldsymbol{\gamma}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\bb}{\boldsymbol{\beta}}
%\newcommand{\bm}{\boldsymbol{m}}
\newcommand{\bta}{\boldsymbol{\eta}}
\newcommand{\ct}{\mathcal{T}}
\newcommand{\jx}{\mathcal{J}}
\newcommand{\kx}{\mathcal{K}}
\newcommand{\qx}{\mathcal{Q}}
\newcommand{\pref}{\prettyref}
\newcommand{\ind}{\text{\rm ind}}
\newcommand{\goe}{\text{\rm GOE}}
\newcommand{\sig}{\sigma}
\newcommand{\semi}{\mu_{\text{\rm sc}}}

\newtheorem{lemma}{\bf Lemma}[section]

\newtheorem{theorem}[lemma]{\bf Theorem}
\newtheorem{corollary}[lemma]{\bf Corollary}
\newtheorem*{prediction}{\bf Prediction}
\newtheorem{example}{\bf Example}
\newtheorem{exercise}{\bf Exercise}
\newtheorem{proposition}[lemma]{\bf Proposition}
\newtheorem{question}{\bf Question}
\newtheorem{acknowledgements}{\bf Acknowledgements}
\theoremstyle{remark}

\newtheorem{remark}{Remark}[section]
\newtheorem{definition}[lemma]{\bf Definition}
\newtheorem{assumption}{\bf Assumption}

\renewcommand\theassumption{\Roman{assumption}}

\numberwithin{equation}{section}

\newcommand{\8}{\infty}
\newcommand{\td}{\widetilde}
%\mathchardef\dash="2D
\newcommand{\ee}{\mathsf{E}}

\newcommand{\ax}{\mathcal{A}}
\newcommand{\bx}{\mathcal{B}}
\newcommand{\cx}{\mathcal{C}}
\newcommand{\dx}{\mathcal{D}}
\newcommand{\ex}{\mathcal{E}}
\newcommand{\fx}{\mathcal{F}}
\newcommand{\gx}{\mathcal{G}}
\newcommand{\ix}{\mathcal{I}}
\newcommand{\lx}{\mathcal{L}}
\newcommand{\mx}{\mathcal{M}}
\newcommand{\nx}{\mathcal{N}}
\newcommand{\px}{\mathcal{P}}
\newcommand{\rx}{\mathcal{R}}
\newcommand{\ux}{\mathcal{U}}

\newcommand{\nz}{\mathbb{N}}
\newcommand{\rz}{\mathbb{R}}
\newcommand{\cz}{\mathbb{C}}
\newcommand{\ez}{\mathbb{E}}
\newcommand{\fz}{\mathbb{F}}
\newcommand{\tz}{\mathbb{T}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\pz}{\mathbb{P}}


\newcommand{\Ga}{\Gamma}
\newcommand{\Om}{\Omega}
\newcommand{\De}{\Delta}

\newcommand{\ds}{\mathscr{D}}
\newcommand{\ps}{\mathscr{P}}
\newcommand{\js}{\mathscr{J}}
\newcommand{\ns}{\mathscr{N}}
\renewcommand{\ss}{\mathscr{S}}

\newcommand{\sfG}{\mathsf G}
\newcommand{\sfF}{\mathsf F}
\newcommand{\sfH}{\mathsf H}
\newcommand{\sfI}{\mathsf I}
\newcommand{\sfL}{\mathsf L}
\newcommand{\sfJ}{\mathsf J}
\newcommand{\sfK}{\mathsf K}
\newcommand{\sfT}{\mathsf T}

\newcommand{\sfa}{\mathsf a}
\newcommand{\sfb}{\mathsf b}
\newcommand{\sfm}{\mathsf m}

\newcommand{\al}{\alpha}
\newcommand{\de}{\delta}
\renewcommand{\si}{\sigma}
\newcommand{\ga}{\gamma}
\newcommand{\la}{\lambda}
\renewcommand{\bt}{\beta}
\newcommand{\ta}{\theta}
\newcommand{\om}{\omega}

\newcommand{\Crt}{\mathrm{Crt}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\GOE}{\mathrm{GOE}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\ra}{\rangle}
\newcommand{\lan}{\langle}
\newcommand{\wt}{\widetilde}
\newcommand{\rmc}{\text{ {\rm c}} }

\newcommand{\argmin}{\mathrm{argmin}}




\newcommand{\dd}{\mathrm{d}}

\newcommand{\supp}{\operatorname{supp}}
\begin{document}
\title{On the expected number of critical points of \\ locally isotropic Gaussian random fields }
\author{Hao Xu\thanks{Department of Mathematics, University of Macau, yc17446@connect.um.edu.mo.} \and Haoran Yang\thanks{School of Mathematical Sciences, Peking University, yanghr@pku.edu.cn.} \and Qiang Zeng\thanks{Department of Mathematics, University of Macau, qzeng.math@gmail.com, research partially supported by SRG 2020-00029-FST and FDCT 0132/2020/A3.}}
\maketitle
\begin{abstract}
We consider locally isotropic Gaussian random fields on the $N$-dimensional Euclidean space for fixed $N$. Using the so called Gaussian Orthogonally Invariant matrices first studied by Mallows in 1961 which include the celebrated Gaussian Orthogonal Ensemble (GOE), we establish the Kac--Rice representation of expected number of critical points of non-isotropic Gaussian fields, complementing the isotropic case obatined by Cheng and Schwartzman in 2018. In the limit $N=\8$, we show that such a representation can be always given by GOE matrices, as conjectured by Auffinger and Zeng in 2020.
\end{abstract}

\section{Introduction}
 Locally isotropic random fields on the $N$-dimensional Euclidean space $\rz^N$ were introduced by Kolmogorov in 1941 \cites{Ko41} for the application in statistical theory of turbulence. Since then, this class of stochastic processes have been extensively studied in both physics and mathematics. In particular, locally isotropic Gaussian random fields have been used to model a particle confined in a random potential and serve as a toy model for the elastic manifold. For an incomplete list of literature, we refer the interested reader to \cites{MP91,En93,Fy04, FS07,FB08, FN12, FLD18, FLD20} for the background in physics and \cites{Kli12, AZ20, AZ22,BBMsd,BBMsd2, XZ22} for the mathematical development.

 The goal of this paper is two-fold. For applications to statistical physics, frequently we need to send $N\to\8$ for the thermodynamic limit, which puts additional restriction on the class of such Gaussian fields. Mathematically, it is also of interest to consider random fields on a finite dimensional Euclidean space. Guiding by this principle, Cheng and Schwartzman gave a representation of the expected critical points of isotropic Gaussian random fields on $\rz^N$ with fixed $N$ using the Kac--Rice formula \cites{CS18}. On the other hand, recently Auffinger and Zeng provided a detailed study on the landscape complexity of non-isotropic Gaussian random fields with isotropic increments \cites{AZ20,AZ22} after the seminal work of Fyodorov for the isotropic case \cites{Fy04}. These works exploit the large $N$ limit of the Kac--Rice representation of expected number of critical points. Here an apparent gap arises: What about the non-isotropic Gaussian fields on the fixed $\rz^N$? We provide several answers to this question. Furthermore, we show that a technical assumption used in \cites{AZ20} is redundant in the large $N$ limit as conjectured by the authors while it is useful to obtain a representation via matrices from the Gaussian Orthogonal Ensemble (GOE) in finite dimensions.

Let us be more precise. A locally isotropic Gaussian random field  $H_N=\left\{H_N(x): x \in \mathbb{R}^N\right\}$ is a centered Gaussian process indexed by $\rz^N$ that satisfies
\begin{align}\label{eq:def}
    \mathbb{E}\left[\left(H_{N}(x)-H_{N}(y)\right)^{2}\right]= D_N\left(\|x-y\|^{2}\right), \quad x, y \in \mathbb{R}^{N} .
\end{align}
Here the function $D_N: \mathbb{R}_{+} \rightarrow \mathbb{R}_{+}$ is called the structure function (or correlator) of $H_N$, $\|\cdot\|$ is the Euclidean norm and $\mathbb{R}_{+}=[0, \infty)$. $H_N$ is also known as a Gaussian random field with isotropic increments. The condition \pref{eq:def} determines the law of $H_{N}$ up to an additive shift by a Gaussian random variable. Following \cite{Ya87}, we recall some basic properties of the structure function. Let $\mathcal{D}_N$ denote the set of all $N$ dimensional structure functions and   $\mathcal{D}_{\infty}$ the set of structure functions which belong to $\mathcal{D}_N$ for any integer $N$. Since any $N$ dimensional structure function is necessarily an $M$ dimensional  structure function for all integers $M$ less than $N$, it is clear that
$$
\mathcal{D}_1 \supset \mathcal{D}_2 \supset \dots \supset \mathcal{D}_N \supset \dots \supset \mathcal{D}_{\infty},
$$
where the symbol $\supset$ denotes inclusion. In the following, we write $D_\8\in\mathcal{D}_{\infty}$ for the structure function of a field that can be defined on $\rz^N$ for all natural numbers $N\in\nz$ and in this case we frequently write $N=\8$.
Let us define
\begin{align*}
\Lambda_N(x) & =2^{(N-2) / 2} \Gamma\left(\frac{N}{2}\right) \frac{J_{(N-2) / 2}(x)}{x^{(N-2) / 2}} \\
& =1-\frac{x^2}{2 N}+\frac{x^4}{2 \cdot 4 \cdot N(N+2)}-\cdots \quad \text { for } N=2,3, \dots, \\
\Lambda_N(x) & =e^{-x^2} \quad \text { for } N=\infty ,
\end{align*}
where $J_N$ is the $N$th Bessel function of the first kind, which has the following representation
$$
J_N(x)=\sum_{m=0}^{\infty} \frac{(-1)^m}{m ! \Gamma(m+N+1)}\left(\frac{x}{2}\right)^{2 m+N}.
$$
From here it can be shown that $\Lambda_N(\sqrt{2N}x)\stackrel{N \rightarrow \infty}{\longrightarrow} e^{-x^2}$.
By \cites{Sch38, Ya57}, locally isotropic Gaussian random fields (or equivalently the class $\dx_N$) can be classified into two cases:

\begin{enumerate}
    \item Isotropic fields. There exists a function $B_N: \mathbb{R}_{+} \rightarrow \mathbb{R}$ such that
\begin{align}\label{Hcov}
\mathbb{E}\left[H_{N}(x) H_{N}(y)\right]= B_N\left(\|x-y\|^{2}\right),
\end{align}
where $B_N$ has the representation
\begin{align*}
B_N(r)=C_{N}+\int_{(0, \infty)} \Lambda_N(\sqrt{rt}) \nu_N(\mathrm{d} t),
\end{align*}
with a constant $C_{N} \in \mathbb{R}_{+}$ and a finite measure $\nu_N$ on $(0, \infty)$. Clearly, in this case we have
$
D_N(r)=2(B_N(0)-B_N(r)).
$ In particular, for the class $\dx_\8$, we write $B(r)=B_\8(r)$ and it can be represented as
$$
B(r)=C+\int_{(0, \infty)} e^{-r t} \nu(\mathrm{d} t).
$$


\item Non-isotropic fields with isotropic increments. The structure function $D_N$ can be written as
\begin{align}
\label{DNr}
D_N(r)=\int_{(0, \infty)}\left(1-\Lambda_N(\sqrt{rt})\right) \nu_N(\mathrm{d} t)+A_N r,
\end{align}
where $A_N \in \mathbb{R}_{+}$ is a constant and $\nu_N$ is a $\sigma$-finite measure with
$$
\int_{(0, \infty)} \frac{t}{1+t} \nu_N(\mathrm{d} t)<\infty.
$$
 For $N=\infty$, the structure function $D(r):=D_\8(r)$
has the following form
\begin{align}\label{Dr}
D(r)=\int_{(0, \infty)}\left(1-e^{-rt}\right) \nu(\mathrm{d} t)+A r,
\end{align}
which is indeed a Bernstein function with $D(0)=0$; see Theorem \ref{Bern} below.
\end{enumerate}
Let $E \subset \mathbb{R}$ and $T_N\subset\mathbb{R}^N$ be Borel subsets. If the random field is twice differentiable,  we define
\begin{align*}
\operatorname{Crt}_N\left(E, T_N\right) & =\#\left\{x \in T_N: \nabla H_N(x)=0,  H_N(x) \in E\right\}, \\
\operatorname{Crt}_{N, k}\left(E, T_N\right) & =\#\left\{x \in T_N: \nabla H_N(x)=0,  H_N(x) \in E, i\left(\nabla^2 H_N(x)\right)=k\right\},
\end{align*}
where $i\left(\nabla^2 H_N(x)\right)$ is the index (or the number of negative eigenvalues) of the Hessian $\nabla^2 H_N(x)$ and $k=0,\dots,N$.
Under some suitable smoothness conditions as in \cite{AT07}, Cheng and Schwartzman gave a representation of
$$\ez[\Crt_{N,k}([u,\8),T)]=?$$
for the \emph{isotropic fields} using the Kac--Rice formula  in \cite{CS18}, where $u\in[-\8,\8)$ and $T$ is a unit ball in $\rz^N$. A key ingredient in the representation is what the authors call the Gaussian Orthogonally Invariant (GOI) matrix, which was first studied by Mallows \cite{Ma61}. Following their terminology, an $N \times N$ real symmetric random matrix $M$ is said to be Gaussian Orthogonally Invariant (GOI) with covariance parameter $c$, denoted by $\operatorname{GOI}(c)$, if its entries $M_{i j}, 1\leq i,j\leq N$ are centered Gaussian variables such that
$$
\mathbb{E}\left[M_{i j} M_{k l}\right]=\frac{1}{2}\left(\delta_{i k} \delta_{j l}+\delta_{i l} \delta_{j k}\right)+c \delta_{i j} \delta_{k l}.
$$
Note that the distribution of such matrices are invariant under the congruence transformation by any orthogonal matrix. Therefore, they share some properties of the GOE matrices.
In particular, if $c=0$ then GOI$(0)$ matrices are exactly GOE matrices. Recall that a Gaussian vector is nondegenerate if and only if the covariance matrix of the Gaussian vector is positive definite. Cheng and Schwartzman showed that a GOI$(c)$ matrix of size $N$ is nondegenerate if and only if $c>-\frac{1}{N}$. Moreover, for the nondegenerate GOI$(c)$ matrix, they derived that the density of the ordered eigenvalues is given by
\begin{align}\label{eq:goidens}
f_c\left(\lambda_1, \ldots, \lambda_N\right)= & \frac{1}{K_N \sqrt{1+N c}} \exp \left\{-\frac{1}{2} \sum_{i=1}^N \lambda_i^2+\frac{c}{2(1+N c)}\left(\sum_{i=1}^N \lambda_i\right)^2\right\}\nonumber \\
& \times \prod_{1 \leq i<j \leq N}\left|\lambda_i-\lambda_j\right| \mathbf{1}{\left\{\lambda_1 \leq \cdots \leq \lambda_N\right\}},
\end{align}
where $K_N=2^{N / 2} \prod_{i=1}^N \Gamma\left(\frac{i}{2}\right)$ is the normalization constant. For $c\neq0$, one may write a GOI($c$) matrix as a GOE matrix plus a random scalar matrix; if $c>0$ the scalar matrix is independent of the GOE matrix,  while if $c<0$ the scalar matrix is no longer independent. In the limit $N\to\8$ or equivalently for correlators $B\in\dx_\8$, the covariance parameter $c$ is nonnegative and thus for these correlators and all dimensions $N\in\nz$ the Kac--Rice representation of $\ez[\Crt_{N,k}(E,T_N)]$ can always be given by GOE matrices. This was the setting in the pioneering works of Fyodorov and Nadal \cites{Fy04, FN12} where the GOE matrices are crucial for asymptotic analysis. However, if one insists on considering correlators from $\dx_N$ for fixed $N$, then additional restriction is needed in order to get a GOE matrix (plus an independent scalar matrix) in the Kac--Rice representation.

Before illustrating similar problems for the non-isotropic Gaussian fields, let us first put a remark on the relationship between the two cases. In general, the two types of Gaussian random fields are quite different, like the Ornstein--Uhlenbeck process and Brownian motion in dimension 1. But if $\nu_N$ is a finite measure in case 2, then the non-isotropic field is essentially a shifted isotropic field. Indeed, let $H_N(x)$ be an isotropic field that satisfies  $\mathbb{E}\left[H_{N}(x) H_{N}(y)\right]=\frac{1}{2}\int_{(0, \infty)} \Lambda_N(\sqrt{\|x-y\|^{2}t}) \nu_N(\mathrm{d} t)$. Then we can verify that $\widehat{H}_N(x):=H_N(x)-H_N(0)$ satisfies
\begin{equation}\label{eq:hnxy}
\mathbb{E}\left[\left(\widehat{H}_{N}(x)-\widehat{H}_{N}(y)\right)^{2}\right]= \int_{(0, \infty)}\left(1-\Lambda_N(\sqrt{\|x-y\|^{2}t})\right) \nu_N(\mathrm{d} t)
\end{equation} and $\widehat{H}_N(0)=0$, which means that $\widehat{H}_N(x)$ is a non-isotropic Gaussian  random field with isotropic increments. Conversely, let $\widehat{H}_N(x)$ be a non-isotropic field satisfying \pref{eq:hnxy} with some finite measure $\nu_N$. Define
$H_N(x)=\widehat{H}_N(x)+H_N(0)$, where
$H_N(0)$ is a Gaussian random variable with mean zero, variance $\ez[H_N(0)^2]=\frac{1}{2}|\nu_N|$ and
$$\mathbb{E}\left[\widehat{H}_{N}(x) H_{N}(0)\right]=\frac{1}{2}\int_{(0, \infty)}\left(\Lambda_N(\sqrt{\|x\|^{2}t})-1\right) \nu_N(\mathrm{d} t).$$
One can check that $H_N(x)$
is an isotropic Gaussian random field. Due to the relation $H_N(x)-H_N(0)=\widehat{H}_N(x)$, the non-isotropic field $\widehat{H}_N$ has the same critical points and landscape as those of the isotropic field $H_N$.
Furthermore, let
$\widetilde{H}_N(x)$ be a non-isotropic Gaussian random field satisfying
\begin{equation*}
\mathbb{E}\left[\left(\widetilde{H}_{N}(x)-\widetilde{H}_{N}(y)\right)^{2}\right]= \int_{(0, \infty)}\left(1-\Lambda_N(\sqrt{\|x-y\|^{2}t})\right) \nu_N(\mathrm{d} t)+A_N \|x-y\|^{2},
\end{equation*}
with some finite measure $\nu_N$ and constant $A_N >0$. In this case, we can split $\widetilde{H}_{N}(x)$ into two independent non-isotropic parts $\widetilde{H}_{N}(x)=\widehat{H}_{N}(x)+\overline{H}_{N}(x)$, where
$\widehat{H}_{N}$ satisfies
\pref{eq:hnxy} and $\overline{H}_{N}(x)$ satisfies
$$\mathbb{E}\left[\left(\overline{H}_{N}(x)-\overline{H}_{N}(y)\right)^{2}\right]=A_N \|x-y\|^{2}.$$
Note that we may write $\overline{H}_{N}(x)= \langle x,\xi\rangle$ for a centered Gaussian random vector $\xi=(\xi_1,\xi_2,\dots,\xi_N)$ with covariance matrix $A_N\mathbf{I}_N$, where $\langle \cdot,\cdot\rangle$ is the Euclidean inner product and $\mathbf{I}_{N}$ is the $N\times N$ identity matrix. In this case, the field $\overline{H}_N$ is almost trivial since its gradient is a fixed random vector and we have $\nabla\widetilde{H}_{N}(x)=0$ if and only if  $\nabla \widehat{H}_{N}(x)=-\xi$.
Therefore, when we talk about the non-isotropic fields, we may assume that $\lim_{r\to\8} D(r)=\8$ and the measure $\nu_N$
is $\sigma$-finite but not finite.

For the non-isotropic fields with isotropic increments, it is expected that the above representation problem is more challenging since the distribution of the field $H_N(x)$ depends on the location variable $x\in \rz^N$.
Recently, Auffinger and Zeng considered the Kac--Rice representation of $\ez\operatorname{Crt}_{N, k}\left(E, T_N\right)$ for the non-isotropic Gaussian fields with isotropic increments in the limit $N\to\8$ during their study of the landscape complexity  in \cites{AZ20,AZ22}, where GOE matrices are indispensable in the analysis as the isotropic case. This left the representation problem open for the fixed finite dimensions. Moreover, in order to utilize GOE matrices, they assumed a technical condition (see Assumption \ref{assumption3} below) and verified it for some special cases or subclass of $\dx_\8$. They conjectured that this condition always holds for all structure functions belonging to $\dx_\8$.

This paper aims to resolve these issues for the non-isotropic Gaussian fields with isotropic increments. In Section \ref{sec2}, we give various representations of $\ez\operatorname{Crt}_{N, k}\left(E, T_N\right)$ using GOI($c$) matrices. The basic tool is the Kac--Rice formula as usual. Under Assumption \ref{assumption3} or in the special case of $E=\rz$, we may reduce the GOI($c$) matrices to GOE matrices in the representation. These results can be regarded as the non-isotropic analog of those for the isotropic Gaussian fields in \cite{CS18}. In Section \ref{sec3}, we show that for the structure functions in $\dx_\8$ it is always possible to use GOE matrices to represent $\ez\operatorname{Crt}_{N, k}\left(E, T_N\right)$ when $T_N$ is a shell domain. In other words, the aforementioned technical condition is redundant as conjectured in \cite{AZ20}.


%\subsection*{Acknowledgements}


\section{Representations for non-isotropic Gaussian fields on $\rz^N$} \label{sec2}


\begin{comment}
Recall that an $N\times N$ matrix $M$ is taken  from the Gaussian Orthogonal Ensemble (GOE), i.e., $M$ is a real symmetric matrix whose entries $M_{i j}, i \leq j$ are independent centered Gaussian random variables with variance
$$
\mathbb{E} M_{i j}^{2}=\frac{1+\delta_{i j}}{2 } .
$$
 We also write $\GOE_N$ for an $N\times N$ GOE matrix.

From \cite{Me04}, the density of the ordered eigenvalues of $\GOE_N$ is given by
\begin{align}\label{eq:dens}
f\left(\lambda_1, \ldots, \lambda_N\right)=\frac{1}{K_N} \exp \left\{-\frac{1}{2} \sum_{i=1}^N \lambda_i^2\right\} \prod_{1 \leq i<j \leq N}\left|\lambda_i-\lambda_j\right| \mathbf{1}{\left\{\lambda_1 \leq \cdots \leq \lambda_N\right\}},
\end{align}
where $K_N$ is the normalization constant
\begin{align}\label{KN}
K_N=2^{N / 2} \prod_{i-1}^N \Gamma\left(\frac{i}{2}\right).
\end{align}
We denote the expectation under the GOE eigenvalues density (\ref{eq:dens}) by $\mathbb{E}_{\mathrm{GOE}}^N$, that is, for a measurable function $g$,
$$
\mathbb{E}_{\mathrm{GOE}}^N\left[g\left(\lambda_1, \ldots, \lambda_N\right)\right]=\int_{\mathbb{R}^N} g\left(\lambda_1, \ldots, \lambda_N\right) f\left(\lambda_1, \ldots, \lambda_N\right) \dd \lambda_1 \cdots \dd \lambda_N.
$$
Based on GOE,
Similarly, denote the expectation under the $\mathrm{GOI}(c)$ eigenvalues density (\ref{eq:goidens}) by $\mathbb{E}_{\mathrm{GOI}(c)}^N$, that is, for a measurable function $g$,
$$
\mathbb{E}_{\mathrm{GOI}(c)}^N\left[g\left(\lambda_1, \ldots, \lambda_N\right)\right]=\int_{\mathbb{R}^N} g\left(\lambda_1, \ldots, \lambda_N\right) f\left(\lambda_1, \ldots, \lambda_N\right) \dd \lambda_1 \cdots \dd \lambda_N.
$$
\end{comment}

\subsection{A perturbed GOI matrix model}

For clarity of exposition, we introduce a random matrix model which is a special perturbation of the GOI model. We call an $N\times N$ real symmetric random matrix $M$ Spiked Gaussian Orthogonally Invariant (SGOI) with parameters $d_1,d_2$ and $d_3$, denoted by $\operatorname{SGOI}(d_1,d_2,d_3)$, if its entries $M_{i j}, 1\leq i,j\leq N$ are centered Gaussian variables such that
\begin{align}\label{copgoi}
\mathbb{E}\left[M_{i j} M_{k l}\right]=\frac{1}{2}\left(\delta_{i k} \delta_{j l}+\delta_{i l} \delta_{j k}\right)+d_1 \delta_{i j} \delta_{k l}+d_2(\delta_{i 1}\delta_{ j1}\delta_{k l}+\delta_{k1}\delta_{l1}\delta_{i j})+d_3\delta_{i 1}\delta_{ j1}\delta_{k1}\delta_{l1}.
\end{align}
Clearly, if $d_2=0$ and $d_3=0$, the $\operatorname{SGOI}(d_1,0,0)$ matrix is exactly a $\operatorname{GOI}(c)$ matrix with $c=d_1$.
 %The following lemma gives the nondegenerate condition of the $\operatorname{SGOI}(d_1,d_2,d_3)$ matrix.
\begin{lemma}\label{NonPGOI}
Let $M$ be an $N\times N$ $\operatorname{SGOI}(d_1,d_2,d_3)$ matrix. Then $M$ is nondegenerate if and only if $d_1>-\frac{1}{N-1}$ and $1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}>0$.
\end{lemma}
\begin{proof}
Since $M$ is symmetric and the diagonal entries are independent of the off-diagonal entries, $M$ is nondegenerate if and only if the vector of diagonal entries $(M_{11},M_{22},\dots,M_{NN})$ is nondegenerate. By (\ref{copgoi}), the covariance matrix of the vector $(M_{11},M_{22},\dots,M_{NN})$ is
\begin{align}\label{Sigma}
\textbf{$\mathbf{\Sigma}$}=
    \begin{pmatrix}
      1+d_1+2d_2+d_3& d_1+d_2
      & \dots & d_1+d_2 \\
       d_1+d_2  &  d_1+1 & \dots & d_1 \\
      \vdots & \vdots &\ddots  & \vdots \\d_1+d_2 & d_1& \dots &  d_1+1 \\
    \end{pmatrix}.
  \end{align}
  It is straightforward to check that the lower right $(N-1)\times (N-1)$ submatrix of $\mathbf{\Sigma}$ is positive definite if and only if $d_1>-\frac{1}{N-1}$. Note that $\mathbf{\Sigma}$ is positive definite if and only if the determinant of $\mathbf{\Sigma}$ is large than $0$ and the lower right $(N-1)\times (N-1)$ submatrix of $\mathbf{\Sigma}$ is positive definite. By the Schur complement formula, one can show that $\det\mathbf{\Sigma}=1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}$, which gives the desired result.
\end{proof}

%For an $N\times N$ $\operatorname{SGOI} (d_1,d_2,d_3)$ matrix $M$, if the parameters satisfy the nondegenerate condition in Lemma \ref{NonPGOI}, we give the details of how to construct $M$.

Let $M$ be an $N\times N$ $\operatorname{SGOI} (d_1,d_2,d_3)$ matrix. In general, $M$ can be represented as the following form
\begin{align}\label{M}
   M =\begin{pmatrix}
      \zeta_1& \xi^\mathsf T \\
       \xi & \operatorname{GOI}(d_1)
    \end{pmatrix}=
    \begin{pmatrix}
      \zeta_1& \xi^\mathsf T \\
       \xi & \GOE_{N-1}+\zeta_2\mathbf{I}_{N-1}
    \end{pmatrix} ,
\end{align}
where $\xi$ is a centered column Gaussian vector with covariance matrix $\frac{1}{2}\mathbf{I}_{N-1}$, which is independent of $\zeta_1,\zeta_2$ and the $(N-1) \times (N-1)$ GOE matrix $\GOE_{N-1}$. If $d_1\geq 0$, we set $\zeta_2\sim N(0,d_1)$, which is independent of $\GOE_{N-1}$; and we take $\zeta_1\sim N(0,1+d_1+2d_2+d_3)$ with
  $$\mathrm{Cov}(\zeta_1,\zeta_2)=
\frac{d_1^2+d_1d_2}{1+d_1},\quad \mathbb{E}[\zeta_1\GOE_{N-1}]=\frac{d_1+d_2}{1+d_1}\mathbf{I}_{N-1}.$$
If $-\frac{1}{N-1}<d_1<0$, then we set $\zeta_2\sim N(0,-d_1)$ with $\mathbb{E}[\zeta_2\GOE_{N-1}]=d_1\mathbf{I}_{N-1}$;  we also require $\zeta_1\sim N(0,1+d_1+2d_2+d_3)$ such that $\zeta_1$ is independent of $\zeta_2$ and
  $$ \mathbb{E}[\zeta_1\GOE_{N-1}]=(d_1+d_2)\mathbf{I}_{N-1}.$$
One can verify that this construction is proper and we always have $$\mathbb{E}[\zeta_1(\GOE_{N-1}+\zeta_2\mathbf{I}_{N-1})]=(d_1+d_2)\mathbf{I}_{N-1}.$$

Note that the distribution of SGOI matrices is not invariant under the orthogonal congruence transformation, which makes it hard to deduce the joint eigenvalue density for this matrix model. Fortunately, it is sufficient to use the joint eigenvalue density of GOI matrices \pref{eq:goidens} for our representation formulas below.
By the conditional distribution of Gaussian vectors,  we have \begin{align}\label{goicond}
    \mathbb{E}(\GOE_{N-1}+\zeta_2\mathbf{I}_{N-1}\mid \zeta_1=y)=&\frac{(d_1+d_2)y}{1+d_1+2d_2+d_3}\mathbf{I}_{N-1},\nonumber\\
   \left( \GOE_{N-1}+\zeta_2\mathbf{I}_{N-1}\mid \zeta_1=y\right)\stackrel{d}{=}&\frac{(d_1+d_2)y}{1+d_1+2d_2+d_3}\mathbf{I}_{N-1}+M'.
\end{align}  where $M'$ is an $(N-1)\times (N-1)$ GOI$(c)$ matrix with parameter $c=\frac{d_1+d_1d_3-d_2^2}{1+d_1+2d_2+d_3}$.





\begin{comment}
\begin{lemma}
Let $M$ be an $N\times N$ $\operatorname{SGOI}(d_1,d_2,d_3)$ with the parameters satisfying the nondegenerate condition in Lemma \ref{NonPGOI}. Then the density function of the ordered eigenvalues $\lambda_1 \leq \cdots \leq \lambda_N$ of $M$ can be written as the following form
\begin{align*}
   f\left(\lambda_1, \ldots, \lambda_N\right)=&\frac{\prod_{1 \leq i<j \leq N}\left|\lambda_i-\lambda_j\right| \mathbf{1}_{\left\{\lambda_1 \leq \cdots \leq \lambda_N\right\}}}{K_N\sqrt{1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}}}\exp \left\{-\frac{1}{2}\sum_{i=1}^N \lambda_i^2+\frac{1}{2}d_1'\left(\sum_{i=1}^N \lambda_i\right)^2\right\} \\ &\times  \int \exp \left\{d_2'(U^\mathsf T M U)_{11}\left(\sum_{i=1}^N \lambda_i\right)+\frac{1}{2}d_3'(U^\mathsf T MU)_{11}^2\right\}d m_N(U),
\end{align*}
where $m_N$ is the Haar probability measure on $\mathcal{O}_N$ the orthogonal group of size $N$ and the parameters $d_1',d_2', d_3'$ depending on $d_1,d_2, d_3$ are given by
\begin{align}\label{ddd}
    d_1'&=\frac{d_1+d_1d_3-d_2^2}{1+2d_2+d_3+Nd_1+(N-1)d_1d_3-(N-1)d_2^2},\nonumber\\
d_2'&=\frac{d_2+d_2^2-d_1d_3}{1+2d_2+d_3+Nd_1+(N-1)d_1d_3-(N-1)d_2^2},\nonumber\\
d_3'&=\frac{Nd_1d_3+d_3-Nd_2^2}{1+2d_2+d_3+Nd_1+(N-1)d_1d_3-(N-1)d_2^2}.
\end{align}
\end{lemma}
\begin{proof}
For any fixed orthogonal matrix $U$, we know that the random matrix $M'=UMU^\mathsf T$ has the same eigenvalues with $M$, which means $M'$ and $M$ have the same density function of the eigenvalues. For $i,j\leq N$, it is clear that $M'_{ij}=\sum_{k=1}^N\sum_{l=1}^N U_{ik}M_{kl}U_{jl}.$
By (\ref{copgoi}), we obtain the covariance between the entries of $M'$
\begin{align}\label{cosgoi}
\mathbb{E}\left[M'_{i j} M'_{k l}\right]=\frac{1}{2}\left(\delta_{i k} \delta_{j l}+\delta_{i l} \delta_{j k}\right)+d_1 \delta_{i j} \delta_{k l}+d_2(U_{i 1}U_{ j1}\delta_{k l}+U_{k1}U_{l1}\delta_{i j})+d_3U_{i 1}U_{ j1}U_{k1}U_{l1}.
\end{align}
Let us denote a new vector Vec$(M')$ based on matrix $M'$, which takes the diagonal and above diagonal entries of $M'$, i.e.,
$$
\operatorname{Vec}(M')=\left(M'_{11}, \ldots, M'_{N N}, M'_{i j}, 1 \leq i < j \leq N\right)^T.
$$
Let $w,v$ be two $\frac{N(N+1)}{2}$ dimensional column vector  such that
$w^\mathsf T=(\mathbf{1}_N^\mathsf T,0_{\frac{N(N-1)}{2}}^\mathsf T)$ and $$v^\mathsf T=(U_{i1}U_{j1})_{1\leq i\leq j\leq N}=(U_{11}^2,U_{21}^2,\dots,U_{N1}^2,U_{11}U_{21},U_{11}U_{31},\dots,U_{N-1,1}U_{N,1}).$$
Therefore, by(\ref{cosgoi}), the covariance matrix of Vec$(M')$ can be represented as
\begin{align}
   \mathbf{\Sigma}'& =
    \begin{pmatrix}
      \mathbf{I}_N& 0\\
      0 & \frac{1}{2}\mathbf{I}_{N(N-1)/2}
\end{pmatrix}+d_1ww^\mathsf T+d_2(vw^\mathsf T+wv^\mathsf T)+d_3vv^\mathsf T\nonumber\\
&=\begin{pmatrix}
      \mathbf{I}_N& 0\\
      0 & \frac{1}{2}\mathbf{I}_{N(N-1)/2}
\end{pmatrix}+(w,v,w,v) \begin{pmatrix}
      d_1&0&0&0 \\
      0&d_3&0&0\\
      0&0&d_2&0\\
      0&0&0&d_2
\end{pmatrix}\begin{pmatrix}
      w^\mathsf T\\
     v^\mathsf T
     \\v^\mathsf T
     \\w^\mathsf T
\end{pmatrix}\nonumber\\
&=:A+WDV.
\end{align}
The Woodbury matrix identity implies that
\begin{align}\label{Siginv}
  \mathbf{\Sigma'}^{-1}=A^{-1} -A^{-1}W(D^{-1}+VA^{-1}W)^{-1}VA^{-1}.
\end{align}
Since $A^{-1}=\begin{pmatrix}
      \mathbf{I}_N& 0\\
      0 & 2\mathbf{I}_{N(N-1)/2}
\end{pmatrix},$  we deduce that
\begin{align}
VA^{-1}W=\begin{pmatrix}
      w^\mathsf T\\
     v^\mathsf T
     \\v^\mathsf T
     \\w^\mathsf T
\end{pmatrix}\begin{pmatrix}
      \mathbf{I}_N& 0\\
      0 & 2\mathbf{I}_{N(N-1)/2} \end{pmatrix}(w,v,w,v)
      =\begin{pmatrix}
      w^\mathsf T\\
    \widetilde{v}^\mathsf T
     \\\widetilde{v}^\mathsf T
     \\w^\mathsf T
\end{pmatrix} (w,v,w,v),
\end{align}
where $\widetilde{v}^\mathsf T=(U_{11}^2,U_{21}^2,\dots,U_{N1}^2,2U_{11}U_{21},2U_{11}U_{31},\dots,2U_{N-1,1}U_{N,1}).$
Note that $ w^\mathsf T w=N$, $ w^\mathsf T v=1$, $\widetilde{v}^\mathsf T w=1$ and $\widetilde{v}^\mathsf T v=1$. Therefore, we arrive at
\begin{align}
 VA^{-1}W= \begin{pmatrix}
      N&1&N&1 \\
      1&1&1&1\\
      1&1&1&1\\
      N&1&N&1
\end{pmatrix}=\begin{pmatrix}
      1&1 \\
      1&0\\
      1&0\\
      1&1
\end{pmatrix}\begin{pmatrix}
      1&0 \\
     0&N-1
\end{pmatrix}\begin{pmatrix}
      1&1&1&1 \\
      1&0&1&0
\end{pmatrix}.
\end{align}
Using the Woodbury matrix identity again, we have
\begin{align}\label{fourinv}
&(D^{-1}+VA^{-1}W)^{-1}\nonumber\\&=D-D\begin{pmatrix}
      1&1 \\
      1&0\\
      1&0\\
      1&1
\end{pmatrix}\left(\begin{pmatrix}
      1&0 \\
      0&\frac{1}{N-1}
\end{pmatrix}+\begin{pmatrix}
      1&1&1&1 \\
      1&0&1&0
\end{pmatrix}D\begin{pmatrix}
      1&1 \\
      1&0\\
      1&0\\
      1&1
\end{pmatrix} \right)^{-1}\begin{pmatrix}
      1&1&1&1 \\
      1&0&1&0
\end{pmatrix}D.
\end{align}
Since for an invertible $2\times 2$ matrix $\begin{pmatrix}
      a&b \\
      c&d
\end{pmatrix}$, we have
$\begin{pmatrix}
      a&b \\
      c&d
\end{pmatrix}^{-1}=\frac{1}{ad-bc}$$\begin{pmatrix}
      d&-b \\
      -c&a
\end{pmatrix}$, which gives
\begin{align}\label{twoinv}
    &\left(\begin{pmatrix}
      1&0 \\
      0&\frac{1}{N-1}
\end{pmatrix}+\begin{pmatrix}
      1&1&1&1 \\
      1&0&1&0
\end{pmatrix}D\begin{pmatrix}
      1&1 \\
      1&0\\
      1&0\\
      1&1
\end{pmatrix} \right)^{-1}\nonumber\\&=\frac{1}{(1+d_1+2d_2+d_3)(d_1+\frac{1}{N-1})-(d_1+d_2)^2}\begin{pmatrix}
      d_1+\frac{1}{N-1}&-(d_1+d_2) \\-
      (d_1+d_2)&1+d_1+2d_2+d_3  \end{pmatrix}.
\end{align}
Plugging (\ref{twoinv}) into
(\ref{fourinv}) implies
\begin{align}
 &(D^{-1}+VA^{-1}W)^{-1}= \frac{1}{1+2d_2+d_3+Nd_1+(N-1)d_1d_3-(N-1)d_2^2}\nonumber\\&\times
 \begin{pmatrix}\label{woodinv}
      d_1x_{d}&-d_1d_3(1-(N-1)d_2)&-d_1d_2(N+(N-1)d_3)&-d_1d_2(1-(N-1)d_2) \\
      -d_1d_3(1-(N-1)d_2)&d_3x_{d}&-d_2d_3(1-(N-1)d_2)&-d_2d_3(1+(N-1)d_1)\\
      -d_1d_2(1-(N-1)d_2)&-d_2d_3(1+(N-1)d_1)&d_2y_{d}&-d_2^2(1+(N-1)d_1)\\
      -d_1d_2(N+(N-1)d_3)&-d_2d_3(1-(N-1)d_2)&-d_2^2(N+(N-1)d_3)&d_2y_{d},
\end{pmatrix}
\end{align}
where $x_{d}=1+2d_2+d_3-(N-1)d_2^2$ and $y_d=1+d_2+d_3+Nd_1+(N-1)d_1d_3$.
On the one hand, we note that
\begin{align}\label{invAW}
    A^{-1}W&=\begin{pmatrix}
      \mathbf{I}_N& 0\\
      0 & 2\mathbf{I}_{N(N-1)/2}
\end{pmatrix}(w,v,w,v)=(w,\widetilde{v},w,\widetilde{v}),\nonumber\\
    VA^{-1}&=\begin{pmatrix}
      w^\mathsf T\\
     v^\mathsf T
     \\v^\mathsf T
     \\w^\mathsf T
\end{pmatrix}\begin{pmatrix}
      \mathbf{I}_N& 0\\
      0 & 2\mathbf{I}_{N(N-1)/2}
\end{pmatrix}=\begin{pmatrix}
      w^\mathsf T\\
     \widetilde{v}^\mathsf T
     \\ \widetilde{v}^\mathsf T
     \\w^\mathsf T
\end{pmatrix}.
\end{align}
On the other hand, one can check that
\begin{align}\label{trace}
\operatorname{Vec}(M')^\mathsf T\widetilde{v}&=\sum_{i=1}^N\sum_{j=1}^N U_{i1}U_{j1}M'_{ij}=(U^\mathsf T M'U)_{11},\nonumber\\&
\operatorname{Vec}(M')^\mathsf T w=tr(M').
\end{align}
Combining
(\ref{Siginv}), (\ref{woodinv}), (\ref{invAW}) and (\ref{trace}) yields that $$\operatorname{Vec}(M')^T \Sigma'^{-1} \operatorname{Vec}(M')=\operatorname{tr}\left(M'^2\right)-d_1'(\operatorname{tr}(M))^2-2d_2'(U^\mathsf T M'U)_{11}\operatorname{tr}(M')-d_3'(U^\mathsf T M'U)_{11}^2.$$
where $d'_1,d'_2$ and $d'_3$ are defined in (\ref{ddd}).

Since $U$ is arbitrary, we deduce that the joint density of Vec$(M')$ can be represented by



Let us denote the new vector Vec$(M)$, which takes the diagonal and above diagonal entries of $M$, i.e.,
$$
\operatorname{Vec}(M)=\left(M_{11}, \ldots, M_{N N}, M_{i j}, 1 \leq i < j \leq N\right)^T.
$$
By (\ref{copgoi}), the covariance matrix of Vec$(M)$ is
\begin{align}
   \mathbf{\Sigma}' =
    \begin{pmatrix}
      \mathbf{\Sigma}& 0\\
      0 & \frac{1}{2}\mathbf{I}_{N(N-1)/2}
    \end{pmatrix} ,
  \end{align}
where $\mathbf{\Sigma}$ is the covariance matrix of vector $(M_{11},M_{22},\dots,M_{NN})$ as in (\ref{Sigma}). Note that $\mathbf{\Sigma}=\mathbf{I}_N+d_1\mathbf{1}_N\mathbf{1}_N^\mathsf T+d_2(\mathbf{1}_Ne_1^\mathsf T+e_1\mathbf{1}_N^\mathsf T)+d_3e_1e_1^\mathsf T.$ The Schur complement formula implies that the inverse matrix of $\mathbf{\Sigma}$ can be represented by the same form
\begin{align}
\mathbf{\Sigma}^{-1}=\mathbf{I}_N+d_1'\mathbf{1}_N\mathbf{1}_N^\mathsf T+d_2'(\mathbf{1}_Ne_1^\mathsf T+e_1\mathbf{1}_N^\mathsf T)+d_3'e_1e_1^\mathsf T
\end{align}
with the parameters $d_1',d_2'$ and $d_3'$ are defined in (\ref{ddd}).
Therefore, we arrive at the inverse of $\mathbf{\Sigma}$,
\begin{align}
   \mathbf{\Sigma'}^{-1} =
    \begin{pmatrix}
\mathbf{\Sigma}^{-1}& 0\\
      0 & 2\mathbf{I}_{N(N-1)/2}
    \end{pmatrix}.
  \end{align}
   Since
  $\operatorname{Vec}(M)^T \Sigma'^{-1} \operatorname{Vec}(M)=\operatorname{tr}\left(M^2\right)+d_1'(\operatorname{tr}(M))^2+2d_2'M_{11}\operatorname{tr}(M)+d_3'M_{11}^2$, the joint density of Vec$(M)$ is
\begin{align}\label{denM}
    f(\operatorname{Vec}(M))=&\frac{2^{N(N-1) / 4}}{(2 \pi)^{N(N+1) / 4} \sqrt{1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}}} \nonumber \\ &\quad\times \exp \left\{-\frac{1}{2}\left[ \operatorname{tr}\left(M^2\right)+d_1'(\operatorname{tr}(M))^2+2d_2'M_{11}\operatorname{tr}(M)+d_3'M_{11}^2\right]\right\}.
\end{align}
For any fixed orthogonal matrix $U$, the random matrix $M'=UMU^\mathsf T$  has the same eigenvalues with $M$, which means $M'$ and $M$ have the same density function of the eigenvalues. Note that $M_{11}=(U^\mathsf T M' U)_{11}$. Together with (\ref{denM}), we have
\begin{align*}
    f(\operatorname{Vec}(M))=&\frac{2^{N(N-1) / 4}}{(2 \pi)^{N(N+1) / 4} \sqrt{1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}}}\exp \left\{-\frac{1}{2}\left[ \operatorname{tr}\left(M^2\right)+d_1'(\operatorname{tr}(M))^2\right]\right\} \\ &\times \int \exp \left\{-d_2'(U^\mathsf T M' U)_{11}\operatorname{tr}(M)-\frac{1}{2}d_3'(U^\mathsf T M'U)_{11}^2\right\}d m_N(U).
\end{align*}
 Since $m_N^\beta$ is left translation invariant and right translation invariant, we deduce that
\begin{align*}
    f(\operatorname{Vec}(M))=&\frac{2^{N(N-1) / 4}}{(2 \pi)^{N(N+1) / 4} \sqrt{1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}}}\exp \left\{-\frac{1}{2}\left[ \operatorname{tr}\left(M^2\right)+d_1'(\operatorname{tr}(M))^2\right]\right\} \\ &\times \int \exp \left\{-d_2'(U^\mathsf T M U)_{11}\operatorname{tr}(M)-\frac{1}{2}d_3'(U^\mathsf T MU)_{11}^2\right\}d m_N(U),
\end{align*}
which only depends on the eigenvalues of $M$. Following the same arguments of classical random matrix theory, such as \cite{Me04}, the joint density of the ordered eigenvalues of $M$ is given by
\begin{align*}
   f\left(\lambda_1, \ldots, \lambda_N\right)=&\frac{\prod_{1 \leq i<j \leq N}\left|\lambda_i-\lambda_j\right| \mathbf{1}_{\left\{\lambda_1 \leq \cdots \leq \lambda_N\right\}}}{K_N\sqrt{1+d_3+\frac{d_1+2d_2-(N-1)d_2^2}{1+(N-1)d_1}}}\exp \left\{-\frac{1}{2}\sum_{i=1}^N \lambda_i^2-\frac{1}{2}d_1'\left(\sum_{i=1}^N \lambda_i\right)^2\right\} \\ &\times  \int \exp \left\{-d_2'(U^\mathsf T M U)_{11}\left(\sum_{i=1}^N \lambda_i\right)-\frac{1}{2}d_3'(U^\mathsf T MU)_{11}^2\right\}d m_N(U),
\end{align*}
where $K_N$ is defined in (\ref{KN}). The proof is complete.
\end{proof}
\end{comment}

  \begin{comment}
  In the following, we give the precise expression of $z_1',z_3'$ and the pairwise covariance of $z_1',z_3'$, $\GOE_{N-1}$ when $d_1,d_2,d_3$ taking different values. Let $z_1,z_2,z_3$ be three standard Gaussian random variables, all of which are independent with $\xi$.

  If $1+d_2+d_3\geq 0$, $d_1+d_2\geq 0$ and $-d_2\geq 0$, which implies $d_1\geq 0$ we set
  $$z_1'=\sqrt{1+d_2+d_3}z_1-\sqrt{d_1+d_2}z_2,\quad z_3'=\sqrt{d_1+d_2}z_2+\sqrt{-d_2}z_3,$$
  where $z_1,z_2,z_3$ are independent and  all of them are independent of $\GOE_{N-1}$.

   If $1+d_2+d_3\geq 0$, $d_1\geq 0$ and $d_2\geq 0$, which implies $d_1+d_2\geq 0$, we set
  $$z_1'=\sqrt{1+d_2+d_3}z_1-\sqrt{d_1+d_2}z_2,\quad z_3'=\sqrt{d_1+d_2}z_2+\sqrt{d_2}z_3,$$
  where $z_1$ is indenpendent with $z_2,z_3$ and all of them are independent of $\GOE_{N-1}$ with
$$\mathrm{Cov}(\sqrt{d_1+d_2}z_2,z_3)=
-\sqrt{d_2}.$$

If $1+d_2+d_3\geq 0$, $d_1+d_2\geq 0$ and $d_2\geq 0$ but $d_1<0$, we set
  $$z_1'=\sqrt{1+d_2+d_3}z_1-\sqrt{d_1+d_2}z_2,\quad z_3'=\sqrt{d_1+d_2}z_2+\sqrt{d_2}z_3,$$
  where $z_1,z_2,z_3$ are independent and both of $z_1,z_2$ are independent of $\GOE_{N-1}$ with
$$
\mathbb{E}[z_3\GOE_{N-1}]=\sqrt{d_2}I_N.$$

 If $1+d_2+d_3\geq 0$, $d_1+d_2< 0$ and $-d_2\geq 0$, we set
  $$z_1'=\sqrt{1+d_2+d_3}z_1-\sqrt{-(d_1+d_2)}z_2,\quad z_3'=\sqrt{-(d_1+d_2)}z_2+\sqrt{-d_2}z_3,$$
  where $z_3$ is independent with $z_1,z_2$ and both of $z_1,z_3$ are independent of $\GOE_{N-1}$ with
$$
\mathbb{E}[z_2\GOE_{N-1}]=-\sqrt{-(d_1+d_2)}I_N,\quad
\mathrm{Cov}(\sqrt{1+d_1+d_3}z_1,z_2)=
\sqrt{-(d_1+d_2)}.$$

 If $1+d_2+d_3\geq 0$, $d_1+d_2< 0$ and $d_2\geq 0$, we set
  $$z_1'=\sqrt{1+d_2+d_3}z_1-\sqrt{-(d_1+d_2)}z_2,\quad z_3'=\sqrt{-(d_1+d_2)}z_2+\sqrt{-d_2}z_3,$$
   where $z_1,z_2,z_3$ are independent and $z_1$ is independent of $\GOE_{N-1}$ with
   $$
\mathbb{E}[z_2\GOE_{N-1}]=-\sqrt{-(d_1+d_2)}I_N,\quad
\mathbb{E}[z_3\GOE_{N-1}]=-\sqrt{-d_2}I_N.$$

 If $1+d_2+d_3<0$, $d_1+d_2\geq 0$ and $-d_2\geq 0$, which implies $d_1\geq 0$, we set
  $$z_1'=\sqrt{-(1+d_2+d_3)}z_1-\sqrt{d_1+d_2}z_2,\quad z_3'=\sqrt{d_1+d_2}z_2+\sqrt{-d_2}z_3,$$
  where $z_3$ is indenpendent with $z_1,z_2$ and all of them are independent of $\GOE_{N-1}$ with
$$\mathrm{Cov}(\sqrt{d_1+d_2}z_2,z_1)=
\sqrt{-(1+d_2+d_3)}.$$

If $1+d_2+d_3<0$, $d_2\geq 0$ and $d_1\geq 0$, which implies $d_1+d_2\geq 0$, we set
  $$z_1'=\sqrt{-(1+d_2+d_3)}z_1-\sqrt{d_1+d_2}z_2,\quad z_3'=\sqrt{d_1+d_2}z_2+\sqrt{d_2}z_3,$$
   where $z_3$ is indenpendent with $z_1,z_2$ and all of them are independent of $\GOE_{N-1}$ with
$$\mathrm{Cov}(\sqrt{d_1+d_2}z_2,z_1)=
\sqrt{-(1+d_2+d_3)}.$$

If $1+d_2+d_3<0$, $d_1+d_2\geq 0$ and $d_2\geq 0$ but $d_1<0$, we set
  $$z_1'=\sqrt{-(1+d_2+d_3)}z_1-\sqrt{d_1+d_2}z_2,\quad z_3'=\sqrt{d_1+d_2}z_2+\sqrt{d_2}z_3,$$
  where $z_1,z_2,z_3$ are independent and both of $z_1,z_2$ are independent of $\GOE_{N-1}$ with
$$
\mathbb{E}[z_3\GOE_{N-1}]=-\sqrt{d_2}I_N.$$

Note that $1+d_1+2d_2+d_3$ is the variance of the first entry $M_{11}$, which means one of the value of $1+d_2+d_3$ and $d_1+d_2$ must be large than zero. Therefore, the above discussion contains all the possible situations of the values of $d_1,d_2,d_3$ under the nondegenerate condition in Lemma (\ref{NonPGOI}). As a conclusion, if the covariance $d_1+d_2$ and $d_1$ are both positive, we can construct $z_1'$ and $z_3'$ such that they are independent with $\GOE_{N-1}$. Conversely, at least one of them will has dependence with $\GOE_{N-1}$, which is similar to GOI$(c)$ when the parameter $c<0$.
\end{comment}



\subsection{The nondegeneracy condition}
According to \cite{AZ20}, we need the following  assumption for the model so that the random fields are twice differentiable  almost surely.

\begin{assumption}[Smoothness] \label{assumption1}
    For all $N\in\nz$ and $N=\infty$, the function $D_N$ is four times differentiable at 0 and it satisfies
    \begin{equation*}
    0 < \left|D_N^{(4)}(0)\right| < \infty .
    \end{equation*}
\end{assumption}
For the non-isotropic Gaussian random fields with isotropic increments, we also need the following assumption, which is a natural assumption for stochastic processes with stationary increments.

\begin{assumption}[Pinning] \label{assumption2}
    We have
    \begin{equation*}
        H_{N}(0)=0.
    \end{equation*}
\end{assumption}
First, we recall the covariance structure of the non-isotropic Gaussian random fields with isotropic increments.
\begin{lemma}[\cite{AZ20}*{Lemma A.1}]
\label{Corv}
 Assume Assumptions \ref{assumption1} and \ref{assumption2}. Then for $x \in \mathbb{R}^{N}$,
$$
\begin{aligned}
\operatorname{Cov}\left[H_{N}(x), H_{N}(x)\right] &=D_N\left(\|x\|^{2}\right), \\
\operatorname{Cov}\left[H_{N}(x), \partial_{i} H_{N}(x)\right] &=D_N^{\prime}\left(\|x\|^{2}\right) x_{i}, \\
\operatorname{Cov}\left[\partial_{i} H_{N}(x), \partial_{j} H_{N}(x)\right] &=D_N^{\prime}(0) \delta_{i j}, \\
\operatorname{Cov}\left[H_{N}(x), \partial_{i j} H_{N}(x)\right] &=2 D_N^{\prime \prime}\left(\|x\|^{2}\right) x_{i} x_{j}+\left[D_N^{\prime}\left(\|x\|^{2}\right)-D_N^{\prime}(0)\right] \delta_{i j}, \\
\operatorname{Cov}\left[\partial_{k} H_{N}(x), \partial_{i j} H_{N}(x)\right] &=0, \\
\operatorname{Cov}\left[\partial_{l k} H_{N}(x), \partial_{i j} H_{N}(x)\right] &=-2 D_N^{\prime \prime}(0)\left[\delta_{j l} \delta_{i k}+\delta_{i l} \delta_{k j}+\delta_{k l} \delta_{i j}\right],
\end{aligned}
$$
where $\delta_{i j}$ denotes the Kronecker delta function, $i, j, k, l = 1, \dots, N$.
\end{lemma}
Cheng and Schwartzman \cite{CS18} studied the nondegeneracy condition of the isotropic Gaussian random fields, and they showed that the Gaussian vector
$$\left(H_N(x), \nabla H_N(x), \partial_{i j} H_{N}(x),1\leq i\leq j\leq N\right)$$
is nondegenerate if and only if  $\frac{B_N^{\prime}(0)^2}{B_N^{\prime\prime}(0)}< \frac{N+2}{N}.$  The following lemma gives the nondegeneracy condition of the non-isotropic Gaussian random fields with isotropic increments.
\begin{lemma}
\label{Nond}
Assume Assumptions \ref{assumption1} and \ref{assumption2}. Then for any $x \in \mathbb{R}^N \setminus \{0\}$,
 the Gaussian vector $\left(H_N(x), \nabla H_N(x), \partial_{i j} H_{N}(x),1\leq i\leq j\leq N\right)$ is nondegenerate if and only if for any $r>0$
 \begin{align}\label{nondg}
&D_N\left(r\right)-\frac{D_N^{\prime}(r)^2 r}{D_N^{\prime}(0)}
+\frac{(N+1)D_N^{\prime\prime}(r)^2 r^{2}}{(N+2)D_N^{\prime\prime}(0)}\nonumber\\ &\quad
+\frac{2rD_N^{\prime\prime}(r)\left(D_N^{\prime}(r)-D_N^{\prime}(0)\right)}{(N+2)D_N^{\prime\prime}(0)}
+\frac{N\left(D_N^{\prime}(r)-D_N^{\prime}(0)\right)^2}{2(N+2)D_N^{\prime\prime}(0)}>0.
\end{align}
Recall that $D(r)$ denotes a structure function from the class $\dx_\8$. Then for all $N$, the Gaussian vector $\left(H_N(x), \nabla H_N(x), \partial_{i j} H_{N}(x),1\leq i\leq j\leq N\right)$ is nondegenerate if for any $r>0$,
\begin{align}\label{larn}
D\left(r\right)-\frac{D^{\prime}(r)^2 r}{D^{\prime}(0)}
+\frac{D^{\prime\prime}(r)^2 r^{2}}{D^{\prime\prime}(0)}
+\frac{\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{2D^{\prime\prime}(0)}>0.
\end{align}
\end{lemma}
\begin{proof}
Following Lemma \ref{Corv}, it is clear that the covariance matrix of the random vector $\left(H_N(x), \nabla H_N(x), \partial_{i j} H_{N}(x),1\leq i\leq j\leq N\right)$ is
\begin{align}
    \textbf{$\mathbf{G}$}=
    \begin{pmatrix}
      D_N\left(\|x\|^{2}\right)& \xi_1^\mathsf T& \xi_2^\mathsf T& \xi_3^\mathsf T \\
       \xi_1  &  \textbf{$\mathbf{A}$} & \mathbf{0}_1^\mathsf T & \mathbf{0}_2^\mathsf T \\
       \xi_2  & \mathbf{0}_1 &  \textbf{$\mathbf{B}$}& \mathbf{0}_3^\mathsf T \\
       \xi_3  & \mathbf{0}_2 & \mathbf{0}_3 &  \textbf{$\mathbf{C}$} \\
    \end{pmatrix} \nonumber ,
  \end{align}
where
$$\xi_1=\left(D_N^{\prime}(\|x\|^{2})x_1,\dots,D_N^{\prime}(\|x\|^{2})x_N \right)^\mathsf T$$ is the covariance vector of $H_{N}(x)$ with $\left(\partial_{1} H_{N}(x),\dots,\partial_{N} H_{N}(x) \right)$,
$$\xi_2=\left(2D_N^{\prime\prime}(\|x\|^{2})x_1^2+D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0), \dots,
2D_N^{\prime\prime}(\|x\|^{2})x_N^2+D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0) \right)^\mathsf T$$ is the covariance vector of $H_{N}(x)$ with $\left(\partial_{11} H_{N}(x),\dots,\partial_{NN} H_{N}(x) \right)$,
$$\xi_3=2D_N^{\prime\prime}(\|x\|^{2})\left(x_1x_2,\dots,x_1x_N,
x_2x_3,\dots,x_2x_N,\dots, x_{N-1}x_N \right)^\mathsf T$$ is an $\frac{N(N-1)}{2}$ dimensional vector which is the covariance vector of $H_{N}(x)$ with the Hessian off the diagonal, $\mathbf{0}_1$, $\mathbf{0}_2$ and $\mathbf{0}_3$ are $N\times N$, $\frac{N(N-1)}{2}\times N$, and $\frac{N(N-1)}{2}\times N$  matrices with all elements equal to zero, respectively. In addition to this, $\mathbf{A}=D_N^{\prime}(0)\mathbf{I}_N$, $\mathbf{B}=-4 D_N^{\prime \prime}(0)\mathbf{I}_N-2 D_N^{\prime \prime}(0)\mathbf{1}_N \mathbf{1}_N^\mathsf T$ and $\mathbf{C}=-2D_N^{\prime\prime}(0)\mathbf{I}_{\frac{N(N-1)}{2}}$ , where $\mathbf{1}_N$ is the $N$ dimensional column vector with all elements equal to one hereafter.

To simplify the notation, we denote the upper left submatrix of $\mathbf{G}$ by
\begin{align}
    \textbf{$\mathbf{G}_1$} :=
    \begin{pmatrix}
      D_N\left(\|x\|^{2}\right)& \xi_1^\mathsf T& \xi_2^\mathsf T \\
       \xi_1  &  \textbf{$\mathbf{A}$} & \mathbf{0}_1^\mathsf T  \\
       \xi_2  & \mathbf{0}_1 &  \textbf{$\mathbf{B}$} \\
       \end{pmatrix}\nonumber  ,
  \end{align}
and $\mathbf{C}_1:=\left( \xi_3, \mathbf{0}_2 ,\mathbf{0}_3 \right)$. Using the Schur complement formula for the matrix $\mathbf{G}$,
\begin{align}
\label{detG}
\det\mathbf{G} =\det\mathbf{C} \det\left(\mathbf{G}_1 -\mathbf{C}_1^\mathsf T\mathbf{C}^{-1}\mathbf{C}_1\right).
\end{align}
% We write $\mathbf{G}_1^{\prime}=\mathbf{G}_1 -\mathbf{C}_1^\mathsf T\mathbf{C}^{-1}\mathbf{C}_1$.
By straightforward calculations,
\begin{align}
\textbf{$\mathbf{G}_1^{\prime}$}
:=\mathbf{G}_1 -\mathbf{C}_1^\mathsf T\mathbf{C}^{-1}\mathbf{C}_1=
    \begin{pmatrix}
      D_N\left(\|x\|^{2}\right)-y_1& \xi_1^\mathsf T& \xi_2^\mathsf T \\
       \xi_1  &  \textbf{$\mathbf{A}$} & \mathbf{0}_1^\mathsf T  \\
       \xi_2  & \mathbf{0}_1 &  \textbf{$\mathbf{B}$} \\
       \end{pmatrix} \nonumber ,
  \end{align}
  where $y_1=\frac{-2D_N^{\prime\prime}(\|x\|^{2})^2}{D_N^{\prime\prime}(0)}\left[x_1^2(x_2^2+\dots+x_N^2)+x_2^2(x_3^2+\dots+x_N^2)+\dots+x_{N-1}^2x_N^2\right]$. Following the same argument as above, we denote
  \begin{align}
    \textbf{$\mathbf{G}_2$}:=
    \begin{pmatrix}
    D_N\left(\|x\|^{2}\right)-y_1& \xi_1^\mathsf T \\
       \xi_1  &  \textbf{$\mathbf{A}$}   \\
       \end{pmatrix}\nonumber,
  \end{align}
 and $\mathbf{C}_2:=\left( \xi_2, \mathbf{0}_1 \right)$. For matrix $\mathbf{G}_1^{\prime}$, we have
 \begin{align}
 \label{detG1}
\det \mathbf{G}_1^{\prime} =\det\mathbf{B} \det\left(\mathbf{G}_2 -\mathbf{C}_2^\mathsf T\mathbf{B}^{-1}\mathbf{C}_2\right).
\end{align}
Note that $\mathbf{B}^{-1}=\frac{-1}{4(N+2)D_N^{\prime\prime}(0)}\left[(N+2)\mathbf{I}_N-\mathbf{1}_N \mathbf{1}_N^T\right]$. Straightforward calculations show that
%Letting $y_2=\mathbf{C}_2^\mathsf T\mathbf{B}^{-1}\mathbf{C}_2$, we obtain
%Setting $\mathbf{G}_2^{\prime}=\mathbf{G}_2 -\mathbf{C}_2^\mathsf T\mathbf{B}^{-1}\mathbf{C}_2$ implies
\begin{align}
\textbf{$\mathbf{G}_2^{\prime}$}
:=\mathbf{G}_2 -\mathbf{C}_2^\mathsf T\mathbf{B}^{-1}\mathbf{C}_2=
    \begin{pmatrix}
      D_N\left(\|x\|^{2}\right)-y_1-y_2& \xi_1^\mathsf T \\
       \xi_1  &  \textbf{$\mathbf{A}$}   \\
       \end{pmatrix}\nonumber,
  \end{align}
where
\begin{align*}
y_2&=-\frac{1}{4D_N^{\prime\prime}(0)}\sum_{i=1}^{N}\left(2D_N^{\prime\prime}(\|x\|^{2})x_i^2+ D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0)\right)^2\\&\nonumber\quad+\frac{1}{4(N+2)D_N^{\prime\prime}(0)}
\left(2D_N^{\prime\prime}(\|x\|^{2})\|x\|^{2}+ N\left(D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0)\right) \right)^2.
\end{align*}
Using the Schur complement formula again, we have
  \begin{align}
  \label{detG2}
\det\mathbf{G}_2^{\prime} =\left(D_N\left(\|x\|^{2}\right)-y_1-y_2 -y_3\right)\det\mathbf{A},
\end{align}
where $y_3=\xi_1^\mathsf T\mathbf{A}^{-1}\xi_1=\frac{D_N^{\prime}(\|x\|^{2})^2\|x\|^{2}}{D_N^{\prime}(0)}$.
Combining (\ref{detG}), (\ref{detG1}) and (\ref{detG2}), we arrive at $\det \mathbf{G}=\left(D_N\left(\|x\|^{2}\right)-y_1-y_2-y_3\right)\det\mathbf{A}\det\mathbf{B}\det\mathbf{C}$.
It is clear that all of $\mathbf{A},\mathbf{B}$ and $\mathbf{C}$ are positive definite matrices. By the eigenvalue interlacing theorem, the matrix $\mathbf{G}$ has at most one negative eigenvalue. Therefore, if we can show $\det \mathbf{G}>0$, which is equivalent to $D_N\left(\|x\|^{2}\right)-y_1-y_2-y_3>0$, then the matrix $\mathbf{G}$ is positive definite.
By calculations,
\begin{align}
y_1+y_2&=-\frac{(N+1)D_N^{\prime\prime}(\|x\|^{2})^2\|x\|^{4}}{(N+2)D_N^{\prime\prime}(0)}
-\frac{2D_N^{\prime\prime}(\|x\|^{2})\|x\|^{2}\left(D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0)\right)}{(N+2)D_N^{\prime\prime}(0)}\nonumber \\ & \quad
-\frac{N\left(D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0)\right)^2}{2(N+2)D_N^{\prime\prime}(0)}.
\end{align}
Therefore, we eventually get
\begin{align*}
&D_N\left(\|x\|^{2}\right)-y_1-y_2-y_3=D_N\left(\|x\|^{2}\right)-\frac{D_N^{\prime}(\|x\|^{2})^2\|x\|^{2}}{D_N^{\prime}(0)}
+\frac{(N+1)D_N^{\prime\prime}(\|x\|^{2})^2\|x\|^{4}}{(N+2)D_N^{\prime\prime}(0)}\nonumber\\ &\quad
+\frac{2D_N^{\prime\prime}(\|x\|^{2})\|x\|^{2}\left(D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0)\right)}{(N+2)D_N^{\prime\prime}(0)}
+\frac{N\left(D_N^{\prime}(\|x\|^{2})-D_N^{\prime}(0)\right)^2}{2(N+2)D_N^{\prime\prime}(0)},
\end{align*}
which completes the proof of the first claim by replacing $\|x\|^{2}$ with $r$.

For the second assertion, we need to show that for all $N$
\begin{align*}
&D\left(r\right)-\frac{D^{\prime}(r)^2 r}{D^{\prime}(0)}
+\frac{(N+1)D^{\prime\prime}(r)^2 r^{2}}{(N+2)D^{\prime\prime}(0)}\nonumber\\ &\quad
+\frac{2rD^{\prime\prime}(r)\left(D^{\prime}(r)-D^{\prime}(0)\right)}{(N+2)D^{\prime\prime}(0)}
+\frac{N\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{2(N+2)D^{\prime\prime}(0)}>0.
\end{align*}
Notice that
\begin{align*}
&D\left(r\right)-\frac{D^{\prime}(r)^2 r}{D^{\prime}(0)}
+\frac{(N+1)D^{\prime\prime}(r)^2 r^{2}}{(N+2)D^{\prime\prime}(0)}
+\frac{2rD^{\prime\prime}(r)\left(D^{\prime}(r)-D^{\prime}(0)\right)}{(N+2)D^{\prime\prime}(0)}
+\frac{N\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{2(N+2)D^{\prime\prime}(0)}\nonumber\\ &=
D\left(r\right)-\frac{D^{\prime}(r)^2 r}{D^{\prime}(0)}
+\frac{D^{\prime\prime}(r)^2 r^{2}}{D^{\prime\prime}(0)}
+\frac{\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{2D^{\prime\prime}(0)}
\nonumber\\ &\quad + \frac{\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{-(N+2)D^{\prime\prime}(0)}+\frac{D^{\prime\prime}(r)^2 r^{2}}{-(N+2)D^{\prime\prime}(0)}-\frac{2rD^{\prime\prime}(r)\left(D^{\prime}(r)-D^{\prime}(0)\right)}{-(N+2)D^{\prime\prime}(0)}.
\end{align*}
By the elementary inequality $a^2+b^2\geq 2ab$, we deduce
$$\frac{\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{-(N+2)D^{\prime\prime}(0)}+\frac{D^{\prime\prime}(r)^2 r^{2}}{-(N+2)D^{\prime\prime}(0)}-\frac{2rD^{\prime\prime}(r)\left(D^{\prime}(r)-D^{\prime}(0)\right)}{-(N+2)D^{\prime\prime}(0)}\geq 0.$$
Together with the assumption (\ref{larn}), we obtain the desired result.
\end{proof}

\subsection{Representation with GOI matrices}

Under the nondegeneracy condition (\ref{nondg}),
 we now investigate the expected number of critical points (with or without given indices) of non-isotropic Gaussian random fields with isotropic increments.

To start, we follow the idea for the case $\dx_\8$ in \cites{AZ20,AZ22} and list  the notations from  \cite[{(3.15)}]{AZ20} for later use:
\begin{align}
\label{eq:msialbt}
  m_1 & =m_1(\rho,u)=  \frac{u( 2D_N''(\rho^2)\rho^2+D_N'(\rho^2) -D_N'(0) )}{D_N(\rho^2) -\frac{D_N'(\rho^2)^2 \rho^2}{D_N'(0) }},\nonumber \\
  m_2&=m_2(\rho,u)=  \frac{u(D_N'(\rho^2) -D_N'(0) )}{D_N( \rho^2) -\frac{D_N'(\rho^2)^2 \rho^2}{D_N'(0) }}, \notag\\
  \si_Y &=\si_Y(\rho) =\sqrt{D_N(\rho^2)-\frac{D_N'(\rho^2)^2\rho^2}{D_N'(0)} },\notag \\
\alpha &=\alpha(\rho^2)= \frac{2D_N''(\rho^2)}{ \sqrt{ D_N(\rho^2)-\frac{D_N'(\rho^2)^2 \rho^2}{D_N'(0)}}},  \ \ \
   \beta =\beta(\rho^2)=\frac{D_N'(\rho^2 )-D_N'(0)}{\sqrt{ D_N(\rho^2 )-\frac{D_N'(\rho^2)^2 \rho^2}{D_N'(0)}}}, \notag \\
   \si_1 & =\si_1(\rho^2)= \sqrt{-4D_N''(0)-(\al \rho^2 +\beta)\al\rho^2}, \ \ \
  \si_2  =\si_2(\rho^2)= \sqrt{-2D_N''(0)-(\al\rho^2 +\beta)\beta}.
\end{align}
Using Lemma \ref{Corv}, we define
\begin{align*}
    \Sigma_{01} &=\operatorname{Cov}\left(H_N(x), \nabla H_N(x)\right)=D_N^{\prime}\left(\|x\|^2\right) x^{\top}, \\
    \Sigma_{11} &=\operatorname{Cov}\left(\nabla H_N(x)\right)=D_N^{\prime}(0) \mathbf{I}_N,
\end{align*}
and
\begin{equation*}
    Y=H_N(x)-\Sigma_{01} \Sigma_{11}^{-1} \nabla H_N(x)=H_N(x)-\frac{D_N^{\prime}\left(\|x\|^2\right) \sum_{i=1}^N x_i \partial_i H_N(x)}{D_N^{\prime}(0)}.
\end{equation*}
Then $Y$ is a centered Gaussian random variable whose variance $\sigma_Y$ is defined in (\ref{eq:msialbt}) with $\rho$ replaced by $\|x\|$.
Notice that both $Y$ and $\nabla^2 H_N(x)$ are independent of $\nabla H_N(x)$. By the Kac--Rice formula \cite[Theorem 11.2.1]{AT07},
\begin{equation}\label{Critical1}
    \begin{split}
    & \mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_N\left(E, T_N\right) \\
    & =\int_{T_N} \mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x)\right| \mathbf{1}\left\{Y+ \Sigma_{01} \Sigma_{11}^{-1} \nabla H_N(x) \in E\right\} \middle\vert \nabla H_N(x)=0\right] p_{\nabla H_N(x)}(0) \, \mathrm{d} x \\
    & =\int_{T_N} \mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x)\right| \mathbf{1}\{Y \in E\}\right] p_{\nabla H_N(x)}(0) \, \mathrm{d} x,
    \end{split}
\end{equation}
and similarly
\begin{equation}\label{Critical2}
    \begin{split}
    & \mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_{N,k} \left(E, T_N\right)
   % & =\int_{T_N} \mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x)\right| \mathbf{1}\left\{Y+ \Sigma_{01} \Sigma_{11}^{-1} \nabla H_N(x) \in E, i\left(\nabla^2 H_N(x)\right)=k \right\} \middle\vert \nabla H_N(x)=0\right] \\
   % &\phantom{XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX} \times p_{\nabla H_N(x)}(0) \mathrm{d} x \\
     =\int_{T_N} \mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x)\right| \mathbf{1}\{Y \in E, i\left(\nabla^2 H_N(x)\right)=k \}\right] p_{\nabla H_N(x)}(0) \, \mathrm{d} x,
    \end{split}
\end{equation}
where $p_{\nabla H_N(x)}(0) = (2 \pi D_N'(0))^{-N / 2}$ is the p.d.f.\ of $\nabla H_N(x)$ at $0$.

From Lemma \ref{Corv}, we note that $\nabla^2 H_N(x)$ has the same distribution as $2\sqrt{-D_N^{\prime \prime}(0)}$ $\operatorname{GOI}(\frac{1}{2})$. Arguing as in \cite{AZ20}*{Section 3} and using the spherical coordinates, we deduce that
\begin{align*}
    &\left(\nabla^2 H_N (x) \mid Y=u\right) \stackrel{d}{=}\begin{pmatrix}
      m_1 (\|x\|, u) & 0\\
0&m_2 (\|x\|, u) \mathbf{I}_{N-1}
    \end{pmatrix}
    +2 \sqrt{-D_N^{\prime \prime}(0)} M =: G,
\end{align*}
where $M$ is an  $\operatorname{SGOI} (d_1,d_2,d_3)$ matrix  with parameters
\begin{equation*}
    d_1 = \frac{1}{2}+\frac{\beta (\|x\|^2) ^2}{4D_N'' (0)}, \qquad
    d_2 = \frac{\alpha (\|x\|^2) \beta (\|x\|^2) \|x\|^2}{4D_N'' (0)}, \qquad
    d_3 = \frac{\alpha (\|x\|^2)^2 \|x\|^4}{4D_N'' (0)},
\end{equation*}
and the functions $m_1$, $m_2$, $\alpha$, and $\beta$ are given as in (\ref{eq:msialbt}) with  $\rho$ replaced by $\|x\|$. Similar to (\ref{M}), we may write $G$ as a block matrix
\begin{equation} \label{G}
    G = \begin{pmatrix}
    2\sqrt{- D^{\prime \prime}(0)}\zeta_1+m_1 & 2\sqrt{- D^{\prime \prime}(0)} \xi^{\top} \\
    2\sqrt{- D^{\prime \prime}(0)}\xi & G_{* *}
    \end{pmatrix},
\end{equation}
where
\begin{equation} \label{G**}
    G_{**} := 2\sqrt{- D^{\prime \prime}(0)} \left[ \mathrm{GOE}_{N-1} + \left( \zeta_2+\frac{m_2}{2\sqrt{-D_N^{\prime \prime}(0)}}\right) \mathbf{I}_{N-1} \right].
\end{equation}
From (\ref{goicond}), we know
\begin{equation} \label{y}
    \begin{split}
    \mathbb{E}(\GOE_{N-1}+\zeta_2\mathbf{I}_{N-1}\mid \zeta_1=y) &= \frac{(2D_N^{\prime \prime}(0)+\beta^2+\alpha\beta\|x\|^2)y}{6D_N^{\prime \prime}(0)+(\beta+\alpha\|x\|^2)^2}\mathbf{I}_{N-1}, \\
   \left( \GOE_{N-1}+\zeta_2\mathbf{I}_{N-1}\mid \zeta_1=y\right) &\stackrel{d}{=} \frac{(2D_N^{\prime \prime}(0)+\beta^2+\alpha\beta\|x\|^2)y}{6D_N^{\prime \prime}(0)+(\beta+\alpha\|x\|^2)^2}\mathbf{I}_{N-1}+M'.
   \end{split}
\end{equation}
where $M'$ is an $(N-1)\times (N-1)$ GOI$(c)$ matrix with parameter
\begin{equation}\label{c}
    c = \frac{d_1+d_1d_3-d_2^2}{1+d_1+2d_2+d_3}=\frac{4D_N'' (0)+2\beta^2+\alpha^2\|x\|^4}{12D_N'' (0) + 2(\beta+\alpha\|x\|^2)^2}.
\end{equation}
 Set
\begin{equation}\label{m3}
m_3=\frac{(2D_N^{\prime \prime}(0)+\beta^2+\alpha\beta\|x\|^2)y}{6D_N^{\prime \prime}(0)+(\beta+\alpha\|x\|^2)^2}+\frac{m_2}{2\sqrt{-D_N^{\prime \prime}(0)}},
\end{equation}
where $m_2$ is defined in (\ref{eq:msialbt}). Let $\lambda_1 \leq \cdots \leq \lambda_{N-1}$ be the eigenvalues of the $\operatorname{GOI}(c)$ matrix $M'$. Since the distribution of $\operatorname{GOI}(c)$ matrices is invariant under orthogonal congruence transformation, following the same argument as for the GOE matrices, there exists a random orthogonal matrix $V$ independent of the unordered eigenvalues $\tilde{\lambda}_j, j=1, \ldots, N-1$ such that
\begin{align}\label{rotation}
V M' V^\mathsf T = \begin{pmatrix}
 \tilde{\lambda}_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \tilde{\lambda}_{N-1}
\end{pmatrix}.
\end{align}
Since the law of the Gaussian vector $\xi$ is rotationally invariant, $V \xi$ is a centered Gaussian vector with covariance matrix $\frac{1}{2} \mathbf{I}_{N-1}$ that is independent of $\tilde{\lambda}_j$'s. We can rewrite $V \xi \stackrel{d}{=} Z / \sqrt{2}$, where $Z = (Z_1, \ldots, Z_{N-1} )$ is an $N-1$ dimensional standard Gaussian random vector. We also need the following lemma for the calculation of  $\mathbb{E} \operatorname{Crt}_{N,k}\left(E, \left(R_{1}, R_{2}\right)\right)$. Recall that the signature of a symmetric matrix is the number of positive eigenvalues minus that of negative eigenvalues.
\begin{lemma}
[{\cite[Equation 2]{Laz88}}] Let $S$ be a symmetric block matrix, and write its inverse $S^{-1}$ in block form with the same block structure:
$$
S=\left(\begin{array}{cc}
A & B \\
B^{\sfT} & C
\end{array}\right), \quad S^{-1}=\left(\begin{array}{cc}
A^{\prime} & B^{\prime} \\
\left(B^{\prime}\right)^{\sfT} & C^{\prime}
\end{array}\right) .
$$
Then $\operatorname{sgn}(S)=\operatorname{sgn}(A)+\operatorname{sgn}\left(C^{\prime}\right)$, with $\operatorname{sgn}(M)$ denoting the signature of the matrix $M$.\label{sgn}
\end{lemma}
 Let us define
\begin{equation*}
    \eta (\zeta_1,G_{**}) =m_1+2\sqrt{-D_N^{\prime \prime}(0)}\zeta_1+4D_N^{\prime \prime}(0)\xi^{\top} G_{* *}^{-1} \xi.
\end{equation*}
Following Lemma \ref{sgn}, we have for $k \geq 1$,
\begin{equation*}
    \{i(G)=k\}=\left\{i\left(G_{* *}\right)=k, \eta (\zeta_1,G_{**}) >0\right\} \cup \left\{ i\left(G_{* *}\right)=k-1, \eta (\zeta_1,G_{**}) <0 \right\},
\end{equation*}
and for $k=0$
\begin{equation*}
    \{i(G)=0\}=\left\{i\left(G_{* *}\right)=0, \eta (\zeta_1,G_{**}) >0\right\} .
\end{equation*}
Moreover, by (\ref{y}) and (\ref{rotation}), we deduce
\begin{equation} \label{eta'}
    \begin{split}
    \left(\eta(\zeta_1,G_{**}) \middle\vert \zeta_1=y\right)
    &\stackrel{d}{=} m_1+2\sqrt{-D_N^{\prime \prime}(0)}y-2\sqrt{-D_N^{\prime \prime}(0)}\xi^{\top} \left(M'+m_3\mathbf{I}_{N-1}\right)^{-1} \xi \\
    &\stackrel{d}{=}m_1+2\sqrt{-D_N^{\prime \prime}(0)}y-2\sqrt{-D_N^{\prime \prime}(0)}\sum_{l=1}^{N-1} \frac{Z_l^2}{\lambda_l+m_3} \\
    &=: \eta'(\Lambda).
    \end{split}
\end{equation}

Due to the rotational symmetry of isotropy, we are interested in the critical points in the shell domain
$$T_{N}\left(R_{1}, R_{2}\right)=\{x \in \mathbb{R}^{N}: R_{1}<\|x\|<R_{2}\},\quad  0 \leq R_{1}<R_{2} <\infty,$$
with critical values in an arbitrary Borel subset $E$. For simplicity, we write $\operatorname{Crt}_{N}\left(E,\left(R_{1}, R_{2}\right)\right):=$ $\operatorname{Crt}_{N}\left(E, T_{N}\left(R_{1}, R_{2}\right)\right)$. Together with (\ref{Critical1}) and (\ref{Critical2}), using the spherical coordinates and writing $\rho=\|x\|$, we arrive at
\begin{align}
\mathbb{E} \operatorname{Crt}_N\left(E, \left(R_{1}, R_{2}\right)\right)
&=S_{N-1}\int_{R_1}^{R_2} \int_{E}\mathbb{E}\left[\left|\operatorname{det} G\right| \right] \frac{1}{\sqrt{2 \pi} \sigma_{Y}} e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}p_{\nabla H_N(x)}(0) \rho^{N-1} \dd u \, \mathrm{d} \rho, \label{R1R2_1} \\
\mathbb{E} \operatorname{Crt}_{N,k} \left(E, \left(R_{1}, R_{2}\right)\right)
&=S_{N-1}\int_{R_1}^{R_2} \int_{E}\mathbb{E}\left[\left|\operatorname{det} G\right|  \mathbf{1}\{ i(G) = k \} \right] \frac{1}{\sqrt{2 \pi} \sigma_{Y}} e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}p_{\nabla H_N(x)}(0) \rho^{N-1} \dd u \, \mathrm{d} \rho, \label{R1R2_2}
\end{align}
 where $S_{N-1} = \frac{2 \pi^{N / 2}} { \Gamma(N / 2)}$ is the area of $N-1$ dimensional unit sphere; see \cite[{(4.1)}]{AZ20} for details.

Unlike the isotropic case in \cite{CS18}, here it is difficult to find the joint eigenvalue density of the matrix $G$ due to the lack of invariance property. However, we observe that conditioning on the first entry of an SGOI($d_1,d_2,d_3$) matrix, its lower right $(N-1)\times(N-1)$ submatrix has the same distribution as a GOI$(c)$ matrix for some suitable $c$. We are now ready to state our main result in this section.
\begin{comment}
In general case $E \subseteq\mathbb{R}$, when condition on $Y=u$, the Hessian $\nabla^2 H_N(x)$ minus its expectation has same distribution with $2\sqrt{-D_N^{\prime \prime}(0)}$ $\operatorname{SGOI}(d_1,d_2,d_3)$ with parameters $d_1=\frac{1}{2}+\frac{\beta^2}{4D_N^{\prime \prime}(0)}$, $d_2=\frac{\alpha\beta\|x\|^2}{4D_N^{\prime \prime}(0)}$ and $d_3=\frac{\alpha^2\|x\|^4}{4D_N^{\prime \prime}(0)}$. It seems very difficult to derive the density of eigenvalues of the general $\operatorname{SGOI}(d_1,d_2,d_3)$ matrix. We have tried different methods but all failed. But for the $\operatorname{SGOI} (d_1,d_2,d_3)$ matrix, when condition on its first entry, the lower right $(N-1)\times(N-1)$ matrix minus its expectation has the same distribution with GOI$(c)$. Since the density of eigenvalues of the general GOI$(c)$ is known as in \cite{CS18}, we can use the Schur complement formula to derive the expectation of critical points.
\end{comment}
\begin{theorem}\label{Shell}
Let $H_N=\left\{H_N(x): x \in \mathbb{R}^N\right\}$ be a non-isotropic Gaussian field with isotropic increments. Let $E \subset \mathbb{R}$ be a Borel set and $T_N$ be the shell domain $T_{N}\left(R_{1}, R_{2}\right)=\{x \in \mathbb{R}^{N}: R_{1}<\|x\|<R_{2}\}$, where $0 \leq R_{1}<R_{2} < \infty$.
Assume Assumptions \ref{assumption1}, \ref{assumption2} and the nondegeneracy condition (\ref{nondg}). Then we have
\begin{align*}
    &\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_N\left(E, \left(R_{1}, R_{2}\right)\right) \\
    & =\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_1}^{R_2} \int_{E}\int_{-\infty}^{\infty}\frac{e^{-\frac{2D_N''(0)y^{2}}{ (6D_N''(0)+(\beta+\alpha\rho^2)^2)}}}{\sqrt{2 \pi(-6D_N''(0)-(\beta+\alpha\rho^2)^2)}}  \frac{ e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}} \rho^{N-1} \, \dd y \, \dd u \, \mathrm{d} \rho \\
    & \quad\times \mathbb{E}\left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}y\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{k=1}^{N-1} Z_k^2 \prod_{j \neq k}^{N-1}(\lambda_j+m_3)\right| \right] \right\},
\end{align*}
and for $k=0,\dots,N-1$,
\begin{align*}
    &\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_{N,k} \left(E, \left(R_{1}, R_{2}\right)\right) \\
    & =\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_1}^{R_2} \int_{E}\int_{-\infty}^{\infty}\frac{e^{-\frac{2D_N''(0)y^{2}}{ (6D_N''(0)+(\beta+\alpha\rho^2)^2)}}}{\sqrt{2 \pi(-6D_N''(0)-(\beta+\alpha\rho^2)^2)}}  \frac{ e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}} \rho^{N-1} \, \dd y \, \dd u \, \mathrm{d} \rho \\
    &\; \; \times \mathbb{E}\left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}y\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{k=1}^{N-1} Z_k^2 \prod_{j \neq k}^{N-1}(\lambda_j+m_3)\right| \mathbf{1}_{A_k} \right] \right\},
\end{align*}
where
\begin{align*}
    A_k = \begin{cases}
        \left\{\lambda_k<-m_3<\lambda_{k+1}, \eta' (\Lambda)>0 \right\} \cup \left\{\lambda_{k-1}<-m_3<\lambda_{k}, \eta'(\Lambda)<0\right\}, & k \ge 1, \\
        \left\{\lambda_1 > -m_3, \eta' (\Lambda)>0 \right\}, & k = 0,
    \end{cases}
\end{align*}
with $\eta' (\Lambda)$ defined in (\ref{eta'}), $m_1$ and $\sigma_Y$ are given in (\ref{eq:msialbt}), $c$ is defined in (\ref{c}), $m_3$ is defined in (\ref{m3}), and by convention $\lambda_0=-\infty$, $\lambda_N=\infty$. We emphasize that the $\mathbb{E}_{\mathrm{GOI}(c)}^{N-1}$ is taken with respect to the GOI$(c)$ eigenvalues $\lambda_i$'s.
\end{theorem}
\begin{proof}
Recall that $G = \left( \begin{smallmatrix}
      m_1 &0\\
0&m_2\mathbf{I}_{N-1}
\end{smallmatrix} \right) + 2\sqrt{-D_N^{\prime \prime}(0)}M$, where $M$ is an $\operatorname{SGOI} (d_1,d_2,d_3)$ matrix defined in (\ref{M}) with parameters $d_1=\frac{1}{2}+\frac{\beta^2}{4D_N^{\prime \prime}(0)}$, $d_2=\frac{\alpha\beta\rho^2}{4D_N^{\prime \prime}(0)}$ and $d_3=\frac{\alpha^2\rho^4}{4D_N^{\prime \prime}(0)}$.
The Schur complement formula implies that
\begin{align}\label{detGG}
    \operatorname{det} G&=\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}\zeta_1+4D_N'' (0)\xi^{\top} G_{* *}^{-1} \xi\right)\det\left(G_{* *}\right) \nonumber \\
    &= \left\{ m_1+2\sqrt{-D_N^{\prime \prime}(0)}\zeta_1-2\sqrt{-D_N'' (0)}\xi^\mathsf T \left[ \left(\frac{m_2}{2\sqrt{-D_N'' (0)}}+\zeta_2\right)\mathbf{I}_{N-1}+\GOE_{N-1} \right]^{-1} \xi \right\} \nonumber \\
    &\quad \times2^{N-1}\left(-D_N'' (0)\right)^\frac{N-1}{2}\operatorname{det} \left(\left(\frac{m_2}{2\sqrt{-D_N'' (0)}}+\zeta_2\right)\mathbf{I}_{N-1}+\GOE_{N-1}\right).
\end{align}
Conditioning on $\zeta_1=y$ and
using (\ref{y}) and (\ref{detGG}), we obtain
\begin{equation} \label{EdetG}
    \begin{split}
    &\mathbb{E}\left[\left|\operatorname{det} G\right| \right]
    = \int_{-\infty}^{\infty}\frac{2^{N-1} \left( -D_N'' (0) \right)^\frac{N-1}{2}}{\sqrt{2 \pi(1+d_1+2d_2+d_3)}} e^{-\frac{y^{2}}{2 (1+d_1+2d_2+d_3)}} \\
    &\phantom{\operatorname{det} G} \times \mathbb{E} \left[ \left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}y-2\sqrt{-D_N^{\prime \prime}(0)}\xi^\mathsf T \left(M'+m_3\mathbf{I}_{N-1}\right)^{-1}\xi\right)\operatorname{det} \left(M'+m_3\mathbf{I}_{N-1}\right)\right| \right] \dd y,
    \end{split}
\end{equation}
where $1+d_1+2d_2+d_3=\frac{6D_N'' (0) + (\beta+\alpha\rho^2)^2}{4D_N'' (0)}$.
By (\ref{rotation}) and the independence of $Z$ and $\lambda_i$'s, we deduce
\begin{equation} \label{EdetGG}
    \begin{split}
    &\mathrel{\phantom{=}} \mathbb{E}\left[\left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}x-2\sqrt{-D_N^{\prime \prime}(0)}\xi^\mathsf T \left( M' + m_3\mathbf{I}_{N-1}\right)^{-1}\xi\right)\operatorname{det} \left( M' + m_3\mathbf{I}_{N-1}\right)\right| \right] \\
    &=\mathbb{E}\left[\left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}x\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right] \\
    &=\mathbb{E} \left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left| \left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}x\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right] \right\},
    \end{split}
\end{equation}
where the $\mathbb{E}_{\mathrm{GOI}(c)}^{N-1}$ is taken with respect to $\lambda_i$'s. Combining  (\ref{R1R2_1}), (\ref{EdetG}), and (\ref{EdetGG}), after some simplifications we obtain
\begin{align*}
&\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_N\left(E, \left(R_{1}, R_{2}\right)\right) \\
& =\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_1}^{R_2} \int_{E}\int_{-\infty}^{\infty}\frac{e^{-\frac{2D_N''(0)y^{2}}{ (6D_N''(0)+(\beta+\alpha\rho^2)^2)}}}{\sqrt{2 \pi(-6D_N''(0)-(\beta+\alpha\rho^2)^2)}}  \frac{ e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}} \rho^{N-1} \, \dd y \, \dd u \, \dd \rho \\
& \quad \times \mathbb{E}\left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}y\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right] \right\},
\end{align*}
which is the desired result for the first part.

Let us turn to the second assertion.
For $k \geq 1$, combining (\ref{y}), (\ref{eta'})
 and (\ref{detGG}) gives
\begin{align*}
    &\mathrel{\phantom{=}} \mathbb{E} \left[ \left|\operatorname{det} G\right|  \mathbf{1}\{ i(G) = k \} \right] \\
    &= \mathbb{E} \left[ \left|\operatorname{det} G\right|  \left( \mathbf{1}\{ i\left(G_{* *}\right)=k, \eta (\zeta_1,G_{**}) >0 \} + \mathbf{1}\{ i\left(G_{* *}\right)=k-1, \eta (\zeta_1,G_{**}) <0 \} \right) \right] \\
    &= \int_{-\infty}^{\infty} \frac{2^{N-1} \left( -D_N'' (0) \right)^\frac{N-1}{2}}{\sqrt{2 \pi(1+d_1+2d_2+d_3)}} e^{-\frac{y^{2}}{2 (1+d_1+2d_2+d_3)}} \\
    &\quad \times \mathbb{E} \left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left| \left(m_1+2\sqrt{-D_N'' (0)}x\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N'' (0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right. \right. \\
    &\phantom{XXXXXXX} \left. \left. \vphantom{\left| \prod_{j=1}^{N-1} \right|} \times \left( \mathbf{1} \left\{\lambda_k<-m_3<\lambda_{k+1}, \eta' (\Lambda)>0 \right\} + \mathbf{1} \left\{\lambda_{k-1}<-m_3<\lambda_{k}, \eta'(\Lambda)<0\right\} \right) \right] \right\} \dd y.
\end{align*}
Plugging the above equation into (\ref{R1R2_2}) yields that
\begin{align*}
    &\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_{N,k}\left(E, \left(R_{1}, R_{2}\right)\right) \\
    &= \frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_1}^{R_2} \int_{E}\int_{-\infty}^{\infty}\frac{e^{-\frac{2D_N''(0)y^{2}}{ (6D_N''(0)+(\beta+\alpha\rho^2)^2)}}}{\sqrt{2 \pi(-6D_N''(0)-(\beta+\alpha\rho^2)^2)}}  \frac{ e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}} \rho^{N-1} \\
    & \quad\times \mathbb{E}\left\{\mathbb{E}_{\mathrm{GOI}(c)}^{N-1}\left[\left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}y\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right.\right. \\
    &\phantom{XXX} \left. \left. \vphantom{\left| \prod_{j=1}^{N-1} \right|} \times \left( \mathbf{1} \left\{\lambda_k<-m_3<\lambda_{k+1}, \eta' (\Lambda)>0 \right\} + \mathbf{1} \left\{\lambda_{k-1}<-m_3<\lambda_{k}, \eta'(\Lambda)<0\right\} \right) \right] \right\} \dd y \, \dd  u \, \dd \rho.
\end{align*}
Similarly, for $k=0$ we have
\begin{align*}
    &\mathrel{\phantom{=}} \mathbb{E} \left[ \left|\operatorname{det} G\right|  \mathbf{1}\{ i(G) = 0 \} \right] \\
    &= \mathbb{E} \left[ \left|\operatorname{det} G\right|  \mathbf{1}\{ i\left(G_{* *}\right)=0, \eta (\zeta_1,G_{**}) >0 \} \right] \\
    &= \int_{-\infty}^{\infty} \frac{2^{N-1} \left( -D_N'' (0) \right)^\frac{N-1}{2}}{\sqrt{2 \pi(1+d_1+2d_2+d_3)}} e^{-\frac{y^{2}}{2 (1+d_1+2d_2+d_3)}} \\
    &\quad \times \mathbb{E} \left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left| \left(m_1+2\sqrt{-D_N'' (0)}x\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N'' (0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right. \right. \\
    &\phantom{XXXXXXXXXXXXXXXXXXXXXXXXXXXXX} \left. \left. \vphantom{\left| \prod_{j=1}^{N-1} \right|} \times \mathbf{1} \left\{\lambda_1 > -m_3, \eta' (\Lambda)>0 \right\} \right] \right\} \dd y,
\end{align*}
and
\begin{align*}
    &\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_{N,0}\left(E, \left(R_{1}, R_{2}\right)\right) \\
    &= \frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_1}^{R_2} \int_{E}\int_{-\infty}^{\infty}\frac{e^{-\frac{2D_N''(0)y^{2}}{ (6D_N''(0)+(\beta+\alpha\rho^2)^2)}}}{\sqrt{2 \pi(-6D_N''(0)-(\beta+\alpha\rho^2)^2)}}  \frac{ e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}} \rho^{N-1} \\
    & \quad\times \mathbb{E} \left\{ \mathbb{E}_{\mathrm{GOI}(c)}^{N-1} \left[ \left|\left(m_1+2\sqrt{-D_N^{\prime \prime}(0)}y\right)\prod_{j=1}^{N-1}(\lambda_j+m_3)-\sqrt{-D_N^{\prime \prime}(0)}\sum_{l=1}^{N-1} Z_l^2 \prod_{j \neq l}^{N-1}(\lambda_j+m_3)\right| \right.\right. \\
&\phantom{XXXXXXXXXXXXXXXXXXXXXXXXXX} \left. \left. \vphantom{\left| \prod_{j=1}^{N-1} \right|} \times \mathbf{1} \left\{\lambda_1 > -m_3, \eta' (\Lambda)>0 \right\} \right] \right\} \dd y \, \dd  u \, \dd \rho,
\end{align*}
which completes the proof of the second part.
\end{proof}

\subsection{Representation with GOE matrices}
In order for the large $N$ asymptotic analysis, it is desirable to write the GOI matrix in the above representation as a sum of a GOE matrix and an independent scalar matrix. Most of results in this subsection follow from arguments similar to those in \cite{AZ20,AZ22}. First, as observed in these works, if the critical values are not restricted (i.e., $E=\rz$), there is no need to consider the conditional distribution of Hessian and it is straightforward to employ GOE matrices for the Kac--Rice representation.
\begin{theorem}
Let $H_N=\left\{H_N(x): x \in \mathbb{R}^N\right\}$ be a non-isotropic Gaussian field with isotropic increments and $T_N$ be a Borel subset of $\mathbb{R}^{N}$.
Assume Assumptions \ref{assumption1} and \ref{assumption2}, we have
\begin{align*}
 \mathbb{E} \operatorname{Crt}_N\left( \mathbb{R}, T_N\right) &= \frac{\left(-2D_N''(0)\right)^{N/2}|T_N|}{\pi^{(N+1)/2} D_N^{\prime}(0)^{N / 2}}\int_{-\infty}^{\infty}\mathbb{E}_{\mathrm{GOE}}^N\left[ \prod_{j=1}^N\left|\lambda_j+y\right|\right]e^{-y^2}\mathrm{d} y, \\
\mathbb{E} \operatorname{Crt}_{N,k}\left(\mathbb{R},T_N\right) &= \frac{\left(-2D_N''(0)\right)^{N/2}|T_N|}{\pi^{(N+1)/2} D_N^{\prime}(0)^{N / 2}}  \int_{-\infty}^{\infty} \mathbb{E}_{\mathrm{GOE}}^N \left[ \prod_{j=1}^N\left|\lambda_j+y\right|\mathbf{1}{ \{\lambda_k<-y<\lambda_{k+1}\} }\right] e^{-y^2}  \mathrm{d} y,
\end{align*}
where $|T_N|$ is the Lebesgue measure of $T_N$, $\lambda_1 \leq \cdots \leq \lambda_N$ are the ordered eigenvalues of the GOE matrix $\GOE_{N}$, and by convention $\lambda_{N+1}=\infty$.
\end{theorem}
The proof is omitted here since it is the same as for \cite{AZ20}*{Theorem 1.1} and \cite{AZ22}*{Theorems 1.1 and 1.2}. The situation is more involved when the critical values are constrained.
The following condition for any fixed $N$ turns out to be sufficient for using GOE matrices in the representation.

\begin{assumption} \label{assumption3}
    For any $r>0$, we have %for all $N\in\nz$ and $N=\infty$
\begin{align}\label{ar}
-2 D_N^{\prime \prime}(0) &>\left(\alpha r+\beta\right) \beta, \\
-4 D_N^{\prime \prime}(0) &>\left(\alpha r+\beta\right) \alpha r,\label{be}\\
\alpha\beta &>0,\nonumber
\end{align}
where $\alpha=\alpha\left(r \right)$ and $\beta=\beta\left(r \right)$ are defined in (\ref{eq:msialbt}) with $\rho^2$ replaced by $r$.
\end{assumption}

This condition was used in \cite{AZ20} for the structure functions in the class $\dx_\8$, and in that case we trivially have $\alpha(r)\beta(r)>0$ for any $r>0$. Actually, we will show that the assumption always holds for all structure functions in $\dx_\8$ in the next section. However, for the class $\dx_N$ with $N\in\nz$, such an assumption is not expected to be always true due to the oscillating nature of the Bessel functions in the representation of structure functions \eqref{DNr}. The following result shows that Assumption \ref{assumption3} is stronger than the nondegeneracy of the field and its derivatives.

%In fact, the condition of Assumption III is stronger than the nondegenerate condition (\ref{nondg}) and we show the claim in the following lemma. In the special case $N=\infty$, we also prove that inequality (\ref{be}) can be deduced by inequality (\ref{ar}), which is deferred to the next section.
\begin{lemma}
For all $N\in\nz$ fixed, if the Assumption \ref{assumption3} holds, then the Gaussian vector $\left(H_N(x), \nabla H_N(x), \partial_{i j} H_{N}(x),1\leq i\leq j\leq N\right)$ is nondegenerate.
\end{lemma}

\begin{proof}
 Using the same notations as for Lemma \ref{Nond}, we write
\begin{align*}
\textbf{$\mathbf{A}_{11}$}=\begin{pmatrix}
      D_N\left(\|x\|^{2}\right)& \xi_1^\mathsf T\\
       \xi_1  &  \textbf{$\mathbf{A}$} \\
\end{pmatrix},
\textbf{$\mathbf{A}_{12}$}=\begin{pmatrix}
     \xi_2^\mathsf T& \xi_3^\mathsf T\\
       \mathbf{0}_1^\mathsf T  &   \mathbf{0}_2^\mathsf T \\
       \end{pmatrix},
\textbf{$\mathbf{A}_{21}$}=\begin{pmatrix}
     \xi_2 &  \mathbf{0}_1\\
       \xi_3  &   \mathbf{0}_2 \\
       \end{pmatrix},
       \textbf{$\mathbf{A}_{22}$}=
    \begin{pmatrix}
     \textbf{$\mathbf{B}$}& \mathbf{0}_3^\mathsf T \\
      \mathbf{0}_3 &  \textbf{$\mathbf{C}$} \\
    \end{pmatrix}.
  \end{align*}
By Assumption \ref{assumption3}, $D_N(\|x\|^{2})-\frac{D_N^{\prime}(\|x\|^{2})^{2}\|x\|^{2}}{D_N^{\prime}(0)}>0$ for all $x \in \mathbb{R}^N \backslash\{0\}$, which means that the matrix $\mathbf{A}_{11}$ is positive definite. Following \cite{AZ20}*{Section 3}, Assumption \ref{assumption3} implies that the covariance matrix of vector $(\partial_{i j} H_{N}(x),1\leq i\leq j\leq N)$ conditioning on $Y=H_N(x)-\frac{D_N^{\prime}\left(\|x\|^2\right) \sum_{i=1}^N x_i \partial_i H_N(x)}{D_N^{\prime}(0)}$ is  positive definite, since the conditional distribution can be represented using GOE matrices. Observing that both $Y$ and $(\partial_{i j} H_{N}(x),1\leq i\leq j\leq N)$ are independent of $\nabla H_N(x)$, we deduce that the covariance matrix of $(\partial_{i j} H_{N}(x),1\leq i\leq j\leq N)$ conditioning on the vector $\left(H_N(x),\nabla H_N(x)\right)$ is the same as that conditioning on $Y$.  But the covariance matrix of $(\partial_{i j} H_{N}(x),1\leq i\leq j\leq N)$ conditioning on $\left(H_N(x),\nabla H_N(x)\right)$ is
$\mathbf{D}:=\mathbf{A}_{22}-\mathbf{A}_{21}\mathbf{A}_{11}^{-1}\mathbf{A}_{12}$,
  which is positive definite by the above analysis. By the Schur complement formula, we may rewrite the covariance matrix of vector $\left(H_N(x), \nabla H_N(x), \partial_{i j} H_{N}(x),1\leq i\leq j\leq N\right)$ as
  \begin{align*}
\textbf{$\mathbf{G}$}=\begin{pmatrix}
    \mathbf{I}_{N+1}& \mathbf{0}_4\\
    \mathbf{A}_{21}\mathbf{A}_{11}^{-1}  &   \mathbf{I}_{\frac{N(N+1)}{2}} \\
       \end{pmatrix}\begin{pmatrix}
    \mathbf{A}_{11}& \mathbf{0}_4\\
      \mathbf{0}_4^{\mathsf T}&   \mathbf{D} \\
       \end{pmatrix}
       \begin{pmatrix}
\mathbf{I}_{N+1}&\mathbf{A}_{11}^{-1}\mathbf{A}_{12} \\ \mathbf{0}_4^{\mathsf T}
      &   \mathbf{I}_{\frac{N(N+1)}{2}} \\
       \end{pmatrix},
  \end{align*}
  where $\mathbf{0}_4$ is an $(N+1) \times\frac{N(N+1)}{2}$ matrix with all elements equal to zero.
  Since both $\mathbf{A}_{11}$ and $\mathbf{D}$ are positive definite matrices, $\mathbf{G}$ is clearly a positive definite matrix, which gives the assertion.
\end{proof}

Under Assumption \ref{assumption3}, according to \cite{AZ20}*{Section 3}, the matrix $G$ in ($\ref{G}$) can be represented as the following form
\begin{align}\label{eq:gun}
    G= G(u) =
    \begin{pmatrix}
      z_1'& \xi^\mathsf T \\
       \xi & \sqrt{-4D_N''(0)} (\GOE_{N-1}-z_3'\mathbf{I}_{N-1})
    \end{pmatrix}=: \begin{pmatrix}
        z_1'& \xi^\mathsf T \\
         \xi & G_{**}
      \end{pmatrix}  ,
  \end{align}
  where with $z_1,z_2,z_3$ being independent standard Gaussian random variables,
  \begin{align*}
    z_1'&=\si_1 z_1 - \si_2  z_2 + m_1, \quad
z_3'=\frac1{\sqrt{-4D_N''(0)}}\Big(\si_2 z_2+   \sqrt{\al\beta}\rho z_3 - m_2\Big),
  \end{align*}
  $\xi$ is a centered column Gaussian vector with covariance matrix $-2D_N''(0)\mathbf{I}_{N-1}$, which is independent of $z_1,z_2,z_3$ and the GOE matrix $\GOE_{N-1}$.
The conditional
distribution of $z_1^{\prime}$ given $z_3^{\prime}=y$ is
\begin{align}\label{eq:cond}
( z_1^{\prime} \mid z_3^{\prime}=y ) \sim N\left(\overline{\mathrm{a}}, \mathrm{b}^2\right),
\end{align}
where
\begin{align}\label{eq:ab2}
\overline{\mathrm{a}}&=  m_1-\frac{\sigma_2^2\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)}{\sigma_2^2+\alpha \beta \rho^2}\nonumber \\
&=  \frac{-2 D_N^{\prime \prime}(0) \alpha \rho^2 u}{\left(-2 D_N^{\prime \prime}(0)-\beta^2\right) \sqrt{D_N\left(\rho^2\right)-\frac{D_N^{\prime}\left(\rho^2\right)^2 \rho^2}{D_N^{\prime}(0)}}} -\frac{\left(-2 D_N^{\prime \prime}(0)-\beta^2-\alpha \beta \rho^2\right) \sqrt{-4 D_N^{\prime \prime}(0)} y}{-2 D_N^{\prime \prime}(0)-\beta^2}, \nonumber \\
\mathrm{b}^2&=  \sigma_1^2+\sigma_2^2-\frac{\sigma_2^4}{\sigma_2^2+\alpha \beta \rho^2}=-4 D_N^{\prime \prime}(0)+\frac{2 D_N^{\prime \prime}(0) \alpha^2 \rho^4}{\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)} .
\end{align}
Define random variable
\begin{align}\label{aN}
\mathrm{a}_N
=\mathrm{a}_N(\rho, u, y)
=\overline{\mathrm{a}} - \sqrt{-D_N''(0)} \sum_{i=1}^{N-1} \frac{Z_i^2}{ \lambda_i-y},
%=\overline{\mathrm{a}}-Q(y),
\end{align}
%where
%\begin{align}\label{Q}
%Q=Q\left(z_3^{\prime}\right)
%=\xi^{\top} G_{* *}^{-1} \xi
%\stackrel{d}{=}\sqrt{-D_N^{\prime \prime}(0)} \sum_{i=1}^{N-1} \frac{Z_i^2}{ \lambda_i-z_3^{\prime}},
%\end{align}
where $Z_i, 1\leq i \leq N-1,$ are independent standard Gaussian random variables and $\lambda_1 \leq \cdots \leq \lambda_{N-1}$ are the ordered eigenvalues of the GOE matrix $\GOE_{N-1}$. By the above analysis, given Assumption \ref{assumption3}, we can express the expected number of critical points (with or without given indices) of non-isotropic Gaussian random fields with isotropic increments using the eigenvalue density of the GOE matrix. We omit the proof here since it follows from an easy adaption of the arguments in \cites{AZ20,AZ22}.


\begin{theorem}\label{shell}
Let $H_N=\left\{H_N(x), x \in \mathbb{R}^N\right\}$ be a non-isotropic Gaussian field with isotropic increments. Let $E \subset \mathbb{R}$ be a Borel set and $T_N$ be the shell domain $T_{N}\left(R_{1}, R_{2}\right)=\{x \in \mathbb{R}^{N}: R_{1}<\|x\|<R_{2}\}$, where $0 \leq R_{1}<R_{2} < \infty$.
Assume Assumptions \ref{assumption1}, \ref{assumption2}, and \ref{assumption3}. Then we have
\begin{align*}
 &\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_N\left(E, \left(R_{1}, R_{2}\right)\right)\\
 &=\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_{1}}^{R_{2}} \int_{E} \int_{\mathbb{R}} \frac{\exp \left\{ -\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)} - \frac{u^{2}}{2 \sigma_{Y}^{2}} \right\}}{2 \pi \sigma_{Y} \sqrt{-2 D_N^{\prime \prime}(0)-\beta^2}} \rho^{N-1}\\
& \quad \times \mathbb{E}\left\{\mathbb{E}_{\mathrm{GOE}}^{N-1}\left[ \prod_{j=1}^{N-1}\left|\lambda_j-y\right| \left(\mathrm{a}_N \left(\Phi\left(\frac{ \mathrm{a}_N}{\mathrm{~b}}\right)-\Phi\left(-\frac{ \mathrm{a}_N}{\mathrm{~b}}\right)\right)+\frac{\sqrt{2}\mathrm{b}}{\sqrt{\pi }} e^{-\frac{\mathrm{a}_N^2}{2 \mathrm{b}^2}}\right)\right]\right\} \mathrm{d} y \, \dd  u \, \dd  \rho,
%\end{align*}
\intertext{for $k\ge 1$,}
%\begin{align*}
&\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_{N,k}\left(E, \left(R_{1}, R_{2}\right)\right)\\
&=\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_{1}}^{R_{2}} \int_{E} \int_{\mathbb{R}} \frac{\exp \left\{ -\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)} - \frac{u^{2}}{2 \sigma_{Y}^{2}} \right\}}{2 \pi \sigma_{Y} \sqrt{-2 D_N^{\prime \prime}(0)-\beta^2}} \rho^{N-1} \\
&\quad \times \mathbb{E}\left\{\mathbb{E}_{\mathrm{GOE}}^{N-1}\left[ \prod_{j=1}^{N-1}\left|\lambda_j-y\right| \left(\mathrm{a}_N \Phi\left(\frac{ \mathrm{a}_N}{\mathrm{b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{ \mathrm{a}_N^2}{2\mathrm{b}^2}}\right) \mathbf{1}\left\{\lambda_k< y<\lambda_{k+1}\right\} \right. \right. \\
&\quad \left. \left. + \prod_{j=1}^{N-1}\left|\lambda_j-y\right| \left(-\mathrm{a}_N \Phi\left(-\frac{ \mathrm{a}_N}{\mathrm{b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{ \mathrm{a}_N^2}{2\mathrm{b}^2}}\right) \mathbf{1} \left\{\lambda_{k-1}<y<\lambda_{k}\right\} \right]\right\} \mathrm{d} y \, \dd  u \, \dd  \rho,
%&\quad+\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_{1}}^{R_{2}} \int_{E} \int_{\mathbb{R}} \mathbb{E}\left\{\mathbb{E}_{\mathrm{GOE}}^{N-1}\left[ \prod_{j=1}^{N-1}\left|\lambda_j-y\right| \mathbf{1}\left\{\lambda_{k-1}<y<\lambda_{k}\right\}\right.\right. \\
%&\quad\quad\quad \left. \left. \vphantom{\prod_{j=1}^{N-1}} \left(-\mathrm{a}_N \Phi\left(-\frac{ \mathrm{a}_N}{\mathrm{b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{ \mathrm{a}_N^2}{2\mathrm{b}^2}}\right)\right]\right\}\frac{\exp \left\{-\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}\right\}}{\sqrt{2 \pi\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}}\frac{e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}}  \rho^{N-1} \mathrm{d} y  \dd  u \dd  \rho,
%\end{align*}
\intertext{and for $k=0$,}
%\begin{align*}
&\mathrel{\phantom{=}} \mathbb{E} \operatorname{Crt}_{N,0}\left(E, \left(R_{1}, R_{2}\right)\right)\\
&=\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_{1}}^{R_{2}} \int_{E} \int_{\mathbb{R}} \frac{\exp \left\{ -\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)} - \frac{u^{2}}{2 \sigma_{Y}^{2}} \right\}}{2 \pi \sigma_{Y} \sqrt{-2 D_N^{\prime \prime}(0)-\beta^2}} \rho^{N-1} \\
&\quad \times \mathbb{E}\left\{\mathbb{E}_{\mathrm{GOE}}^{N-1}\left[ \prod_{j=1}^{N-1}\left|\lambda_j-y\right| \left(\mathrm{a}_N \Phi\left(\frac{ \mathrm{a}_N}{\mathrm{b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{ \mathrm{a}_N^2}{2\mathrm{b}^2}}\right) \mathbf{1} \left\{ y<\lambda_{1} \right\} \right]\right\} \mathrm{d} y \, \dd  u \, \dd  \rho,
%&=\frac{2\left(-2D_N''(0)\right)^{N/2}}{ D_N^{\prime}(0)^{N / 2}\Gamma(N / 2)}\int_{R_{1}}^{R_{2}} \int_{E} \int_{\mathbb{R}} \mathbb{E}\left\{\mathbb{E}_{\mathrm{GOE}}^{N-1}\left[ \prod_{j=1}^{N-1}\left|\lambda_j-y\right| \mathbf{1}\left\{ y<\lambda_{1}\right\}\right. \right. \\
%&\quad\quad\quad \left. \left. \vphantom{\prod_{j=1}^{N-1}} \left(\mathrm{a}_N \Phi\left(\frac{ \mathrm{a}_N}{\mathrm{b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{ \mathrm{a}_N^2}{2\mathrm{b}^2}}\right)\right]\right\} \frac{\exp \left\{-\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}\right\}}{\sqrt{2 \pi\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}}\frac{e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}}{\sqrt{2 \pi} \sigma_{Y}}  \rho^{N-1} \mathrm{d} y  \dd  u \dd  \rho,
\end{align*}
where $m_2$ and $ \si_Y$ are given by (\ref{eq:msialbt}), $\mathrm{b}$ and $\mathrm{a}_N$ are defined in (\ref{eq:ab2}) and (\ref{aN}), $\Phi$ is the c.d.f.\ of the standard Gaussian random variable and by convention $\lambda_0=-\infty$, $\lambda_N=\infty$.
\end{theorem}


\begin{comment}
\begin{proof}
  From Lemma \ref{Corv}, we have $\operatorname{Var}\left(H_N(x)\right)= D_N\left(\|x\|^2\right)$ and $\Sigma_{01}:=\operatorname{Cov}\left(H_N(x), \nabla H_N(x)\right)=D_N^{\prime}\left(\|x\|^2\right) x^{\top}$, $\Sigma_{11}:=\operatorname{Cov}\left(\nabla H_N(x)\right)=D_N^{\prime}(0) I_N$. Note that
$$
Y:=H_N(x)-\Sigma_{01} \Sigma_{11}^{-1} \nabla H_N(x)=H_N(x)-\frac{D_N^{\prime}\left(\|x\|^2\right) \sum_{i=1}^N x_i \partial_i H_N(x)}{D_N^{\prime}(0)}
$$
is independent with $\nabla H_N(x)$ and $\nabla H_N(x)$ is independent with $\nabla^2 H_N(x)$. By the Kac--Rice formula \cite[Theorem 11.2.1]{AT07},
\begin{align*}
& \mathbb{E} \operatorname{Crt}_N\left(E, B_N\right)\nonumber \\
& =\int_{B_N} \mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x)\right| \mathbf{1}\left\{Y+ \Sigma_{01} \Sigma_{11}^{-1} \nabla H_N(x) \in E\right\} \mid \nabla H_N(x)=0\right] p_{\nabla H_N(x)}(0) \mathrm{d} x \nonumber\\
& =\int_{B_N} \mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x)\right| \mathbf{1}\{Y \in E\}\right] p_{\nabla H_N(x)}(0) \mathrm{d} x.
\end{align*}
When $E=\mathbb{R}$, the restriction on $Y$ disappears. Using the spherical coordinates and writing $\rho=\|x\|$, we arrive at
\begin{align*}
 \mathbb{E} \operatorname{Crt}_N\left(\mathbb{R}, \left(R_{1}, R_{2}\right)\right)=S_{N-1}\int_{R_{1}}^{R_{2}}\mathbb{E}\left[\left|\operatorname{det} \nabla^2 H_N(x) \right| \right] p_{\nabla H_N(x)}(0) \rho^{N-1}\mathrm{d} \rho,
\end{align*}
where $p_{\nabla H_N(x)}(0)=\frac{1}{(2 \pi)^{N / 2} D_N^{\prime}(0)^{N / 2}}$ and $S_{N-1}=\frac{2 \pi^{N / 2}}{\Gamma(N / 2)}$ is the area of $N-1$ dimensional unit sphere.
By Lemma \ref{Corv}, the Hessian $\nabla^2 H_N(x)$ has same distribution with $\sqrt{-4D_N''(0)}\left(\GOE_N+zI_{N}\right)$, where $z$ is a Gaussian random variable with mean $0$ and variance $\frac{1}{2}$, which is independent with $\GOE_N$.
Conditioning on $z=y$, plugging the value of $p_{\nabla H_N(x)}(0)$ and $S_{N-1}$ implies
\begin{align*}
 \mathbb{E} \operatorname{Crt}_N\left(\mathbb{R}, \left(R_{1}, R_{2}\right)\right)=\frac{\left(-4D_N''(0)\right)^{N/2}}{(2 \pi)^{N / 2} D_N^{\prime}(0)^{N / 2}}\frac{2 \pi^{N / 2}}{\Gamma(N / 2)}\int_{R_{1}}^{R_{2}}\int_{-\infty}^{\infty}\mathbb{E}_{\mathrm{GOE}}^N\left[ \prod_{j=1}^N\left|\lambda_j+y\right|\right]\frac{1}{\sqrt{\pi}}e^{-y^2} \rho^{N-1}\mathrm{d} y\mathrm{d} \rho,
\end{align*}
which gives the desired result after some simplification.
\end{proof}
\end{comment}


\begin{comment}
\begin{proof}
By the Kac--Rice formula\cite[Theorem 11.2.1]{AT07}, conditioning on
$Y=u$, using the spherical coordinates and writing $\rho=\|x\|$,

\begin{align}\label{kacr}
&\mathbb{E} \operatorname{Crt}_{N}\left(E,\left(R_{1}, R_{2}\right)\right)\nonumber\\
&=S_{N-1}  \int_{R_{1}}^{R_{2}} \int_{E} \mathbb{E}[|\det G|] \frac{1}{\sqrt{2 \pi} \sigma_{Y}} e^{-\frac{u^{2}}{2 \sigma_{Y}^{2}}}\frac{1}{(2 \pi)^{N / 2} D_N^{\prime}(0)^{N / 2}} \rho^{N-1} \dd  u \dd  \rho;
\end{align}
see \cite[{(4.1)}]{AZ20} for details. Recall the representation (\ref{eq:gun}). Let $\lambda_1 \leq \cdots \leq \lambda_{N-1}$ be the ordered eigenvalues of $\operatorname{GOE}_{N-1}$. The eigenvalues of $G_{* *}$ are $\left\{\sqrt{-4 D_N^{\prime \prime}(0)}\left(\left(\frac{N-1}{N}\right)^{1 / 2} \lambda_i-z_3^{\prime}\right)\right\}_{i=1}^{N-1}$. Therefore, there exists a random orthogonal matrix $V$ which is independent of the eigenvalues of $\operatorname{GOE}_{N-1}$ and $z_3^{\prime}$, such that
$$
G_{* *}=\sqrt{-4 D_N^{\prime \prime}(0)} V^{\top}\left(\begin{array}{ccc}
\left(\frac{N-1}{N}\right)^{1 / 2} \lambda_1-z_3^{\prime} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \left(\frac{N-1}{N}\right)^{1 / 2} \lambda_{N-1}-z_3^{\prime}
\end{array}\right) V .
$$
In addition, $V \xi$ is a centered Gaussian vector with covariance matrix $-2 D_N^{\prime \prime}(0) I_{N-1}$ that is independent of $z_3^{\prime}$ and eigenvalues of $\operatorname{GOE}_{N-1}$. We  rewrite $V \xi \stackrel{d}{=} \sqrt{-2 D_N^{\prime \prime}(0)} Z$, where $Z=\left(Z_1, \ldots, Z_{N-1}\right)^{\top}$ is an $N-1$ dimensional standard Gaussian random vector.  Let us write
$$
\zeta=\zeta\left(z_1^{\prime}, z_3^{\prime}\right)=z_1^{\prime}-\xi^{\top} G_{* *}^{-1} \xi.
$$
 Using the Schur complement formula, we have
\begin{align}\label{eq:detg}
\det G=\det\left(G_{* *}\right)\left(z_1^{\prime}-\xi^{\top} G_{* *}^{-1} \xi\right).
\end{align}
Denote
\begin{align}\label{Q}
Q=Q\left(z_3^{\prime}\right)=\xi^{\top} G_{* *}^{-1} \xi\stackrel{d}{=}\sqrt{-D_N^{\prime \prime}(0)} \sum_{i=1}^{N-1} \frac{Z_i^2}{\left(\frac{N-1}{N}\right)^{1 / 2} \lambda_i-z_3^{\prime}},
\end{align}
so that $\zeta=z_1^{\prime}-Q\left(z_3^{\prime}\right).$ The conditional
distribution of $z_1^{\prime}$ given $z_3^{\prime}=y$ is given by
\begin{align}\label{eq:cond}
z_1^{\prime} \mid z_3^{\prime}=y \sim N\left(\overline{\mathrm{a}}, \mathrm{b}^2\right),
\end{align}
where

\begin{align}\label{eq:ab2}
\overline{\mathrm{a}}= & m_1-\frac{\sigma_2^2\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)}{\sigma_2^2+\alpha \beta \rho^2}\nonumber \\
= & \frac{-2 D_N^{\prime \prime}(0) \alpha \rho^2 u}{\left(-2 D_N^{\prime \prime}(0)-\beta^2\right) \sqrt{D_N\left(\rho^2\right)-\frac{D_N^{\prime}\left(\rho^2\right)^2 \rho^2}{D_N^{\prime}(0)}}} -\frac{\left(-2 D_N^{\prime \prime}(0)-\beta^2-\alpha \beta \rho^2\right) \sqrt{-4 D_N^{\prime \prime}(0)} y}{-2 D_N^{\prime \prime}(0)-\beta^2}, \nonumber \\
\mathrm{b}^2= & \sigma_1^2+\sigma_2^2-\frac{\sigma_2^4}{\sigma_2^2+\alpha \beta \rho^2}=-4 D_N^{\prime \prime}(0)+\frac{2 D_N^{\prime \prime}(0) \alpha^2 \rho^4}{\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)} .
\end{align}
Let $z$ be a standard Gaussian r.v. and $\Phi$ the c.d.f. of $z$. For $a \in \mathbb{R}, b>0$, notice that
\begin{align}\label{eq:exp}
\mathbb{E}[(a+b z) \mathbf{1}\{a+b z>0\}]&=a \Phi\left(\frac{a}{b}\right)+\frac{b}{\sqrt{2 \pi}} e^{-\frac{a^2}{2 b^2}},\nonumber \\
\mathbb{E}[-(a+b z) \mathbf{1}\{a+b z<0\}]&=-a \Phi\left(-\frac{a}{b}\right)+\frac{b}{\sqrt{2 \pi}} e^{-\frac{a^2}{2 b^2}}.
\end{align}
Using  (\ref{eq:detg}), (\ref{eq:cond}),(\ref{eq:exp}) and by conditioning gives
\begin{align}\label{eq:large}
\mathbb{E} & {\left[|\det G| \mathbf{1}\left\{ \zeta>0\right\}\right]=\mathbb{E}\left(\left|\det G_{* *} \right| | z_1^{\prime}-\xi^{\top} G_{* *}^{-1} \xi| \mathbf{1}\left\{ \zeta>0\right\}\right) }\nonumber \\
= & {\left[-4 D_N^{\prime \prime}(0)\right]^{\frac{N-1}{2}} \mathbb{E}\left[\left|\operatorname{det}\left(\left(\frac{N-1}{N}\right)^{1 / 2} \mathrm{GOE}_{N-1}-z_3^{\prime} I_{N-1}\right)\right| \right.}\nonumber \\
& \left.\quad \times\mathbb{E}\left(\left|z_1^{\prime}-Q\left(z_3^{\prime}\right)\right| \mathbf{1}\left\{z_1^{\prime}-Q\left(z_3^{\prime}\right)>0\right\} \mid \lambda_1^{N-1}, z_3^{\prime}, \xi\right)\right] \nonumber\\
= & {\left[-4 D_N^{\prime \prime}(0)\right]^{\frac{N-1}{2}} \int_{\mathbb{R}} \mathbb{E}_{\mathrm{GOE}}^N\left[ \prod_{j=1}^N\left|\left(\frac{N-1}{N}\right)^{1 / 2}\lambda_j-y\right|\left(\mathrm{a}_N \Phi\left(\frac{ \mathrm{a}_N}{\mathrm{b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{\mathrm{a}_N^2}{2 \mathrm{b}^2}}\right) \right]}\nonumber \\
& \quad \times \frac{\sqrt{-4 D_N^{\prime \prime}(0)} \exp \left\{-\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}\right\}}{\sqrt{2 \pi\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}} \mathrm{d} y,
\end{align}
where $\lambda_1^{N-1}$ is the $N-1$ eigenvalues of the $\mathrm{GOE}_{N-1}$ and
\begin{align}\label{aN}
\mathrm{a}_N=\mathrm{a}_N(\rho, u, y)=\overline{\mathrm{a}}-Q(y),
\end{align}
which is a random term depending on Gaussian vector $Z$ and eigenvalues of the $\mathrm{GOE}_{N-1}$.
Following the same arguments as above
\begin{align}\label{eq:small}
\mathbb{E} & {\left[|\det G| \mathbf{1}\left\{ \zeta<0\right\}\right]=\mathbb{E}\left(|\det G_{* *} | | z_1^{\prime}-\xi^{\top} G_{* *}^{-1} \xi| \mathbf{1}\left\{ \zeta<0\right\}\right) } \nonumber \\
= & {\left[-4 D_N^{\prime \prime}(0)\right]^{\frac{N-1}{2}} \int_{\mathbb{R}} \mathbb{E}_{\mathrm{GOE}}^N\left[ \prod_{j=1}^N\left|\left(\frac{N-1}{N}\right)^{1 / 2}\lambda_j-y\right|\left(-\mathrm{a}_N \Phi\left(-\frac{ \mathrm{a}_N}{\mathrm{~b}}\right)+\frac{\mathrm{b}}{\sqrt{2 \pi }} e^{-\frac{\mathrm{a}_N^2}{2 \mathrm{b}^2}}\right)\right]}\nonumber \\
& \quad \times \frac{\sqrt{-4 D_N^{\prime \prime}(0)} \exp \left\{-\frac{\left(\sqrt{-4 D_N^{\prime \prime}(0)} y+m_2\right)^2}{2\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}\right\}}{\sqrt{2 \pi\left(-2 D_N^{\prime \prime}(0)-\beta^2\right)}} \mathrm{d} y.
\end{align}
It is clear that
$$\mathbb{E} \left[|\det G| \right]=\mathbb{E} \left[|\det G| \mathbf{1}\left\{ \zeta>0\right\}\right]+\mathbb{E} \left[|\det G| \mathbf{1}\left\{ \zeta<0\right\}\right].$$
Therefore, combining (\ref{kacr}), (\ref{eq:large}) and (\ref{eq:small}), after some simplification, we finish the proof.
\end{proof}
To calculate $\mathbb{E} \operatorname{Crt}_{N,k}\left(E, \left(R_{1}, R_{2}\right)\right)$, we may need the following lemma.
\begin{lemma}
\cite[Equation 2]{Laz88}. Let $S$ be a symmetric block matrix, and write its inverse $S^{-1}$ in block form with the same block structure:
$$
S=\left(\begin{array}{cc}
A & B \\
B^{\top} & C
\end{array}\right), \quad S^{-1}=\left(\begin{array}{cc}
A^{\prime} & B^{\prime} \\
\left(B^{\prime}\right)^{\top} & C^{\prime}
\end{array}\right) .
$$
Then $\operatorname{sgn}(S)=\operatorname{sgn}(A)+\operatorname{sgn}\left(C^{\prime}\right)$, with $\operatorname{sgn}(M)$ denoting the signature of the matrix $M$.
\end{lemma}
 By the representation (\ref{eq:gun}), using the interlacement theorem, for all $j \in\{1, \ldots, N-1\}$,
$$
\lambda_j(G) \leq \lambda_j\left(G_{* *}\right) \leq \lambda_{j+1}(G),
$$
where $\lambda_j(M)$ are the ordered eigenvalues of $M$. Together with the above lemma,  for $k \geq 1$,
$$
\{i(G)=k\}=\left\{i\left(G_{* *}\right)=k, \zeta>0\right\} \cup\left\{i\left(G_{* *}\right)=k-1, \zeta<0\right\}
$$
and for $k=0$
$$
\{i(G)=0\}=\left\{i\left(G_{* *}\right)=0, \zeta>0\right\} .
$$
By the Kac-Rice formula, we have for $k\ge 1$
$$
\begin{aligned}
& \mathbb{E} \operatorname{Crt}_{N, k}\left(E,\left(R_1, R_2\right)\right)=S_{N-1} \int_{R_1}^{R_2} \int_E \mathbb{E}\left[| \det  G | \left(\mathbf{1}\left\{i\left(G_{* *}\right)=k, \zeta>0\right\}\right.\right. \\
& \left.\left.+\mathbf{1}\left\{i\left(G_{* *}\right)=k-1, \zeta<0\right\}\right)\right] \frac{1}{\sqrt{2 \pi} \sigma_Y} e^{-\frac{u^2}{2 \sigma_Y^2}} \frac{1}{(2 \pi)^{N / 2} D_N^{\prime}(0)^{N / 2}} \rho^{N-1} \mathrm{~d} u \mathrm{~d} \rho.
\end{aligned}
$$
By the same arguments as for $\mathbb{E}\operatorname{Crt}_{N}\left(E, \left(R_{1}, R_{2}\right)\right)$, we get the following corollary immediately.
\end{comment}

\section{Representation for fields with structure functions in $\mathcal{D}_\infty$} \label{sec3}

In this section, we show that if a structure function has the representation (\ref{Dr}), then Assumption \ref{assumption3} always holds.
\begin{lemma}
When $D (r)$  has the form (\ref{Dr}), if the inequality (\ref{ar}) holds, then both inequalities
(\ref{larn}) and (\ref{be}) hold automatically.
\end{lemma}
\begin{proof}
Since $D (r)$ has the form (\ref{Dr}), the function $D^{\prime}(r)$ is positive and strictly convex on $[0,\infty)$. Together with the mean value theorem, for any $r>0$, we have $D^{\prime\prime}(r)r>D^{\prime}(r)-D^{\prime}(0)$, which further implies
\begin{align}\label{ab}
\alpha r>2 \beta,\quad  \quad\alpha\beta r<2 \beta^2.
\end{align}
If the inequality (\ref{ar}) holds, by (\ref{ab}), we deduce that $\left(\alpha r+\beta\right) \alpha r<2\left(\alpha r+\beta\right)\beta<-4 D^{\prime \prime}(0)$, which yields the inequality (\ref{be}). On the other hand, for any $r>0$, note that
\begin{align*}
\frac{2D^{\prime\prime}(r)^2
r^{2}+\left(D^{\prime}(r)-D^{\prime}(0)\right)^2}{D\left(r\right)-\frac{D^{\prime}(r)^2 r}{D^{\prime}(0)}}=\frac{\alpha^2 r^2}{2}+\beta^2.
\end{align*}
Thus, to prove (\ref{larn}), we only need to show
\begin{align*}
-2D^{\prime\prime}(0)>\frac{\alpha^2 r^2}{2}+\beta^2.
\end{align*}
Similarly, from (\ref{ab}), we have $\frac{\alpha^2 r^2}{2}+\beta^2<\left(\alpha r+\beta\right) \beta<-2D^{\prime\prime}(0),$ which proves (\ref{larn}).
\end{proof}
Recall that
a function $f \colon (0, \infty) \to \mathbb{R}$ is a Bernstein function if $f$ is of class $C^{\infty}, f(\lambda) \geq 0$ for all $\lambda>0$ and
$(-1)^{n-1} f^{(n)}(\lambda) \geq 0$ for all $n \in \mathbb{N}$ and $\lambda>0$. For the Bernstein functions, we have the following theorem.
\begin{theorem}[{\cite[Theorem 3.2]{SSV12}}]
 A function $f:(0, \infty) \rightarrow \mathbb{R}$ is a Bernstein function if and only if, it admits the representation
$$
f(\lambda)=a+b \lambda+\int_{(0, \infty)}\left(1-e^{-\lambda t}\right) \mu(\dd t),
$$
where $a, b \geq 0$ and $\mu$ is a measure on $(0, \infty)$ satisfying $\int_{(0, \infty)}(1 \wedge t) \mu(\dd t)<\infty$. In particular, the triplet $(a, b, \mu)$ determines $f$ uniquely and vice versa.\label{Bern}
\end{theorem}
In the paper \cite{AZ20}, the authors conjecture that all Bernstein functions with the form of $D(r)$ defined in (\ref{Dr}) satisfy the inequality (\ref{ar}).
The following theorem shows that it is indeed true.
\begin{theorem}
Assume Assumptions \ref{assumption1} and \ref{assumption2}. Let $D (r)$ denote the Bernstein function defined in (\ref{Dr}).
     Then we have
\begin{equation*}
        -2 D'' (0) > (\alpha r + \beta) \beta, \quad \forall r > 0,
    \end{equation*}
    where $\alpha=\alpha\left(r \right)$ and $\beta=\beta\left(r \right)$ are defined in (\ref{eq:msialbt}) with $\rho^2$ replaced by $r$.
\end{theorem}

\begin{proof}
    It suffices to show
    \begin{align}
        -2 D''(0) \left( D(r) + r D'(0) - 2r D'(r) \right) &> 2r D''(0) \left( D'(r) - D'(0) \right) + \left( D'(r) - D'(0) \right)^2, \label{eq:thm2pr1} \\
        2r D''(0) \frac{\left( D'(r) - D'(0) \right)^2}{D'(0)} &\geq 2r \left( D''(r) - D''(0) \right) \left( D'(r) - D'(0) \right), \label{eq:thm2pr2}
    \end{align}
    for any $r > 0$. In fact, adding the above two inequalities implies that
    \begin{equation*}
        -2 D'' (0) \left( D(r) - \frac{D' (r)^2}{D' (0)} r \right)
        > 2r D'' (r) \left( D'(r) - D'(0) \right) + \left( D'(r) - D'(0) \right)^2, \quad \forall r > 0,
    \end{equation*}
    which is equivalent to the desired conclusion.

    To prove \eqref{eq:thm2pr1}, notice that
    \begin{gather*}
        - D''(0) = \int_{(0, \infty)} t^2 \, \nu(\mathrm{d} t) > 0, \\
        \left( D'(r) - D'(0) \right)^2
        = \left[ \int_{(0, \infty)} t \left( e^{-rt} - 1 \right) \nu(\mathrm{d} t) \right]^2
        \leq -D''(0) \int_{(0, \infty)} \left( e^{-rt} - 1 \right)^2 \nu(\mathrm{d} t).
    \end{gather*}
    Therefore, it suffices to show
    \begin{equation*}
        2 \left( D(r) + r D'(0) - 2r D'(r) \right) > -2r \left( D'(r) - D'(0) \right) + \int_{(0, \infty)} \left( e^{-rt} - 1 \right)^2 \nu(\mathrm{d} t), \quad \forall r > 0,
    \end{equation*}
    i.e.,
    \begin{equation*}
        2D(r) - 2r D'(r) - \int_{(0, \infty)} \left( 1 - e^{-rt} \right)^2 \nu(\mathrm{d} t) > 0, \quad \forall r > 0.
    \end{equation*}
    By the definition of $D(r)$ we have
    \begin{align*}
        &\mathrel{\phantom{=}} 2D(r) - 2r D'(r) - \int_{(0, \infty)} \left( 1 - e^{-rt} \right)^2 \nu(\mathrm{d} t) \\
        &= 2 \int_{(0, \infty)} \left( 1 - e^{-rt} \right) \nu (\mathrm{d} t) + 2Ar - 2r \int_{(0, \infty)} t e^{-rt} \, \nu(\mathrm{d} t) - 2Ar - \int_{(0, \infty)} \left( 1 - e^{-rt} \right)^2 \nu(\mathrm{d} t) \\
        &= \int_{(0, \infty)} \left( 1 - 2rt e^{-rt} - e^{-2rt} \right) \nu(\mathrm{d} t).
    \end{align*}
    Since $1 - 2x e^{-x} - e^{-2x} > 0$ for any $x > 0$,
    \begin{equation*}
        2D(r) - 2r D'(r) - \int_{(0, \infty)} \left( 1 - e^{-rt} \right)^2 \nu(\mathrm{d} t)
        = \int_{(0, \infty)} \left( 1 - 2rt e^{-rt} - e^{-2rt} \right) \nu(\mathrm{d} t)
        > 0, \quad \forall r > 0,
    \end{equation*}
    which shows \eqref{eq:thm2pr1}.

    To prove \eqref{eq:thm2pr2}, it suffices to show that
    \begin{equation*}
        D''(0) \left( D'(r) - D'(0) \right) \leq D'(0) \left( D''(r) - D''(0) \right), \quad \forall r > 0,
    \end{equation*}
    as $D'(0) > 0$ and $D'(r) - D'(0) < 0$. By definition,
    \begin{align*}
        &\mathrel{\phantom{=}} D''(0) \left( D'(r) - D'(0) \right) - D'(0) \left( D''(r) - D''(0) \right) \\
        &= -\int_{(0, \infty)} s^2 \, \nu (\mathrm{d} s) \int_{(0, \infty)} t \left( e^{-rt} - 1 \right) \nu (\mathrm{d} t) - \left( \int_{(0, \infty)} t \, \nu (\mathrm{d} t) + A \right) \int_{(0, \infty)} s^2 \left( 1 - e^{-rs} \right) \nu (\mathrm{d} s) \\
        &= \int_{(0, \infty)^2} s^2 t \left( e^{-rs} - e^{-rt} \right) \nu (\mathrm{d} s) \nu (\mathrm{d} t) - A \int_{(0, \infty)} s^2 \left( 1 - e^{-rs} \right) \nu (\mathrm{d} s) \\
        &= \frac{1}{2} \int_{(0, \infty)^2} s t (s - t) \left( e^{-rs} - e^{-rt} \right) \nu (\mathrm{d} s) \nu (\mathrm{d} t) - A \int_{(0, \infty)} s^2 \left( 1 - e^{-rs} \right) \nu (\mathrm{d} s).
    \end{align*}
    Since $A \geq 0$, $s^2 \left( 1 - e^{-rs} \right) > 0$, and $s t (s - t) \left( e^{-rs} - e^{-rt} \right) \leq 0$ for any $s, t, r > 0$, we have
    \begin{equation*}
        D''(0) \left( D'(r) - D'(0) \right) - D'(0) \left( D''(r) - D''(0) \right)
        \leq 0, \quad \forall r > 0,
    \end{equation*}
    which gives \eqref{eq:thm2pr2}.
\end{proof}

\begin{comment}
\begin{remark}
    For $A = 0$ and infinite measure $\nu$, this proof only implies a slightly weaker conclusion,
    \begin{equation*}
        -2 D'' (0) \geqslant (\alpha r + \beta) \beta, \quad \forall r > 0.
    \end{equation*}
    But a modification of Lemma \ref{pro} can give the same conclusion, i.e.
    \begin{equation*}
        -2 D'' (0) > (\alpha r + \beta) \beta, \quad \forall r > 0.
    \end{equation*}
\end{remark}

\begin{proof}
    It suffices to show
    \begin{equation*}
        -2 D'' (0) \left( D(r) - \frac{D' (r)^2}{D' (0)} r \right)
        > (2r D'' (r) + D' (r) - D' (0)) (D' (r) - D' (0)), \quad \forall r > 0.
    \end{equation*}
    With the following definition
    \begin{equation*}
        D_0 (r) = \int_{(0, +\infty)} \left( 1 - e^{-r t} \right) \nu (\mathrm{d} t), \quad r \in \mathbb{R}_+,
    \end{equation*}
    we have $-2 D'' (0) = -2 D_0'' (0) > 0$, and
    \begin{equation*}
        (2r D'' (r) + D' (r) - D' (0)) (D' (r) - D' (0))
        = (2r D_0'' (r) + D_0' (r) - D_0' (0)) (D_0' (r) - D_0' (0)).
    \end{equation*}
    Notice that
    \begin{align*}
        D(r) - \frac{D' (r)^2}{D' (0)} r
        &= D_0 (r) + Ar - \frac{(D_0' (r) + A)^2}{D_0' (0) + A} r \\
        &= D_0 (r) + (D_0' (0) - 2 D_0' (r)) r - \frac{(D_0' (0) - D_0' (r))^2}{D_0' (0) + A} r \\
        &\ge D_0 (r) + (D_0' (0) - 2 D_0' (r)) r - \frac{(D_0' (0) - D_0' (r))^2}{D_0' (0)} r \\
        &= D_0 (r) - \frac{D_0' (r)^2}{D_0' (0)} r,
    \end{align*}
    so it suffices to show that for any $r>0$
    \begin{equation} \label{eq:D0}
        -2 D_0'' (0) \left( D_0(r) - \frac{D_0' (r)^2}{D_0' (0)} r \right)
        > (2r D_0'' (r) + D_0' (r) - D_0' (0)) (D_0' (r) - D_0' (0)).
    \end{equation}

    According to Lemma \ref{pro}, \eqref{eq:D0} holds for any probability measure $\nu$ with finite second moment, and then for any finite measure $\nu$ owing to the homogeneity. In general, we denote
    \begin{equation*}
        \nu_n (B) = \nu \left( B \cap \left( \frac{1}{n}, +\infty \right) \right), \quad \forall B \in \mathcal{B} (0, +\infty), \qquad
        D_n (r) = \int_{(0, +\infty)} \left( 1 - e^{-r t} \right) \nu_n (\mathrm{d} t), \quad r \in \mathbb{R}_+,
    \end{equation*}
    for $n = 1, 2, \dots$, then $\nu_n (0, +\infty) = \nu (1/n, +\infty) < +\infty$ implies that \eqref{eq:D0} holds with $D_0$ replaced by $D_n$, $n = 1, 2, \dots$. The dominated convergence theorem shows that
    \begin{equation*}
        \lim_{n \to \infty} D_n^{(\kappa)} (r) = D_0^{(\kappa)} (r), \quad \forall r \in \mathbb{R}_+, \; \kappa = 0, 1, 2,
    \end{equation*}
    which completes the proof.
\end{proof}
\begin{remark}
   In fact, we may don't need the above approximate. In order to use the probability method, we reserve it and one may try to give a direct proof.
\end{remark}
\begin{lemma}\label{pro}
Let $\mu$ be a probability measure on $(0,\infty)$ with finite second moment and denote $D_0(r)=\int_{(0, \infty)}\left(1-e^{-r t}\right) \mu(\mathrm{d} t)$. Then for any $r>0$, we have
that the inequality (\ref{eq:D0}) holds.
\end{lemma}
\begin{proof}
Let $X$ denote the random variable  with distribution $\mu$, then we have
\begin{align}
D_0(r)=\mathbb{E}\left(1-e^{-rX}\right),\quad \quad D_0^{\prime}(r)=\mathbb{E}\left(Xe^{-rX}\right), \quad \quad D_0^{\prime \prime}(r)=-\mathbb{E}\left(X^2e^{-rX}\right).
\end{align}
To prove (\ref{eq:D0}), it suffices to show
\begin{align}
2\mathbb{E}\left(X^2\right)\mathbb{E}\left(1-e^{-rX}\right)-
2r\frac{\mathbb{E}\left(X^2\right)}{\mathbb{E}\left(X\right)}\left[\mathbb{E}\left(Xe^{-rX}\right)\right]^2
\geq&2r\mathbb{E}\left(X^2e^{-rX}\right)\mathbb{E}\left(X\left(1-e^{-rX}\right)\right)
\nonumber\\ & +\left[\mathbb{E}\left(X\left(1-e^{-rX}\right)\right)\right]^2.
\end{align}
For fixed $r>0$, replacing $X$ with $\frac{X}{r}$ implies that the above inequality is equivalent to the following inequality
\begin{align}
2\mathbb{E}\left(X^2\right)\mathbb{E}\left(1-e^{-X}\right)-
2\frac{\mathbb{E}\left(X^2\right)}{\mathbb{E}\left(X\right)}\left[\mathbb{E}\left(Xe^{-X}\right)\right]^2
\geq2\mathbb{E}\left(X^2e^{-X}\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right)+\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2.
\end{align}
Observe that \begin{align}
\left[\mathbb{E}\left(Xe^{-X}\right)\right]^2&=\left[ \mathbb{E}\left(X\right)-\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2
\nonumber\\ & =\left[\mathbb{E}\left(X\right)\right]^2 -2\mathbb{E}\left(X\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right)
+\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2.
\end{align}
So it suffices to show
\begin{align}
&2\mathbb{E}\left(X^2\right)\mathbb{E}\left(1-e^{-X}\right)-
2\mathbb{E}\left(X^2\right)\mathbb{E}\left(X\right)+4\mathbb{E}\left(X^2\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right) -2\frac{\mathbb{E}\left(X^2\right)}{\mathbb{E}\left(X\right)}\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2
\nonumber\\ &\geq 2\mathbb{E}\left(X^2e^{-X}\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right)+\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2.
\end{align}
For the first term of the right hand side of the above inequality, it is clear that
$$2\mathbb{E}\left(X^2e^{-X}\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right)
=2\mathbb{E}\left(X^2\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right)
-2\mathbb{E}\left(X^2(1-e^{-X})\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right).$$
Thus, we only need to prove
\begin{align}\label{eve1}
&\mathbb{E}\left(X^2\right)\left[2\mathbb{E}\left(1-e^{-X}\right)-
2\mathbb{E}\left(X\right)+2\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right] -2\frac{\mathbb{E}\left(X^2\right)}{\mathbb{E}\left(X\right)}\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2
\nonumber\\ &\geq  -2\mathbb{E}\left(X^2(1-e^{-X})\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right)+\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2.
\end{align}
By the CauchySchwarz inequality,
$$\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2
\leq \mathbb{E}\left(X^2\right)\mathbb{E}\left(\left(1-e^{-X}\right)^2\right), $$
Then we have
\begin{align}
&\mathbb{E}\left(X^2\right)\left(2\mathbb{E}\left(1-e^{-X}\right)-
2\mathbb{E}\left(X\right)+2\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right)
-\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2
\nonumber\\ &\geq \mathbb{E}\left(X^2\right)\mathbb{E}\left( 1-2Xe^{-X}-e^{-2X}\right).
\end{align}
It can be shown that $1-2xe^{-x}-e^{-2x}\geq 0$ for $x\geq0$. Therefore, we have
\begin{align}\label{eve2}
\mathbb{E}\left(X^2\right)\left(2\mathbb{E}\left(1-e^{-X}\right)-
2\mathbb{E}\left(X\right)+2\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right)
\geq \left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2.
\end{align}
On the other hand, since $\left(x-\tilde{x}\right)\left(e^{-\tilde{x}}-e^{-x}\right)\ge 0$ for $x,\tilde{x}\ge0$, we have
\begin{align}
2\mathbb{E}\left(X\right)\mathbb{E}\left(X^2(1-e^{-X})\right)-
2\mathbb{E}\left(X^2\right)\mathbb{E}\left(X(1-e^{-X})\right)&=
2\mathbb{E}\left(X^2\right)\mathbb{E}\left(Xe^{-X}\right)
-2\mathbb{E}\left(X\right)\mathbb{E}\left(X^2e^{-X}\right)
\nonumber\\ & =\mathbb{E}\left(X\tilde{X}\left(X-\tilde{X}\right)\left(e^{-\tilde{X}}-e^{-X}\right)\right)\geq 0,
\end{align}
where $\tilde{X}$ is an independent copy of $X$. This inequality implies that
\begin{align}\label{eve3}
-2\frac{\mathbb{E}\left(X^2\right)}{\mathbb{E}\left(X\right)}
\left[\mathbb{E}\left(X\left(1-e^{-X}\right)\right)\right]^2 \geq
-2\mathbb{E}\left(X^2(1-e^{-X})\right)\mathbb{E}\left(X\left(1-e^{-X}\right)\right).
\end{align}
Combining (\ref{eve2}) and (\ref{eve3}), we have verified (\ref{eve1}) and the proof is complete.
\end{proof}
\end{comment}

\bibliographystyle{amsplain}
\bibliography{reference}
\end{document}

