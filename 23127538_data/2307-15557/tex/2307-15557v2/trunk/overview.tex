\section{Technical Overview}\label{sec:overview}

In this section, we give a high-level overview of our algorithms and discuss several technical challenges that we need to handle for dynamically maintaining a $k$-center solution on graphs rather than on point sets.


\paragraph{Reduction to $k$-Bounded Maximal Independent Set on Threshold Graphs.} We start by reviewing a known reduction from $2$-approximate $k$-center to $k$-bounded maximal independent set (MIS)~\cite{HochbaumS86}. This reduction is also the basis of some of the fully dynamic $k$-center algorithms on point sets in arbitrary metric spaces~\cite{BateniEFHJMW23, chan2018fully}. The idea for obtaining a $2$-approximation is to guess the optimal value $R^*$ of the $k$-center instance via a binary search, and return
any maximal distance-$2R^*$ independent set $M$. It can be shown that $M$ must be of size at most $k$. Recall that $M$ is a maximal subset of vertices such that no two vertices $u,v \in M$ are within distance $2R^*$ of each other. The vertices of $M$ then correspond to the centers of the $k$-center instance. 

For utilizing this idea in the dynamic setting, rather than guessing the value of $R^*$, we maintain the MIS on each \textit{$r$-threshold graph} $G_r$, for every \emph{distance range} $r \leq (1+\epsilon)^{i}$, where $i \in [0, \log_{1+\epsilon} (nW)]$ and $W$ is the maximum edge-weight of the graph. 
Here, by $r$-threshold graph we mean a graph such that there is an edge between two vertices if and only if they are within distance $r$.
This general framework has been used in the dynamic $k$-center algorithms on point sets \cite{BateniEFHJMW23, chan2018fully}. Actually, they  maintain a relaxation of the MIS,
called a \emph{$k$-bounded MIS}. The observation is that it is sufficient to either return an MIS of size at most $k$ on each $r$-threshold graph $G_r$, or to simply report that there is an independent set of size at least $k+1$ as a witness that the distance range $r$ is not the correct guess.

\paragraph{Technical Challenges in Graphs vs Point Sets.}
As discussed before there are several important technical differences between the graph and point sets setting. The first difference is that, unlike the point sets setting, in graphs we do not have direct access to distances. Thus, we also need to maintain the appropriate distances simultaneously while the dynamic MIS is modified on the $r$-threshold graphs.
For this, we would like to combine a dynamic MIS maintenance algorithm with partially dynamic (approximate) shortest paths algorithms. At a high-level, our goal is to maintain a $k$-bounded MIS $M$ dynamically on each $r$-threshold graph $G_r$ and at the same time maintain a dynamic (approximate) SSSP algorithm from a super-source which is connected to all the vertices in the dynamic set $M$. Hence the efficiency of the algorithm will depend on the number of times the set $M$ is modified over all the updates. Equivalently, the efficiency will depend on the number of times the dynamic SSSP algorithm is restarted. 


In both point sets and graph setting, we need to bound the \textit{recourse}, where by recourse we mean the number of times a new center is introduced by the algorithm. However, we argue that a \textit{stronger recourse guarantee} is needed for graphs. Recall that another important difference between these two settings is that in the point sets setting adding or removing points has a more local impact, whereas in a graph an update may impact the distances between many vertices. 
In other words, in the graph setting an edge update may \textit{distort} the metric itself. This difference in graphs together with the fact that the efficiency will depend on the number of times the dynamic set $M$ is modified/dynamic SSSP is restarted, requires our algorithm to have an overall recourse guarantee, as opposed to an amortized one that suffices for the point sets setting. 
Thus an amortized recourse of $\tilde{O}(k)$ \emph{per update} is not enough, and we need the stronger guarantee that the recourse is $\tilde{O}(k)$ \emph{over all updates}. 
More concretely, this total bound on recourse will let us argue that in total we need to re-initialize a dynamic $(1+\epsilon)$-SSSP algorithm $\tilde{O}(k)$ times from each center, and an amortized guarantee would not be enough for getting our desired update bound. We will see that maintaining this stronger recourse guarantee is more challenging in the incremental setting than in the decremental setting. 

Note that these types of recourse guarantees have been studied in point sets from arbitrary metric spaces under the name \emph{consistent} clustering~\cite{LattanziV17,FichtenbergerLNS21,LackiHGRJ23}.
However, we would like to emphasize that the known sublinear bounds on the total recourse in the point sets setting do not carry over to the graph setting.

\paragraph{Decremental $k$-Center on Graphs.} 
We can obtain a decremental $(2+\epsilon)$-approximation algorithm for the $k$-center problem on graphs, by maintaining a decremental $(1+\epsilon)$-SSSP algorithm from a
super-source which is connected to all the centers in each of the $O(\log nW)$ $r$-threshold graphs. Bounding the recourse in the decremental setting is relatively straightforward based on the following observation. Whenever a new center forms a cluster due to a distance increase in a given $r$-threshold graph, it stays disjoint of other clusters throughout the algorithm and thus it stays a valid center. Furthermore, as soon as we get more than $k$ centers, we move to the next distance range and so, the recourse is upper bounded by $k$ on each $r$-threshold graph and by $O(k \log nW)$ overall. Hence the $(1+\epsilon)$-SSSP algorithm
is restarted at most $O(k \log nW)$ times in total. This combined with the time needed for maintaining partially dynamic $(1+\epsilon)$-approximate SSSP leads to our desired $kn^{o(1)}$ amortized update time.

\subsection{Incremental $k$-Center on Graphs}
\paragraph{Incremental Low Recourse Ruling Sets.} 
Bounding the recourse in the incremental setting is more challenging compared to the decremental setting, for the following reason. After an edge insertion in the input graph, a center $c_1$ of a cluster
may come within distance $r$ of an existing center $c_2$ of another cluster. 
In turn, this means that the two vertices $c_1$ and $c_2$ become neighbors in the $r$-threshold graph $G_r$.
We cannot simply merge the corresponding clusters in some way and still maintain a $2$-approximation, as some vertices in such a merged cluster would go beyond 
the desired distance range after each update. Hence we need a new technical idea to keep the recourse low. The idea is to 
maintain a small (i.e., of size $\tilde{O}(k)$) \textit{dominating set} $S$ on $G_r$ such that, at a high-level, maintaining a maximal independent set on $G_r[S]$ will give us an \textit{approximate} maximal independent set on $G_r$. More formally, by maintaining a $k$-bounded MIS $M$ on the dominating set $S$ in $G_r$, we can show that $M$ is also a $k$-bounded $(2,2)$-ruling set\footnote{While our $k$-center algorithms work for weighted graphs, the ruling set subroutines always perform on unweighted graphs regardless on the input to the $k$-center problem.} on $G_r$. That is, a subset $M$ of vertices of size at most $k$ such that: (i) the distance in $G_r$ between any pair of vertices in $M$ is at least $2$, and (ii) for each vertex in $V$ there exists a vertex in $M$ within distance $2$ in $G_r$.
Introducing this small dominating set allows us to maintain a dynamic $k$-bounded maximal independent set $M$ on the smaller subgraph $G_r[S]$ more efficiently, at the cost of losing a factor $2$ in the approximation due to the fact that $M$ is only a $(2,2)$-ruling set on $G_r$. 
For maintaining such a dominating set, we use a recursive algorithm that maintains a union of hitting sets on a sequence of sparsified subgraphs of $G_r$. The hitting sets are obtained by a standard sampling procedure on the subgraphs corresponding to each recursive call. Informally, the sampling rate of the hitting sets is tuned depending on the densities of these subgraphs and the recursion continues until the remaining set of low degree vertices is sufficiently small. 
Since we have an incremental graph, the set of \textit{low degree vertices defined based on a specific degree threshold} that are not covered by the hitting sets shrinks over time. This together with observations regarding the sampling, the degrees, and a property of the $k$-center problem allows us to bound the recursion depth by $O(\log n)$.


\paragraph{Challenges of Working with the $r$-Threshold Graphs.} 
The high-level idea described above will give us an algorithm for maintaining a $k$-bounded $(2,2)$-ruling set on an incremental graph in $\tilde{O}(k)$ amortized update time with an overall recourse of $\tilde{O}(k)$ \antonis{modify it}. Similar to the decremental algorithm, our goal is to maintain such a ruling set on all $r$-threshold graphs. This, combined with an incremental $(1+\epsilon)$-SSSP algorithm, will lead to an incremental $(4+\epsilon)$-approximation algorithm for the $k$-center problem on graphs. The main remaining challenge is that an edge insertion into the input graph $G$ could lead to many edge insertions in the $r$-threshold graph. In turn, the density of the $r$-threshold graphs could be $n^2 \geq m$. To overcome this challenge, we do not explicitly store the $r$-threshold graphs.
Instead, we utilize the construction of the dominating set on each $r$-threshold graph $G_r$ together with the fact
that the dominating set for each $G_r$ is of small size, to ensure that only \textit{relevant} edges
are processed. That is, edges that either participate in the construction of the dominating set or those that cause a conflict.
Overall, by bounding the number of candidate centers and taking advantage of the construction of the dominating sets, 
we ensure that $\tilde{O}(k)$ incremental $(1+\epsilon)$-SSSP algorithms are re-initialized in total, 
and the \textit{relevant} information is maintained. In turn, this leads to an amortized update time of $kn^{o(1)}$.





