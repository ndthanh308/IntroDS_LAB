%\newpage
\appendix

\section*{Appendix} % Start the appendix part
%\parttoc % Insert the appendix TOC
The purpose of this appendix is twofold: to present the propositions and their proofs of a lower bound between the uniform and discrete distributions and to provide experimental evidence showing that our proposed generator enjoys the two desirable properties. We illustrate the effect of the penalization parameter $\lambda$ on the trained generator using the \LIPERM1 framework. We observe that our generator indeed does not replicate the observations from the training sample while being close to the true data distribution. The reproducible code of all the experiments can be found in the supplementary material. 

%-----------------------
% ete 2 experimentel dnum enq nerqevinna ete che verevine 
%-----------------------

% The purpose of this appendix is twofold: to present the propositions and their proofs of a lower bound between the uniform and discrete distributions and to provide experimental evidence showing that our proposed generator enjoys the two desirable properties. First, we illustrate the effect of the penalization parameter $\lambda$ on the trained generator using the \LIPERM1 framework. Secondly, we measure the discrepancy between the measure induced by our generator and the empirical distribution of the training sample. We observe that our generator indeed does not replicate the observations from the training sample while being close to the true data distribution. The reproducible code of all the experiments can be found in the supplementary material. 

\section{Lower bound on the distance between the 
uniform and  discrete distributions}

\begin{proposition}\label{prop:lower_bound}
    % For an set  of points $a_1,\ldots,a_n\in [0,1]^d$ 
 Let $A = \{a_1,\ldots, a_n\}$, $A \subset \mathbb{R}^d$. Let us define the function
    \begin{align}
        g_A:[0,1]^d\to\mathbb [0,1],\qquad g_A(x) = 
        \min\big(1,\min_{a\in A}\|x-a\|\big), \label{eq:condgA}
    \end{align}
    and any
    set of weights $w_1,\ldots,w_n\geq 0$ summing to one. Then, assuming that $\mathcal{F}$ contains all the $1$-Lipschitz-continuous functions from $\mathbb{R}^d $ to $\mathbb{R}$, we have 
    \begin{align}
        d_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{a_i}\Big) 
        \geqslant 
        \frac{1}{2 + 2(2{\sf V}_d n)^{1/d}},
    \end{align}        
    where $\mathcal U_d$ is the uniform distribution on $[0,1]^d$
     with ${\sf V}_d = \frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2} +1)}$ being the volume of the unit ball\footnote{$\Gamma$ is Euler's gamma function.} in $\mathbb R^d$.
     %and $C_d = 2{\sf V}_d^{1/d}$
\end{proposition}

\begin{proof} We start the proof by noticing that for any $A \subset \mathbb{R}^d$ the function $g_A$ is $1$-Lipschitz. Therefore, we have $\{g_A: A \subset \mathbb{R}^d\} \subset \text{Lip}_1 \subset \mathcal{F}$. Therefore, 
\begin{align}
    d_\mathcal{F}\Big(\mu, \sum_{i=1}^n w_i\delta_{a_i}\Big) & \geq \sup_{g_A \in \mathcal{F}} \bigg(\int_{[0,1]^d} g_A(x)\,\mu(dx) - \sum_{i=1}^n w_i g_A(a_i)\bigg)\\ &\geq \int_{[0,1]^d} g_A(x)\,\mu(dx) - \sum_{i=1}^n w_i g_A(a_i) = \int_{[0,1]^d} g_A(x)\,\mu(dx).
\end{align}   
    Let us define $A_i = B(a_i,\varepsilon_n)$, for $i=1,\ldots,n$. We denote by $B(x_0, r)$ the ball in $\mathbb{R}^d$ with center $x_0$ and radius $r$, $B(x_0, r) = \{ x \in \mathbb{R}^d : \| x - x_0 \| \le r\}$. 
    %Denote the smallest number of balls in $\mathbb R^d$ of radius $\varepsilon_n$ covering a part of $[0,1]^d$ with $\mu$-measure at least $1/2$ by $N_{\mu, \varepsilon_n}$.
    %Since $n < \mathcal N_{\mu, \varepsilon_n}$, we have $\mu(A_1\cup\ldots\cup A_n)\leq 1/2 $. 
    Using this notation we arrive at
    \begin{align}
        \int_{[0,1]^d} g_A(x)\,\mu(dx) &\geq 
        \int_{(\cup_{i=1}^n A_i)^c} \min\big(1,\min_{y\in A}\|x-y\|\big)\,\mu(dx)\\
        &\ge 
        \int_{(\cup_{i=1}^n A_i)^c} \min\big(1,\varepsilon_n \big)\,\mu(dx).  
        %\geq (1/2)\min(1,\varepsilon_n). 
        \label{eq:minepsn}
    \end{align}

We now state an auxiliary proposition, the proof of which is deferred to the end of the section. 
\begin{proposition}\label{prop:2}
Let $S = [0,1]^d$ with $d \in \mathbb{N}$ and let $\mu$ be a probability
measure on $[0,1]^d$ admitting a density with respect to the Lebesgue measure bounded by some constant $b<\infty$. For any $\eps>0$ and $k\in \mathbb N$, if $B_1,\ldots,B_k \subset S$ are balls of radius $\varepsilon$ such that 
\begin{gather}
    \mu(B_1 \cup \ldots \cup B_k) \geq {1}/{2}
\end{gather}
then,
\begin{align}
    k\geq \frac{1}{2b \cdot {\sf V}_d} \cdot \eps^{-d},\quad \text{ where }\quad {\sf V}_d = {\rm Vol}(\textup{unit ball in }\mathbb{R}^d).
\end{align}
\end{proposition}


Combining the result stated in (\ref{eq:minepsn}) with \Cref{prop:2}, 
and taking $n {\color{red} \geq } (2{\sf V}_d)^{-1}\varepsilon_n^{-d} $
% we check that the uniform distribution $\mathcal U_d$ on $[0,1]^d$  satisfies $ N_{\mathcal U_d, \varepsilon} \geq (1/2){\sf V}_d^{-1}\varepsilon^{-d}$, for every 
% $\varepsilon>0$. Therefore, if $\varepsilon_n$ satisfies 
% $(1/2){\sf V}_d^{-1}\varepsilon_n^{-d} \geq n$ condition, then it also
% satisfies $ N_{\mathcal U_d,\varepsilon_n} >n$. Since
% \begin{align}
%     (1/2){\sf V}_d^{-1}\varepsilon_n^{-d} \geq n\quad\Longleftrightarrow
%     \quad \varepsilon_n \leq (2n {\sf V}_d)^{-1/d}.
% \end{align}
 leads to the lower bound
\begin{align}
        d_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{a_i}\Big) 
        \geq (1/2)\min(1,(2{\sf V}_dn)^{-1/d} ).
\end{align}    
In other terms, if $n\geq (2{\sf V}_d)^{-1}$, then 
\begin{align}
        d_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{a_i}\Big) 
        \geq \frac12 (2{\sf V}_d n)^{-1/d} = \frac{1}{2^{(d+1)/d}} ({\sf V}_d n)^{-1/d}  %= \frac{1}{2 C_d n^{1/d}}, \text{ with } C_d = 2{\sf V}_d^{1/d},
\end{align}    
otherwise
\begin{align}
        d_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{a_i}\Big) 
        \geq \frac12. 
\end{align}
Putting together the inequalities from the last two displays concludes the proof with desired lower bound.
\end{proof}

\begin{proof}[Proof of \Cref{prop:2}]
Let $\nu$ be the Lebesgue measure on $[0,1]^d$. 
We know that 
    $\mu(A) = \int_A \varphi(x)\, \nu(dx)$ with a probability density function $\varphi$ satisfying $0\leq \varphi(x)\leq b$ for all $x\in [0,1]^d$. Therefore,
\begin{align}
    \frac{1}{2} \leq \mu(B_1 \cup \ldots \cup B_k) & \leq \sum\limits_{j=1}^k \mu(B_j)
     = \sum\limits_{j=1}^k \int_{B_j} \varphi(x)\,\nu(dx) 
     \leq \sum\limits_{j=1}^k b \cdot \nu(B_j) \label{eq:2.1}
\end{align}
Moreover, we know that $\nu(B_j) = {\sf V}_d \varepsilon^d$ for all $j = 1, \dots k$. Combining this inequality with (\ref{eq:2.1}), we get
\begin{align}
    \frac{1}{2} \leq \mu(B_1 \cup \ldots \cup B_k) 
    & \leq \sum\limits_{j=1}^k b \cdot {\sf V}_d \cdot \eps^d
     = k b \cdot {\sf V}_d \cdot \eps^d.
\end{align}
This yields $k\geq \frac{1}{2 b \cdot {\sf V}_d} \cdot \eps^{-d}$.
\end{proof}
%\fi


\section{Numerical experiments on Gaussian mixture models}
In this appendix, we illustrate the advantages of our proposed method \LIPERM1 by implementing the additional penalty term on top of classical WGAN with a gradient penalty on the discriminator. To illustrate the behavior of our generator $\hat{g}_n$, consider data generated from the distribution $P^* = 1/3 \mathcal{N}(\mu_1, 2I_2) + 1/3 \mathcal{N}(\mu_2, 2I_2) + 1/3 \mathcal{N}(\mu_3, 2I_2)$, a mixture of 3 isotropic Gaussian distributions. Here $I_2$ denotes the identity matrix of size $2\times2$ and we choose $\mu_1 = [0, 6]^\top$, $\mu_2 = [5, 0]^\top$ and $\mu_3 = [8, 8]^\top$. 
In ~\Cref{fig:original} we show the original Gaussian mixture distribution from which the training data was generated. This heatmap was plotted by generating $n=2^{16}$ points, which for visual purposes resembles the true distribution $P^*$. The corresponding training data, with the empirical distribution $\hat{P}_n$ and $n=256$, is illustrated in \Cref{fig:training}.
 
% Figure environment removed


\subsection{The effect of the penalization parameter $\lambda$}
We visualize the effect of the left-inverse penalization parameter $\lambda$ by plotting the generated distribution on the original data, for different values of $\lambda$. We have considered $\lambda \in \{0, 1, 2, 4, 8, 16 \}$. The scenario when $\lambda = 0$ corresponds to data generated from the model without the left-inverse penalization parameter, effectively reducing it to the classical Wasserstein-GAN with a gradient penalty on the discriminator. Once the generative models with and without the left-inverse penalty term are trained, a total of $2^{16}$ data points are generated. Subsequently, we present the heatmaps of the corresponding 2D histograms in ~\Cref{fig:heatmapsgenerated}, which visually illustrate the distributions generated by the generator $\hat{g}_n$ with left-inverse penalty model for different values of $\lambda \in \{0, 1, 2, 4, 8, 16 \}$. As anticipated, as the value of $\lambda$ increases, the generated data points become more dispersed around the mean values. In other words, when $\lambda=0$ the generator is less smooth and tends to concentrate around the mean value of one of the clusters (see \Cref{fig:lambda0}), while for larger values of $\lambda$, it is becoming smoother and when $\lambda = 4$ (\Cref{fig:lambda4}) the generated distribution resembles more with the original Gaussian mixture. It is also worth noticing that when the parameter $\lambda$ becomes even larger, $\lambda = 8$ or $\lambda=16$, the distribution generated by \LIPERM1 becomes even smoother slightly diverging from the Gaussian mixture model. Indeed, a larger penalization parameter $\lambda$ compels the generator to produce smoother distributions. At the same time, the minimization of the Wasserstein distance between the empirical distributions of the training sample and the generated points has a reduced impact. That being said, $\lambda$ can be viewed as the parameter that controls the smoothness of our generator. 

% % 3 harkani 

% % Figure environment removed

% 2 harkani 

% Figure environment removed

\end{document}
% Figure environment removed
