\begin{thebibliography}{}

\bibitem[\protect\astroncite{Arjovsky et~al.}{2017}]{pmlr-v70-arjovsky17a}
Arjovsky, M., Chintala, S., and Bottou, L. (2017).
\newblock {W}asserstein generative adversarial networks.
\newblock In Precup, D. and Teh, Y.~W., editors, {\em Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of {\em Proceedings
  of Machine Learning Research}, pages 214--223. PMLR.

\bibitem[\protect\astroncite{Bai et~al.}{2018}]{bai_ma_risteski_2019}
Bai, Y., Ma, T., and Risteski, A. (2018).
\newblock Approximability of discriminators implies diversity in gans.
\newblock {\em ArXiv}, abs/1806.10586.

\bibitem[\protect\astroncite{Belomestny et~al.}{2023}]{belomestny2023rates}
Belomestny, D., Moulines, E., Naumov, A., Puchkin, N., and Samsonov, S. (2023).
\newblock Rates of convergence for density estimation with generative
  adversarial networks.

\bibitem[\protect\astroncite{Biau et~al.}{2020}]{Biau}
Biau, G., Cadre, B., Sangnier, M., and Tanielian, U. (2020).
\newblock {Some theoretical properties of GANS}.
\newblock {\em The Annals of Statistics}, 48(3):1539 -- 1566.

\bibitem[\protect\astroncite{Biau et~al.}{2021}]{BiauST21}
Biau, G., Sangnier, M., and Tanielian, U. (2021).
\newblock Some theoretical insights into wasserstein gans.
\newblock {\em J. Mach. Learn. Res.}, 22:119:1--119:45.

\bibitem[\protect\astroncite{Block et~al.}{2020}]{Block}
Block, A., Mroueh, Y., and Rakhlin, A. (2020).
\newblock Generative modeling with denoising auto-encoders and langevin
  sampling.
\newblock {\em CoRR}, abs/2002.00107.

\bibitem[\protect\astroncite{Boissard and Gouic}{2014}]{Boissard}
Boissard, E. and Gouic, T.~L. (2014).
\newblock {On the mean speed of convergence of empirical and occupation
  measures in Wasserstein distance}.
\newblock {\em Annales de l'Institut Henri Poincaré, Probabilités et
  Statistiques}, 50(2):539 -- 563.

\bibitem[\protect\astroncite{Chae et~al.}{2023}]{chae2023likelihood}
Chae, M., Kim, D., Kim, Y., and Lin, L. (2023).
\newblock A likelihood approach to nonparametric estimation of a singular
  distribution using deep generative models.
\newblock {\em Journal of Machine Learning Research}, 24(77):1--42.

\bibitem[\protect\astroncite{Chen et~al.}{2022}]{chen2022minimax}
Chen, S., Li, J., Li, Y., and Meka, R. (2022).
\newblock Minimax optimality (probably) doesn't imply distribution learning for
  gans.
\newblock {\em arXiv preprint arXiv:2201.07206}.

\bibitem[\protect\astroncite{De~Bortoli et~al.}{2022}]{debortoli2022riemannian}
De~Bortoli, V., Mathieu, E., Hutchinson, M., Thornton, J., Teh, Y.~W., and
  Doucet, A. (2022).
\newblock Riemannian score-based generative modelling.

\bibitem[\protect\astroncite{De~Bortoli et~al.}{2021}]{Bortoli1}
De~Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021).
\newblock Diffusion schr\"{o}dinger bridge with applications to score-based
  generative modeling.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W., editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 17695--17709. Curran Associates, Inc.

\bibitem[\protect\astroncite{Dudley}{1969}]{dudley_1969}
Dudley, R.~M. (1969).
\newblock {The Speed of Mean Glivenko-Cantelli Convergence}.
\newblock {\em The Annals of Mathematical Statistics}, 40(1):40 -- 50.

\bibitem[\protect\astroncite{Fekri et~al.}{2019}]{fekri2019generating}
Fekri, M.~N., Ghosh, A.~M., and Grolinger, K. (2019).
\newblock Generating energy data for machine learning with recurrent generative
  adversarial networks.
\newblock {\em Energies}, 13(1):130.

\bibitem[\protect\astroncite{Gagne et~al.}{2020}]{gagne2020machine}
Gagne, D.~J., Christensen, H.~M., Subramanian, A.~C., and Monahan, A.~H.
  (2020).
\newblock Machine learning for stochastic parameterization: Generative
  adversarial networks in the lorenz'96 model.
\newblock {\em Journal of Advances in Modeling Earth Systems},
  12(3):e2019MS001896.

\bibitem[\protect\astroncite{Goodfellow
  et~al.}{2014}]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial networks.
\newblock {\em Advances in Neural Information Processing Systems},
  27:2672--2680.

\bibitem[\protect\astroncite{Huang et~al.}{2022}]{huang_etal_2022}
Huang, J., Jiao, Y., Li, Z., Liu, S., Wang, Y., and Yang, Y. (2022).
\newblock An error analysis of generative adversarial networks for learning
  distributions.
\newblock {\em Journal of Machine Learning Research}, 23:1--43.

\bibitem[\protect\astroncite{Kingma and Welling}{2013}]{kingma_welling_2013}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[\protect\astroncite{Kwon and Chae}{2023}]{kwon2023minimax}
Kwon, H.~K. and Chae, M. (2023).
\newblock Minimax optimal density estimation using a shallow generative model
  with a one-dimensional latent variable.
\newblock {\em arXiv preprint arXiv:2305.06755}.

\bibitem[\protect\astroncite{Li and Farnia}{2023}]{pmlr-v206-ting-li23a}
Li, C.~T. and Farnia, F. (2023).
\newblock Mode-seeking divergences: Theory and applications to gans.
\newblock In Ruiz, F., Dy, J., and van~de Meent, J.-W., editors, {\em
  Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics}, volume 206 of {\em Proceedings of Machine Learning
  Research}, pages 8321--8350. PMLR.

\bibitem[\protect\astroncite{Liang}{2021}]{Liang}
Liang, T. (2021).
\newblock How well generative adversarial networks learn distributions.
\newblock {\em J. Mach. Learn. Res.}, 22(1).

\bibitem[\protect\astroncite{Maziarka et~al.}{2020}]{maziarka2020mol}
Maziarka, {\L}., Pocha, A., Kaczmarczyk, J., Rataj, K., Danel, T., and
  Warcho{\l}, M. (2020).
\newblock Mol-cyclegan: a generative model for molecular optimization.
\newblock {\em Journal of Cheminformatics}, 12(1):1--18.

\bibitem[\protect\astroncite{Nie et~al.}{2017}]{nie2017medical}
Nie, D., Trullo, R., Lian, J., Petitjean, C., Ruan, S., Wang, Q., and Shen, D.
  (2017).
\newblock Medical image synthesis with context-aware generative adversarial
  networks.
\newblock In {\em Medical Image Computing and Computer Assisted Intervention-
  MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
  September 11-13, 2017, Proceedings, Part III 20}, pages 417--425. Springer.

\bibitem[\protect\astroncite{Niles-Weed and
  Rigollet}{2022}]{weed2022estimation}
Niles-Weed, J. and Rigollet, P. (2022).
\newblock {Estimation of Wasserstein distances in the Spiked Transport Model}.
\newblock {\em Bernoulli}, 28(4):2663 -- 2688.

\bibitem[\protect\astroncite{Oko et~al.}{2023}]{oko2023diffusion}
Oko, K., Akiyama, S., and Suzuki, T. (2023).
\newblock Diffusion models are minimax optimal distribution estimators.
\newblock {\em arXiv preprint arXiv:2303.01861}.

\bibitem[\protect\astroncite{Paganini et~al.}{2018}]{paganini2018calogan}
Paganini, M., de~Oliveira, L., and Nachman, B. (2018).
\newblock Calogan: Simulating 3d high energy particle showers in multilayer
  electromagnetic calorimeters with generative adversarial networks.
\newblock {\em Physical Review D}, 97(1):014021.

\bibitem[\protect\astroncite{Repecka et~al.}{2021}]{repecka2021expanding}
Repecka, D., Jauniskis, V., Karpus, L., Rembeza, E., Rokaitis, I., Zrimec, J.,
  Poviloniene, S., Laurynenas, A., Viknander, S., Abuajwa, W., et~al. (2021).
\newblock Expanding functional protein sequence spaces using generative
  adversarial networks.
\newblock {\em Nature Machine Intelligence}, 3(4):324--333.

\bibitem[\protect\astroncite{Roth et~al.}{2017}]{roth_gan_regulariz}
Roth, K., Lucchi, A., Nowozin, S., and Hofmann, T. (2017).
\newblock Stabilizing training of generative adversarial networks through
  regularization.

\bibitem[\protect\astroncite{Schreuder
  et~al.}{2021}]{schreuder_brunel_dalalyan_2021}
Schreuder, N., Brunel, V.-E., and Dalalyan, A. (2021).
\newblock Statistical guarantees for generative models without domination.
\newblock In Feldman, V., Ligett, K., and Sabato, S., editors, {\em Proceedings
  of the 32nd International Conference on Algorithmic Learning Theory}, volume
  132 of {\em Proceedings of Machine Learning Research}, pages 1051--1071.
  PMLR.

\bibitem[\protect\astroncite{Wiese et~al.}{2020}]{wiese2020quant}
Wiese, M., Knobloch, R., Korn, R., and Kretschmer, P. (2020).
\newblock Quant gans: deep generation of financial time series.
\newblock {\em Quantitative Finance}, 20(9):1419--1440.

\bibitem[\protect\astroncite{Xi and Bloem-Reddy}{2023}]{xi_reddy_2023}
Xi, Q. and Bloem-Reddy, B. (2023).
\newblock Indeterminacy in generative models: Characterization and strong
  identifiability.
\newblock {\em arXiv preprint arXiv:2206.00801}.

\bibitem[\protect\astroncite{Yan et~al.}{2018}]{yan2018deeplesion}
Yan, K., Wang, X., Lu, L., and Summers, R.~M. (2018).
\newblock Deeplesion: automated mining of large-scale lesion annotations and
  universal lesion detection with deep learning.
\newblock {\em Journal of medical imaging}, 5(3):036501--036501.

\bibitem[\protect\astroncite{Zhu et~al.}{2017}]{zhu_park_isola_efros_2017}
Zhu, J.-Y., Park, T., Isola, P., and Efros, A.~A. (2017).
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2868--2876.

\end{thebibliography}
