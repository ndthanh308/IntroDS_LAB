\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{mathrsfs}
% \usepackage{bbold}
%\usepackage{bm}
% \usepackage[font=footnotesize]{caption}
% \usepackage{enumitem}
%\usepackage{framed}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{amsmath, amsfonts, amssymb, amsthm}



\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting


\definecolor{darkmidnightblue}{rgb}{0.0, 0.2, 0.4}
\definecolor{darkpowderblue}{rgb}{0.0, 0.2, 0.6}
\definecolor{dukeblue}{rgb}{0.0, 0.0, 0.61}

%\RequirePackage
%[colorlinks,citecolor=blue,urlcolor=blue,breaklinks=true]
%{hyperref}
\hypersetup{
    colorlinks = true,
    citecolor=blue,
    urlcolor=blue,
    breaklinks=true,
    linkcolor = dukeblue,
    linkbordercolor = {white},
}

\usepackage{cleveref}
\usepackage{autonum}

\usepackage{natbib}%[round,sort]
\bibliographystyle{apa}


%\usepackage{float}


\def\LIPERM1{\hyperlink{LIPERM}{\textup{\sf{LIPERM}}}}
\newcommand{\AssA}[2]{\hyperlink{AssA}{\textup{\textsf{A}}}$({#1},{#2})$}

\def\hat{\widehat}
\def\rmd{{\rm d}}

\newcommand{\Var}{\operatorname{Var}}
\newcommand{\ip}[2]{\ensuremath{\left\langle {#1}, {#2} \right\rangle}}
\newcommand{\inlineip}[2]{\ensuremath{\langle {#1}, {#2} \rangle}}

\def\Id{\operatorname{\sf Id}}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\argmin}{\mathrm{arg}\,\mathop{\mathrm{min}}}
\newcommand{\argmax}{\mathrm{arg}\,\mathop{\mathrm{max}}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\ol}{\overline}
\renewcommand{\ln}{\log}
\newcommand{\di}{\mathrm{d}}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\var}{\Var}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\cond}{|}
\newcommand{\ind}[1]{\bm 1 \left( #1 \right)}
\newcommand{\gaussdist}{\mathcal{N}}
\newcommand{\poissondist}{\mathsf{Poi}}
\newcommand{\binomdist}{\mathsf{Bin}}
\newcommand{\lamin}{\lambda_{\mathrm{min}}}
\newcommand{\lamax}{\lambda_{\mathrm{max}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\excessrisk}{\mathcal{E}}


\def\bU{\boldsymbol{U}}
\def\bu{\boldsymbol{u}}
\def\bx{\boldsymbol{x}}
\def\bX{\boldsymbol{X}}
\def\bZ{\boldsymbol{Z}}
\def\wass{{\sf W}}

\newcommand{\Flin}{\mathcal{F}_{\mathrm{lin}}}
\newcommand{\Zs}{\mathcal{Z}}
\newcommand{\erm}{\mathrm{erm}}
\newcommand{\ferm}{\wh g_{\mathrm{erm}}}
\newcommand{\werm}{\wh w_{\mathrm{erm}}}
\renewcommand{\top}{\mathsf{T}}

%\usepackage[textwidth=2.0cm, textsize=small]{todonotes} % for writting
    \newcommand{\arn}[2][noinline]{\todo[color=yellow!20,#1]{{\bf arnak:} #2}}
    \newcommand{\tig}[2][noinline]{\todo[color=yellow!20,#1]{{\bf tig:} #2}}
    \newcommand{\son}[2][noinline]{\todo[color=yellow!20,#1]{{\bf sona:} #2}}
    \newcommand{\eln}[2][noinline]{\todo[color=yellow!20,#1]{{\bf elen:} #2}}
    \newcommand{\ars}[2][noinline]{\todo[color=yellow!20,#1]{{\bf arshak:} #2}}



%\usepackage{xspace}

\newcommand{\ie}{\textit{i.e.}\@\xspace}
\newcommand{\eg}{e.g.\@\xspace}
\newcommand{\iid}{i.i.d.\@\xspace}
\newcommand{\as}{almost surely\@\xspace}
\newcommand{\resp}{respectively{ }}

\newcommand{\opnorm}[1]{\| {#1} \|_{\mathrm{op}}}
\newcommand{\Opnorm}[1]{\Big\| {#1} \Big\|_{\mathrm{op}}}
\newcommand{\kl}{\mathrm{KL}}
\newcommand{\kll}[2]{\kl ({#1}, {#2})}
\newcommand{\kls}[2]{\mathrm{kl} ( #1 , #2 )} 
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}{Fact}
\newtheorem{informaltheorem}{Theorem}
\renewcommand*{\theinformaltheorem}{\Alph{informaltheorem}}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{dfprop}[definition]{Definition-proposition}
\newtheorem{strategy}{Strategy}
\newtheorem{example}{Example}
\newtheorem{examples}[example]{Examples}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{remarks}{Remarks}
\newtheorem{consequence}{Consequence}
\newtheorem*{consequence*}{Consequence}




\title{Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Elen Vardanyan \\
  %Department of Computer Science\\
  Yerevan State University\\
  Yerevan, Armenia\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  \And
  Arshak Minasyan \\
  CREST, ENSAE, IP Paris \\
  Palaiseau, France\\
  \And 
  Sona Hunanyan\\
  Yerevan State University\\
  Yerevan, Armenia\\
  \AND
  Tigran Galstyan \\
  Russian-Armenian University\\
  Yerevan, Armenia\\
  \And
  Arnak S.\ Dalalyan \\
  CREST, ENSAE, IP Paris \\
  Palaiseau, France\\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle

\begin{abstract}
    Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.

This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.

We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to general integral probability metrics used to quantify errors in probability distribution spaces, with the Wasserstein-$1$ distance being the central example. %We also include numerical examples to illustrate our theoretical findings.
\end{abstract}


% \begin{abstract}
%   Generative adversarial networks (GANs) represent a two-step procedure designed to learn the underlying data distribution. In this paper, we propose a new framework for generative modeling based on constrained empirical risk minimization (ERM). We first show that the proposed generator indeed induces a transformation of the $d$ dimensional hypercube that is close to the true data distribution, where $d$ is the ambient dimension, typically much smaller than the input dimension. The risk measured by, for example, Wasserstein-$p$ ($p\ge 1$) distance is proved to be of order $n^{-1/d}$. Due to the lower bound of the same order our generator achieves the optimal minimax rate. Moreover, we prove a lower bound on the distance from the empirical distribution of the initial sample. Namely, we show that for any generator that has a Lipschitz-continuous right inverse, the Wasserstein-$p$ distance from the empirical distribution is at least of order $n^{-1/d}$. This is the first result of this type, which explains why properly trained GANs tend not to repeat observations from the training sample. We also perform numerical analysis on two benchmark datasets: Gaussian mixtures and CIFAR-10. 
  
% \end{abstract}

\section{Introduction}

Generative modeling is a widely-used machine learning technique that has found applications in various scientific and industrial domains, including health \citep{yan2018deeplesion,nie2017medical}, climate \citep{gagne2020machine}, finance \citep{wiese2020quant}, energy \citep{fekri2019generating}, physics \citep{paganini2018calogan}, chemistry \citep{maziarka2020mol}, and biology \citep{repecka2021expanding}. The primary goal of generative models is to simulate new examples by learning from a given set of training data, while ensuring diversity and avoiding the replication of examples from the training set.

Assessing the performance of a generative model can be done qualitatively by evaluating the realism of the generated examples, which we refer to as accuracy. However, accuracy should be balanced with another crucial property: the diversity of generated examples. It is essential to avoid overfitting, where the algorithm produces examples that are slight modifications of those in the training sample. Evaluating diversity qualitatively becomes challenging due to the large size of training databases, making it impossible to retain all the examples they contain. Nonetheless, diversity is as important as accuracy, particularly in applications where generative models aim to enrich datasets in cases where data acquisition is expensive or infeasible. Generating examples that closely resemble the observed data diminishes the utility of such algorithms.

The success of deep neural nets in generative modeling has attracted significant attention from the machine learning community. The number of proposed methods in recent years, following the influential work by \cite{goodfellow2014generative}, is extensive, making it impractical to cite all of them here\footnote{For a comprehensive list, refer to \url{https://github.com/hindupuravinash/the-gan-zoo}}. While many of these methods have been empirically validated and justified using heuristics, a more comprehensive mathematical quantification of their strengths and limitations is often lacking.

In this paper, we investigate theoretical aspects of training generative models, with a specific focus on two fundamental properties: (i) the convergence of the error between the true data-generating distribution and the trained data-generating distribution, and (ii) the dissimilarity between the trained distribution and any distribution that replicates examples from the training data. To quantify these properties, we propose a new variant of Wasserstein GANs \citep{pmlr-v70-arjovsky17a} called LIPERM, which stands for left-inverse penalized empirical risk minimizer. We provide non-asymptotic results in the form of finite sample risk bounds, offering valuable insights into the behavior of generative models, including the dependence on the sample size $n$, the dimensionality $d$ of the latent space, and the independence of the dimensionality of the ambient space. We consider integral probability metrics to measure errors in probability distribution spaces, with the well-known Wasserstein-1 distance serving as a central example.

Our analysis demonstrates that as the sample size approaches infinity, the error of replacing the true data-generating distribution with the trained distribution converges optimally to zero. The lower bounds for this setting have been established in \citep{schreuder_brunel_dalalyan_2021}. Additionally, we show that the trained distribution is sufficiently distant from any distribution that merely replicates examples from the training data, ensuring the production of diverse and novel samples. Specifically, when the dimension of the latent space $d$ is larger than 2, we prove that the error of LIPERM in Wasserstein-1 distance is at most of the order $n^{-1/d}$, while its distance from any distribution concentrated on the training sample is at least of the same order. It is worth noting that $n^{-1/d}$ represents the error of approximating the true data-generating distribution by its empirical counterpart \citep{dudley_1969, Boissard,weed2022estimation}. Therefore, any generative model that lies at a significantly larger distance from the empirical distribution would have sub-optimal accuracy.

In \Cref{fig:1}, we provide an illustration of the framework explored in this paper by considering the task of generating points on a 2D spiral using a 1D latent space. In the leftmost panel, we have the map $g: [0,1] \to \mathbb{R}^2$ that corresponds to a generator with high diversity and a small left-inverse penalty (LIP). This means that the generated examples exhibit a wide range of variations and deviations from the training distribution. Moving to the second panel, the map $g$ is inferior to the first one in terms of diversity, resulting in less varied generated examples, and it has a higher LIP. A closer examination reveals that for certain points on the spiral that are in close proximity to each other, their preimages under the mapping $g$ are located far apart. Finally, in the rightmost panel, the map $g$ generates only a few examples, effectively producing a restricted set of outputs, and it has a LIP equal to $+\infty$, indicating that it lacks the ability to accurately reconstruct the latent space. This illustration highlights the trade-off between diversity and the penalty imposed on the left-invertibility constraint.


% Figure environment removed

\paragraph{Prior work}
In recent years, there has been a notable increase in the number of papers dedicated to the mathematical analysis of generative models. Some of them have considered generative models as methods of distribution estimation and have proven their optimality in this regard \citep{Liang,belomestny2023rates,BiauST21,Biau, oko2023diffusion}. On a related note, shallow networks and adversarial training have been used in \citep{kwon2023minimax} to achieve minimax optimality in density estimation. Another application of generative models to nonparametric estimation is discussed in \citep{chae2023likelihood}. The convergence rate of adversarial generative models, depending solely on the dimension of the latent space, has been highlighted in \citep{huang_etal_2022,schreuder_brunel_dalalyan_2021} under a collection of integral probability metrics.

The analysis of diffusion-based generative models is presented in \citep{Bortoli1,debortoli2022riemannian}, while the investigation of autoencoders and their relation to the Langevin process is carried out in \cite{Block}. The intriguing interplay between minimax optimality and distribution learning is explored in \citep{chen2022minimax}.

A regularization scheme for training deep generative models based on GANs is introduced in \citep{roth_gan_regulariz}. It addresses the severe failure modes of GANs caused by dimensional misspecifications or non-overlapping support between the data and model distributions by adding a penalty on the weighted gradient-norm of the discriminator. The theoretical characterization of the mode-seeking behavior of general $f$-divergences and Wasserstein distances, along with a performance guarantee for mixture models, is provided in \citep{pmlr-v206-ting-li23a}. In the context of generator invertibility and mode collapse in GANs, \citep{bai_ma_risteski_2019} propose an approach involving invertible generators, demonstrating their effectiveness in making generative models more robust to overfitting. In a recent work, \cite{xi_reddy_2023} propose a theoretical framework for analyzing the indeterminacies of latent variable models, showing that the generative model is strongly identifiable if and only if the indeterminacy set of the generative model is the set of all Borel automorphisms that are equal almost everywhere to the identity mapping. 

\paragraph{Organization} 
The remainder of this paper is organized as follows. \Cref{sec:2} introduces the framework and formalizes the problem. \Cref{sec:3} presents our main theoretical results, providing finite sample risk bounds that capture the convergence properties and dissimilarity measures. In \Cref{sec:4}, we discuss the implications and applications of our findings. Finally, \Cref{sec:5} concludes the paper and highlights directions for future research.


\paragraph{Notation}
For every integer $d>1$, we denote by $\mathcal{U}_d$
the uniform distribution on $[0,1]^d$. The norm $\|\bx\|$ of an element $\bx$ from an Euclidean space
is always the Euclidean norm. For a random vector $\bX$ and a real number $q\geq 1$,  we use the notation $\|\bX\|_{\mathbb L_q} = \mathbb E^{1/q}[\|\bX\|^q]$. 
For two subsets $A$ and $B$ of some Euclidean spaces,
and a positive number $L$, we say that a function 
$f:A\to B$ is $L$-Lipschitz-continuous, if 
$\|f(\bx) - f(\bx')\|\leq L\|\bx - \bx'\|$ for every
$\bx,\bx'\in A$. The set of all the $L$-Lipschitz
continuous functions from $A$ to $B$ is denoted by
$\textup{Lip}_L(A\to B)$. We denote by $\mathbb E[\bX]$the expectation of a random variable. If necessary, we write $\mathbb E_P[\bX]$ to stress
that the expectation is considered under the condition
that $\bX$ is drawn from $P$. The Dirac mass at a point $\bx$ is denoted by $\delta_{\bx}$. The notation
$P_{n,Z}$ is often used to design the empirical
distribution $(1/n)\sum_{i=1}^n\delta_{\bZ_i}$ 
($Z$ can be replaced by other letters). 
We set ${\sf V}_d = \pi^{d/2}/\Gamma(1+d/2)$ 
to be the volume of the unit ball. Notation 
$\Id_d$ stands for the identity mapping $\Id_d(\bx) 
=\bx$ on $\mathbb R^d$, or any subset of it. 







% \section{Problem statement}
% In this section, we formalize the problem described in the previous section. This paper aims to gain a theoretical understanding of two crucial yet different questions. 
% \begin{enumerate}
%     \item How close is the estimated distribution we sample from to the underlying distribution $P$? 
%     \item Can we guarantee that sampling from $\widehat{g}$ will not result in repeating one of the observations, \ie will not generate from the empirical distribution $\widehat{P}_n$?
% \end{enumerate}
% We answer the first question by proving an upper bound on the risk $R(\widehat{g}) \triangleq {\sf d}_{\mathcal{F}}(\hat{g}\sharp\,\mathcal{U}_d, P^*)$, which scales as $n^{-1/d}$. This is known to be the optimal rate in such frameworks. Moreover, it turns out that the answer to the second question is also affirmative. We guarantee that the distance between $\widehat{g}\sharp\, \mathcal{U}_d$ and $\widehat{P}_n$ is at least of order $n^{-1/d}$, ensuring that we are not sampling from $\widehat{P}_n$. Formally, there exists a constant $c$ such that 
% \[
%     {\sf d}_{\mathcal{F}}(\widehat{g}\sharp\, \mathcal{U}_d, \widehat{P}_n) \ge c n^{-1/d}.
% \]

% This result holds with most generality, namely for any metric integral probability metric (IPM) ${\sf d}_{\mathcal{F}}(\cdot, \cdot)$. In the sequel, we also discuss particular choices of class $\mathcal{F}$ that yield well-known and broadly studied metrics, such as Wasserstein distance, total variation distance, etc [add references]. 

% We now proceed with the formal problem statement. Let $X_1, \dots, X_n \in \mathbb{R}^D$ be $D$-dimensional vectors drawn independently from an unknown distribution $P^*$ supported on a $d$-dimensional manifold (typically, $d \ll D$). The goal is to be able to generate new points from some distribution $\widetilde{P}$ that is close to $P^*$ without repeating the points from the initial sample.  Formally, we are interested in designing a smooth function $g : [0,1]^d \to [0,1]^D$ such that $g\sharp\,\mathcal{U}_d$ is close to $P^*$, where $\mathcal{U}_d$ is the uniform distribution over $[0,1]^d$. The problem of finding such a function is known as generative modeling. The generator function is typically learned using a neural network and trained to minimize divergence between the target distribution and the generated distribution. 


%\newpage
\section{Training a generative model by 
left-inverse-penalized empirical risk 
minimization}\label{sec:2}

Let $P^*$ represent the distribution of the training data $\bX_1,\ldots,\bX_n$ in the $D$-dimensional Euclidean space $\mathbb R^D$ equipped with the Borel $\sigma$-algebra $\mathscr B(\mathbb R^D)$. Typically, the ambient dimension, $D$, is quite large, but it is reasonable to assume that the examples $\bX_i$ may have originated from latent variables residing in a lower-dimensional space. To address this, we select an integer $d>0$ significantly smaller than $D$ and aim to learn a generative distribution that is a smooth transformation of the uniform distribution in $[0,1]^d$. We express this notion using the following notation:
\begin{align}
\label{AssA}\tag*{{\textsf{A}\text{$(L^*,\sigma^*)$}}}
\exists\,g^*\in \text{Lip}_{L^*}([0,1]^d\to[0,1]^D)
\qquad \text{such that}\qquad \wass_1(g^*{\sharp}\,\mathcal U_d,P^*)\leqslant \sigma^*.
\end{align}
This assumption allows us to cover the mis-specified scenario. Specifically, we refer to the well-specified setting when the true distribution $P^*$ is the push-forward of $\mathcal U_d$ by an $L^*$-Lipschitz-continuous function, corresponding to \textsf{A}$(L^*,0)$. Thus, the parameter 
$\sigma^*$ quantifies the level of mis-specification or departure from a well-specified setting.

In this paper, our proposed strategy for learning a generator involves minimizing the penalized empirical risk using a suitable penalty. To be specific, let $\mathcal{H}\subset \text{Lip}_{L_{\mathcal H}}([0,1]^D\to[0,1]^d)$ and $\mathcal{G}\subset \text{Lip}_{L_{\mathcal G}}([0,1]^d\to[0,1]^D)$. For $\lambda>0$, we define the 
learned generator $\hat P_{n} = \hat P_n(\mathcal{G},\mathcal{H},\lambda)$ as the push-forward distribution $\hat{g}_n\sharp\mathcal U_d$, with $\hat g_n = \hat g_n(\lambda,\mathcal{G},\mathcal{H})$ being a solution to the \hypertarget{LIPERM}{minimization problem}
\begin{align}\label{LIPERM}
\tag{LIPERM}
    \widehat{g}_{n} \in \argmin_{g \in \mathcal{G}} 
     \{ {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, \widehat{P}_n) + \lambda\, \text{pen}_{\mathcal H} (g)\},
\end{align}
where the penalty $\text{pen}_{\mathcal H}(g)$ is defined by  
\begin{align}\label{LIP}\tag{LIP}
    \text{pen}_{\mathcal H}(g) = \min_{h\in \mathcal{H}}
    \int_{[0,1]^d} \| h \circ g(\bu) - \bu\|^q \rmd\bu
    = \min_{h\in \mathcal{H}} \|h\circ g - 
    \Id_d\|_{\mathbb L_q}^q, 
\end{align}
with $q\geqslant 1$. The choice of $\lambda$ is important and will be thoroughly discussed later. Notably, the sets $\mathcal{G}$ and $\mathcal{H}$ are entirely determined by the user's choice. We refer to $\hat g_n$ defined by (\ref{LIPERM}) as the {left-inverse-penalized empirical risk minimizer} and the penalty (\ref{LIP}) as the left-inverse penalty.

The rationale behind considering this learning procedure is as follows: When training a generative model, the objective is to generate a distribution that is both easy to sample from and exhibits two desirable properties. First, it should be close to the distribution of the training data, and second, it should avoid replicating the examples in the training dataset. The cost function in (\ref{LIPERM}) consists of two terms, each contributing to one of these desired properties. If the term ${\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, \widehat{P}_n)$ is small, the generator $g\sharp\mathcal U_d$ closely approximates the empirical distribution. Additionally, if $\text{pen}_{\mathcal H}(g)$ is small, the mapping $g$ becomes nearly invertible with a smooth inverse. Intuitively, when the mapping $g$ from the latent space to the ambient space possesses a smooth inverse, it disperses the unit hypercube $[0,1]^d$ across a large region in $[0,1]^D$ rather than concentrating around the neighborhoods of the training data examples.

The subsequent sections of this paper aim to provide mathematical characterizations of the success achieved by (\ref{LIPERM}) in fulfilling these desired goals. Furthermore, we establish that this approach optimally accomplishes these objectives in a suitably defined minimax sense.

The left-inverse penalty employed in our work bears resemblance to two prominent methods widely studied in the deep learning literature: variational autoencoders and cycle-GAN. Variational autoencoders \citep{kingma_welling_2013} are specifically designed for nonlinear dimension reduction. They consist of encoder and decoder networks, which are trained jointly to learn a compact latent representation of the input data while enabling the reconstruction of the original data from the latent space. By incorporating a penalty term that encourages the latent variables to follow a specific distribution, variational autoencoders can generate new samples by sampling from the learned latent space. In our notation, autoencoders use the penalty $\|g\circ h - \Id_D\|_{\mathbb L_2}$ instead of \Cref{LIP}. Cycle-GAN \citep{zhu_park_isola_efros_2017}, on the other hand, focuses on style transfer and image-to-image translation tasks. It employs two sets of generator and discriminator networks, with the objective of learning mappings between two different domains. By introducing cyclic consistency loss, which enforces that the translated image, when converted back to the original domain, should closely resemble the original image, cycle-GAN can generate realistic examples with style transfer. 

The similarity between our left-inverse penalty and these well-established methods can be beneficial in terms of implementing \LIPERM1 efficiently. Drawing inspiration from the existing techniques and incorporating their insights can potentially contribute to the development of efficient algorithms for our framework. While the implementation details are beyond the scope of the current paper, we leave them as a direction for future work.


\section{Assessing the accuracy of the
left-inverse-penalized ERM}\label{sec:3}

In this section, we present the first main result, which focuses on measuring the accuracy of a generator using the integral probability metric (IPM) based on a set of test functions $\mathcal{F}$. The IPM ${\sf d}_{\mathcal{F}}$ between two probabilities $P$ and $Q$ on $([0,1]^D,\mathscr{B}([0,1]^D))$ is defined as follows
\begin{align}\label{IPM}\tag{IPM}
    {\sf d}_{\mathcal F}(P,Q) = 
    \sup_{f\in\mathcal F} \big| 
    \mathbb E_P[f(\bX)] - \mathbb E_Q[f(\bX)]\big|. 
\end{align}
We mention two well-known IPMs: the total variation distance, which corresponds to $\mathcal{F}$ being the set of all functions bounded by one, and the Wasserstein-$1$ distance, which corresponds to $\mathcal{F}$ being the set of all $1$-Lipschitz-continuous functions.

\begin{theorem}\label{thm:upper}
    Assuming that all functions in $\mathcal{F}$ are $1$-Lipschitz-continuous and the dimension $d$ of the latent space is larger than 2, if $\mathbf{X}_1, \ldots, \mathbf{X}_n \in [0, 1]^D$ are i.i.d. samples drawn from $P^*$, satisfying assumption \AssA{L^*}{\sigma^*} for some $L^*, \sigma^* > 0$, then \LIPERM1 $\hat{g}_n$ satisfies
    \begin{align}\label{eq:upper}\tag{UB}
        \mathbb E[{\sf d}_ {\mathcal F}(\hat g_n\sharp\,\mathcal U_d,P^*)] \leqslant 
        \inf_{g \in \mathcal{G}}  \big\{ {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) + \lambda\, \textup{pen}_{\mathcal H}
    (g) \big\}  + 4\sigma^* + \frac{cL^*\sqrt{d}}{n^{1/d}},
    \end{align}
    where $c>0$ is a universal constant.
\end{theorem}

\begin{remark}
If we assume that the oracle generator $g^*$ appearing in assumption \AssA{L^*}{\sigma^*} has a left-inverse that is $L_{\mathcal{H}}$-Lipschitz continuous, the penalty term in the risk bound provided by \Cref{thm:upper} vanishes. Consequently, the upper bound becomes $5\sigma^* + {cL^*\sqrt{d}\,}{n^{-1/d}}$. As proven in \citep{schreuder_brunel_dalalyan_2021}, this upper bound is minimax-rate-optimal. Notably, the ambient dimension does not appear in the upper bound. 
\end{remark}

\begin{remark}
    One may inquire whether the assumption $d>2$ is necessary for the result to hold true, and what happens for $d\in\{1,2\}$. However, it is worth noting that this assumption is not essential for obtaining a similar upper bound as (\ref{eq:upper}). In the cases of $d=1$ and $d=2$, the expression in question undergoes some modifications. Specifically, for $d=1$, the denominator becomes $n^{1/2}$, while for $d=2$, an additional $\log n$ factor appears in the numerator. 
    These results can be readily derived by combining our proof---presented below---with the corresponding approximation bounds for the uniform distribution in $\wass_1$ distance, accommodating the cases of $d=1$ and $d=2$.
\end{remark}

The proof of \Cref{thm:upper}, provided below, relies on repeated use of the triangle inequality, the minimization property of \LIPERM1, as well as on exploiting Lipschitz continuity assumptions, and using the dual formulation of the Wasserstein-$1$ distance. The final bound is derived by combining these inequalities with the approximation bound 
in the Wasserstein-$1$ distance of the uniform distribution
by its empirical counterpart.

This result demonstrates that under mild assumptions, the generator trained by \LIPERM1
achieves the optimal rate for any choice of $\lambda$. Therefore, there is flexibility in selecting $\lambda$ to enforce dissimilarity with the training examples without compromising accuracy.

\begin{proof}[Proof of \Cref{thm:upper}]
Let $g$ be an arbitrary element from $\mathcal{G}$. The proof begins by using the triangle inequality multiple times, resulting in the following sequence of inequalities:
\begin{align}
   {\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\, \mathcal{U}_d, P^*) & \leq  {\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\, \mathcal{U}_d, \widehat{P}_n) + {\sf d}_{\mathcal{F}}(\widehat{P}_n, P^*) \nonumber \\
   & = {\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\, \mathcal{U}_d, \widehat{P}_n) + \lambda\, \text{pen}_{\mathcal{H}}(\hat{g}_n) - \lambda\, \text{pen}_{\mathcal{H}}(\hat{g}_n) + {\sf d}_{\mathcal{F}} (\widehat{P}_n, P^*) \nonumber \\
   & \leq {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, \widehat{P}_n) + \lambda\, \text{pen}_{\mathcal H}( g) - \lambda\, \text{pen}_{\mathcal{H}}(\hat{g}_n) + {\sf d}_{\mathcal{F}} (\widehat{P}_n, P^*) \nonumber \\
   & \leq {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) + \lambda\, \text{pen}_{\mathcal H}( g) - \lambda\, \text{pen}_{\mathcal{H}}(\hat{g}_n) + 2{\sf d}_{\mathcal{F}} (\widehat{P}_n, P^*).
\end{align}
In the second inequality, we use the fact that $\hat{g}_{n}$ is the minimizer of (\ref{LIPERM}), and in the last line, the triangle inequality is applied. As the left-inverse penalty is always nonnegative, the last display implies:
\begin{align}\label{eq:oracle-ineq}
    {\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\, \mathcal{U}_d, P^*) &\le \inf_{g\in \mathcal{G}} \big\{ {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) + 
    \lambda\, \text{pen}_{\mathcal H}(g) \big\} + 2{\sf d}_{\mathcal{F}} (\widehat{P}_n, P^*) \\ 
    &\le \inf_{g \in \mathcal{G}}  \big\{ {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) + \lambda\, \text{pen}_{\mathcal H}
    (g) \big\} + 2{\sf d}_{\mathcal{F}} (\widehat{P}_n, g^*\sharp\mathcal U_d) + 2\sigma^*.\label{eq:8}
\end{align}
The last line follows from \AssA{L^*}{\sigma^*} and the triangle inequality. By applying \AssA{L^*}{\sigma^*} again, we can establish the existence of independent random variables $\bU_i\sim\mathcal{U}_d$ such that $\mathbb{E}[\|\bX_i-g(\bU_i)\|]\leqslant \sigma^*$, for every $i\in[n]$. This implies that
\begin{align}
    {\sf d}_{\mathcal{F}} (\widehat{P}_n, g^*\sharp\,
    \mathcal U_d) & = \sup_{f\in\mathcal{F}} \Big|\frac1n \sum_{i=1}^n
    f(\bX_i) - \mathbb E[f\circ g^*(\bU)]\Big|\\
    &= \sup_{f\in\mathcal{F}} \bigg\{\Big|
    \frac1n \sum_{i=1}^n\big(
    f(\bX_i) - f\circ g^*(\bU_i)\big) + 
    \frac1n \sum_{i=1}^n\big(f\circ g^*(\bU_i) - 
    \mathbb E[f\circ g^*(\bU)]\big)\Big|\bigg\}\\
    &\leqslant \frac1n \sum_{i=1}^n\big\| \bX_i - 
    g^*(\bU_i)\big\| + \sup_{\psi \in\text{Lip}_{L^*}}
    \Big|
    \frac1n \sum_{i=1}^n\big(
    \psi(\bU_i) - \mathbb E[\psi(\bU)]\Big|.
\end{align}
Here, we used the fact that $\mathcal{F}\subset \text{Lip}_1([0,1]^D\to \mathbb{R})$ and that the composition of a 1-Lipschitz-continuous and an $L^*$-Lipschitz-continuous functions is itself $L^*$-Lipschitz continuous. Taking the expectation and employing the dual formulation of the Wasserstein-$1$ distance, we arrive at
\begin{align}
    \mathbb E[{\sf d}_{\mathcal{F}} (\widehat{P}_n,
    g^*\sharp\, \mathcal U_d)] 
    &\leqslant \frac1n \sum_{i=1}^n\mathbb E[\big\| \bX_i - 
    g^*(\bU_i)\big\|] + L^*\mathbb E[\wass_1(\hat P_{n,U},
    \mathcal U_d)]\\
    &\leqslant \sigma^* + L^*\mathbb E[\wass_1(\hat P_{n,U},
    \mathcal U_d)].\label{eq:9}
\end{align}
Here, $\hat{P}_{n,U}$ is the empirical distribution of the sample $\bU_1,\ldots,\bU_n$. Finally, using the well-known bound on the error of the empirical distribution in the Wasserstein-$1$ distance  (for example, see \citep[Proposition 1]{weed2022estimation}), we obtain:
\begin{align}\label{wass1}
    \mathbb E[\wass_1(\hat P_{n,U},
    \mathcal U_d)] \leqslant \frac{c\sqrt{d}}{n^{1/d}}. 
\end{align}
Combining this bound with (\ref{eq:8}) and 
(\ref{eq:9}), we get the stated upper bound.
\end{proof}

%\newpage
\section{Lower bound on the deviation from the empirical
distribution}\label{sec:4}


In the previous section, we established that all generators ${\hat g_n(\lambda,\mathcal G,\mathcal H):\lambda>0}$ achieve rate optimality under the assumption \AssA{L^*}{\sigma^*}. However, it is important to recognize that if the class $\mathcal G$ is very rich, it becomes highly likely to contain a function $\hat g$ that overfits the training data. In other words, the distance ${\sf d}_{\mathcal F}(\hat g\sharp\,\mathcal U_d,\hat P_n)$ could be very small or even zero. This behavior is undesirable for most generative modeling applications, such as image or music generation. Simply resampling examples from the training set is not the intended outcome.

We show in this section that this issue can be addressed 
by choosing a value for $\lambda$ bounded away from zero, rather than setting it to zero, when using \LIPERM1 with a large $\mathcal G$. By doing so, we can prevent the undesirable behavior described earlier. Importantly, we can also demonstrate that within the family of generators ${\hat g_n(\lambda,\mathcal G,\mathcal H):\lambda>0}$, there exists an element that maximizes the order of magnitude of the 
distance from the distributions supported by the training set. This element achieves the highest possible distance while maintaining minimax-rate-optimal accuracy.

\subsection{Warm up: the case of hard constraint   ($\lambda=\infty$) }

Our goal is to demonstrate that the learned generator, 
$\hat g_n$, does not replicate the examples from the 
training set, with only minor modifications. To quantify 
this property, we examine the distance between the probability measure induced by the generator, $\hat g_n\sharp\,\mathcal{U}_d$, and the empirical 
distribution, $\hat P_n$ (or any other distribution $Q$ satisfying $Q(\{\bX_1,\ldots,\bX_n\}) = 1$). A 
larger distance indicates a greater dissimilarity, 
which is desirable.

We first consider the generator $\hat g_n$ obtained by imposing the hard constraint $h\circ g = \text{Id}_d$ on feasible solutions. This means that $\hat g_n$ minimizes 
the distance, $d_{\mathcal{F}}$, between the push-forward measure $g\sharp\, \mathcal{U}_d$ and the empirical distribution $\hat P_n$, over the set of all $g\in \mathcal{G}$ for which there exists $h\in \mathcal H$ satisfying $h\circ g = \Id_d$.
Let us stress right away that the trained generator, $\hat g_n$, does not produce repeated observations. The probability of generating a specific example, $\boldsymbol{x}$, corresponds to the Lebesgue measure of the set $\hat 
g_n^{-1}(\boldsymbol{x})$. The feasibility condition implies that $\hat g$ is injective, meaning that the set $\hat g^{-1}(\boldsymbol{x})$ contains at most one point and therefore has zero measure. 

While our main results apply to general IPMs, we primarily focus on the $\wass_1$ distance associated with the function class $\mathcal{F} = \text{Lip}_1([0,1]^D\to \mathbb{R})$. This choice of $\mathcal{F}$ is particularly relevant due to its attractive properties, including its ability to capture the geometric structure of data and provide a meaningful distance metric between probability distributions. Moreover,  $\wass_1$, with its interpretation as an optimal transport distance, has found successful applications in various fields, such as computer vision, economics and biology.
\begin{proposition}\label{thm:lower-bound-hard}
    Let $\bX_1, \ldots, \bX_n \in [0,1]^D$ and let 
    $\mathcal F$ contain all 
    the 1-Lipschitz-continuous functions from $\mathbb R^D$ to $\mathbb R$. 
    For any $g:[0,1]^d \to [0,1]^D$ having an $L_{\mathcal H}$-Lipschitz-continuous
    left inverse (that is there exists $h:\mathbb R^D \to \mathbb R^d$ 
    such that $h \circ g = \Id_d$), it holds that
    \begin{align}\tag{LB-hard}
        {\sf d}_{\mathcal{F}}(g \sharp\, \mathcal{U}_d, \widehat{P}_n) \geq \frac{1}{2L_{\mathcal H}(1 + (2{\sf V}_d n)^{1/d})},
        \label{eq:lb1}
     \end{align}
     where $\hat P_n$ is the empirical distribution of $\bX_1,\ldots,\bX_n$ and\footnote{This value of ${\sf V}_d$ corresponds
     to the volume of the unit ball in $\mathbb R^d$.} ${\sf V}_d = \pi^{d/2}/\Gamma(1+d/2)$.
\end{proposition}

%\son[inline]{Here $C_d$ is not the volume of a unit $d$-ball, it is $C_d = 2({\sf V}_d)^{1/d}$, correct? See \Cref{prop:lower_bound}.}
\begin{proof}
    Recall that
    \begin{align}
        {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, \widehat{P}_n) = \sup_{f \in \mathcal{F}} \bigg| \int_{[0,1]^d} f({g}(\bu))\,\rmd\bu - \frac{1}{n} \sum\limits_{i=1}^n f(\bX_i) \bigg|. 
    \end{align}
    In addition, there exists an $L_{\mathcal H}$-Lipschitz-continuous function $h$ such that $ h\circ g = \Id_d$ and 
    \begin{align}
        \| h(\bx)- h(\bx')\|\leqslant L_{\mathcal H}\|\bx- \bx'\|
        \qquad \forall \bx,\bx'\in\mathbb R^D. 
    \end{align}
    In order to establish the lower bound on ${\sf d}_{\mathcal{F}}({g} \sharp 
    \mathcal{U}_d, \widehat{P}_n)$, we use the fact that $\mathcal F$
    contains all the functions of the form $\psi \circ {h}$, where 
    $\psi:[0,1]^d \to \mathbb{R}$ is any $(1/L_{\mathcal H})$-Lipschitz-continuous function.
    Indeed, since $h$ is $L_{\mathcal H}$-Lipschitz, the function $\psi\circ h$ 
    belongs to $\text{Lip}_1\subseteq \mathcal F$. Therefore,
    \begin{align}
        {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, \widehat{P}_n)
        &\geqslant \sup_{\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}} \bigg\{ \int_{[0,1]^d} (\psi\circ \underbrace{ h\circ {g}}_{=\Id_d})(\bu)\,\rmd\bu - \frac{1}{n} \sum\limits_{i=1}^n (\psi\circ h)(\bX_i) \bigg\}\\
        & = \sup_{\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}} \bigg\{\int_{[0,1]^d} \psi (\bx)\,\rmd\bx - 
        \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg\},
    \end{align}
    where we have used the notation $\bZ_i = h(\bX_i)$, for $i=1,\ldots,n$. 
    Clearly, $\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}$ is equivalent to 
    $L_{\mathcal H}\psi \in \text{Lip}_1$. This implies that 
    \begin{align}
        {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, \widehat{P}_n)
        &\geqslant \frac1{L_{\mathcal H}}\sup_{\psi \in \text{Lip}_1} \bigg\{\int_{[0,1]^d} \psi (\bu)\,\rmd\bu - \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg\}.
    \end{align}
    The right-hand side of the inequality above is precisely the 
    $\wass_1$ distance between the uniform measure on $[0,1]^d$ and 
    the empirical distribution of $\bZ_1, \dots, \bZ_n$. Therefore, we arrive at
    \begin{align}
    {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, \widehat{P}_n) \ge \frac{1}{L_{\mathcal H}} \wass_1\Big(\mathcal{U}_d, \frac{1}{n}\sum_{i=1}^n \delta_{\bZ_i}\Big) \ge \frac{1}{2L_{\mathcal H}(1 + (2{\sf V}_d n)^{1/d})},
\end{align}
where the last inequality follows from \Cref{prop:lower_bound}, postponed to
the supplementary material.
\end{proof}

To the best of our knowledge, our result in \Cref{thm:lower-bound-hard} provides the first mathematical quantification of the diversity of examples generated by the generator. It not only demonstrates that the generator deviates maximally from the empirical distribution but also shows that it maintains the same minimum distance from any distribution concentrated on $n$ points.

Moreover, by combining the lower bound in 
\Cref{thm:lower-bound-hard} with the accuracy upper bound in \Cref{thm:upper} for \LIPERM1, we observe that the right-hand side of equation (\ref{eq:lb1}) approaches zero at the optimal rate as $n$ tends to infinity.

Lastly, it is important to highlight that the derived lower bound is computable as it depends solely on the known quantities $n$, $d$, and $L_{\mathcal H}$.

\subsection{The case of soft constraint: $\lambda\in(0,\infty)$}

We now turn our attention to measuring the dissimilarity between the empirical distribution and
the generator obtained from \LIPERM1 when $\lambda <
+\infty$. This scenario is known as the case of a soft
constraint on left-invertibility. It is important to
recall that the introduction of a penalized version of
the optimization problem, as opposed to the hard
constraint, aims to enhance tractability and
facilitate implementation using available tools.
However, the trade-off is that the lower bound on the
deviation from the empirical distribution, as
presented in the following theorem, is slightly weaker
than that for the constrained generator.

\begin{theorem}\label{thm:lower-bound-pen}
    Let $\bX_1,\dots, \bX_n \in [0,1]^D$ and $\mathcal{H} \subseteq \textup{Lip}_{L_{\mathcal H}}([0,1]^D\to [0,1]^d)$. If $\mathcal{F}$ 
    contains all the $1$-Lipschitz-continuous functions from $\mathbb{R}^D$ to $\mathbb{R}$, then, for any positive $\lambda$ it holds 
    \begin{align}
        {\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\,\mathcal{U}_d, \hat P_n) \ge \frac{1}{2L_{\mathcal H}(1+(2{\sf V}_dn)^{1/d})} - \frac1{L_{\mathcal H}\lambda^{1/q}}  \inf_{g \in \mathcal{G}_0} {\sf d}_{\mathcal{F}}^{1/q}(g\sharp\,\mathcal{U}_d, \hat P_n),
    \end{align}
    where $\hat P_n$ is the empirical distribution of $\bX_1,\ldots,\bX_n$
    %, ${\sf V}_d = \pi^{d/2}/\Gamma(1+d/2)$ is the volume of the unit ball in $\mathbb{R}^d$ 
    and $\mathcal{G}_0 = 
    \{g\in\mathcal G:\textup{pen}(g) = 0\}$.
\end{theorem}
\begin{proof}[Proof of \Cref{thm:lower-bound-pen}]
    Let $\hat h_n$ be a function from $\mathcal H$ 
    attaining the minimum $\min_{h\in\mathcal H} 
    \|h\circ\hat g_n - \Id_d\|_{\mathbb L_q}^q$. 
    Since $\mathcal{F}$ contains all 
    $1$-Lipschitz continuous functions, it also 
    contains all the functions of the form 
    $f = \psi \circ \hat{h}_n$, for $\psi \in \textup{Lip}(1/L_{\mathcal H})$. Using the
    notation $\bZ_i = \hat h_n(\bX_i)$, this 
    implies that
    \begin{align}
        {\sf d}_{\mathcal{F}}(\hat{g}_{n} \sharp\, \mathcal{U}_d, \widehat{P}_n)
        &= \sup_{f \in \mathcal{F}} \bigg| \int_{[0,1]^d} f(\hat{g}_{n}(\bx))d \bx - \frac{1}{n} \sum\limits_{i=1}^n f(\bX_i) \bigg| \\ 
        &\geqslant \sup_{\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}} \bigg\{ \int_{[0,1]^d} (\psi\circ  \hat{h}_n\circ \hat{g}_n)(\bx)\,d\bx - \frac{1}{n} \sum\limits_{i=1}^n (\psi\circ \hat{h}_n)(\bX_i) \bigg\}\\
        &\geqslant \frac1{L_{\mathcal H}}
        \sup_{\psi \in \text{Lip}_1} \bigg\{ \int_{[0,1]^d} (\psi\circ \underbrace{ \hat{h}_n\circ \hat{g}_{n}}_{\approx\Id_d})(\bu)\,d\bu - \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg\}.
        \label{eq:5}
    \end{align}
    By adding and subtracting the term $\int_{
    [0,1]^d} \psi(\bu)\,d\bu$, we arrive at 
    \begin{align}
        {\sf d}_{\mathcal{F}}(\hat{g}_{n} \sharp\, \mathcal{U}_d, \widehat{P}_n)
        & \ge \frac1{L_{\mathcal{H}}} 
        \sup_{\psi \in \text{Lip}_1}  
        \bigg|\int_{[0,1]^d} \psi (\bu)\,d\bu - \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg|
        - \frac1{L_{\mathcal{H}}}  \sup_{\psi \in \text{Lip}_1} 
        \big\|\psi (\hat{h}_n \circ \hat{g}_{n}
        ) - \psi\big\|_{\mathbb L_1}\\ 
        &\geqslant \frac1{L_{\mathcal{H}}} \Big(
        \wass_1(\mathcal{U}_d,\hat P_{n,Z}) 
        -   
        \big\|\hat{h}_n \circ \hat{g}_{n} -
        \Id_d\big\|_{\mathbb L_1}\Big)\\
        &\geq \frac1{L_{\mathcal{H}}} \Big(
        \wass_1(\mathcal{U}_d,\hat P_{n,Z}) 
        -   \text{pen}_{\mathcal H}(\hat g_n)^{1/q}\Big).
        \label{eq:decomposition1}
    \end{align}
    Here, the second inequality follows from the
    Lipschitz-continuity of $\psi$, whereas the
    last inequality is a consequence of the facts that
    the $\mathbb L_1$ norm on $[0,1]^d$ is dominated
    by the $\mathbb L_q$ norm given $q\ge 1$, and  that $\hat h_n$
    is a minimizer of $\|h\circ \hat g_n - \Id_d\|_q$
    over $\mathcal H$.     
    We use the same lower bound here
    \begin{align}\label{eq:lower-bound-term1}
        \wass_1(\mathcal{U}_d,\hat P_{n,Z})  
        \ge \frac{1}{2+2(2{\sf V}_dn)^{1/d}}.  
    \end{align}
    In order to complete the proof we need to find an upper bound on the second term of the right-hand side of (\ref{eq:decomposition1}). 
    In view of (\ref{LIPERM}), we know that for any 
    $g \in \mathcal{G}$ it holds
    \begin{align}
        {\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\, \mathcal{U}_d, \widehat{P}_n) + \lambda\, \text{pen}_{\mathcal H}( \hat{g}_{n}) &\le {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, \widehat{P}_n) + \lambda\, \text{pen}_{\mathcal H}(g).
    \end{align}
    Since the distance ${\sf d}_{\mathcal F}$ is
    always nonnegative, by choosing $g$ from 
    $\mathcal G_0$ the last term of the last
    display vanishes and we get
    \begin{align}\label{eq:penalty-bound}
        \lambda\,\text{pen}_{\mathcal H}(\hat{g}_{n}) 
        \leq  \inf_{g \in \mathcal{G}_0} 
        {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, \widehat{P}_n). 
    \end{align}
    Combining inequalities (\ref{eq:decomposition1}), 
    (\ref{eq:lower-bound-term1}) and 
    (\ref{eq:penalty-bound}) 
    we obtain the claim of the theorem. 
\end{proof}

\begin{corollary}
    If $\lambda $ is chosen so that the inequality
    \begin{align}\label{eq:cor1}
        \lambda \geq 4^q (1+ (2{\sf V}_dn)^{1/d})^q
        \inf_{g \in \mathcal{G}_0}\mathbb E\big[ 
        {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, \widehat{P}_n)\big],
    \end{align}
    holds true, then 
    \begin{align}
        \mathbb E\big[{\sf d}_{\mathcal{F}}(\hat{g}_{n}\sharp\,\mathcal{U}_d, \hat P_n) \big]\ge \frac{1}{4L_{\mathcal H}(1+(2{\sf V}_dn)^{1/d})}.
    \end{align}
\end{corollary}

When $\mathcal F  = \textup{Lip}_1$, and the
corresponding IPM is the $\wass_1$ distance, 
the above corollary tells us that as soon as
the penalty parameter $\lambda$ is larger than
$4^q(1+ (2{\sf V}_dn)^{1/d})^q \inf_{\mathcal{G}_0}
\wass_1(g\sharp\, \mathcal{U}_d, \widehat{P}_n)$, 
\LIPERM1 generator deviates maximally from the
empirical distributions. In order to gain some
understanding on how restrictive this constraint
on $\lambda$ is, let us assume that $g^*$ from 
\AssA{L^*}{\sigma^*} belongs to $\mathcal G_0$. 
Then, one can use the inequality
\begin{align}
    \inf_{\mathcal{G}_0}\mathbb E\big[
\wass_1(g\sharp\, \mathcal{U}_d, \widehat{P}_n)\big]
&\leq \mathbb E\big[
\wass_1(g^*\sharp\, \mathcal{U}_d, \widehat{P}_n)
\big]
\leq \mathbb E\big[\wass_1(g^*\sharp\, \mathcal{U}_d, g^*\sharp\hat P_{n,U})\big] + \mathbb E\big[\wass_1(g^*\sharp\hat P_{n,U}, \widehat{P}_n) \big]\\
&\leq L^*\mathbb E\big[\wass_1(\mathcal{U}_d, \hat P_{n,U})\big] + \sigma^*
\leqslant \frac{cL^*\sqrt{d}}{n^{1/d}} + \sigma^*.
\label{eq:6}
\end{align}
Here, $\hat P_{n,U}$ is the empirical distribution
of $n$ iid random vectors $\bU_i\sim \mathcal U_d$ 
ensuring the optimal coupling between $g^*\sharp\,
\mathcal U_d$ and $P^*$. The second line of the
last display follows from (\ref{wass1}) and 
the inequality $\wass_1 (\hat P_{n,U},\hat P_n)
\leq (1/n)\sum_{i=1}^n \|\bU_i-\bX_i\|$. 

\begin{remark}
In view of (\ref{eq:cor1}) and (\ref{eq:6}), 
when $\sigma^* = 0$ and $q=2$, choosing the penalty
parameter $\lambda$ larger than $C_d n^{1/d}$---
for a constant $C_d$ that depends only on the
dimension of the latent space---
is enough to guarantee that the generator 
\LIPERM1 will significantly deviate from the
empirical distribution.     
\end{remark}




% \section{Experimental results}
% In this section, we provide the results of the experiments conducted both on synthetically generated datasets and real data examples, such as CIFAR-10. We start by describing the setup for a synthetically generated dataset. The procedure for generating these points is the following: we generate points $k=3$ vectors in $\mathbb{R}^2$, that would correspond to the mean vectors for each Gaussian component. For simplicity of the presentation, we choose the covariance matrix to be the identity matrix. 

% \subsection{Gaussian Mixture Model (GMM)}

% \begin{itemize}
%     \item gaussian mixtures experiments of type $(1-\delta)\mathcal{N}(0, 0.1) + \delta \mathcal{N}(1, \sigma)$. 
%     \item CIFAR-10 as an example of experiment 
% \end{itemize}

% \subsection{CIFAR-10}


\section{Summary and Conclusion}
\label{sec:5}

In this paper, we have presented a theoretical analysis of training generative models with two key properties: convergence to the true data-generating distribution at a minimax optimal rate, and avoidance of replication of examples from the training data. Our main contribution is the introduction of the left-inverse-penalized empirical risk minimization (\LIPERM1) framework for learning generative models. The \LIPERM1 approach involves minimizing the penalized empirical risk, where the penalty term encourages the learned generator to possess a smooth inverse. Moreover, the appropriate choice of the penalty coefficient $\lambda$ allows us to lower bound the distance between the push-forward measure of our generator and the empirical distribution of the training sample.

Our results are applicable to general IPMs used to quantify the distance between probability distributions and hold true under model misspecification. Arguably, the most interesting example of IPMs is the Wasserstein distance. The theoretical insights provided in this paper can guide the development of efficient algorithms for generative modeling. The similarity between our left-inverse penalty and existing methods such as variational autoencoders \citep{kingma_welling_2013} and cycle-GAN \citep{zhu_park_isola_efros_2017} suggests that incorporating insights from these techniques can lead to further improvements in generative modeling. Overall, our work contributes to the understanding and advancement of generative modeling methods, enabling their wider application in diverse domains.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize
\bibliography{gen_models}
%\bibliographystyle{alpha}
}


%\iffalse
\newpage

%\appendix

% \part{Appendix} % Start the appendix part
% \parttoc % Insert the appendix TOC


\input{appendix_updated}

\end{document}