%bbbb
@article{bai_ma_risteski_2019,
  title={Approximability of Discriminators Implies Diversity in GANs},
  author={Bai, Y. and Ma, T. and Risteski, A.},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.10586}
}

@article{roth_gan_regulariz,
author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
year = {2017},
month = {05},
pages = {},
title = {Stabilizing Training of Generative Adversarial Networks through Regularization}
}

@article{repecka2021expanding,
  title={Expanding functional protein sequence spaces using generative adversarial networks},
  author={Repecka, Donatas and Jauniskis, Vykintas and Karpus, Laurynas and Rembeza, Elzbieta and Rokaitis, Irmantas and Zrimec, Jan and Poviloniene, Simona and Laurynenas, Audrius and Viknander, Sandra and Abuajwa, Wissam and others},
  journal={Nature Machine Intelligence},
  volume={3},
  number={4},
  pages={324--333},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{maziarka2020mol,
  title={Mol-CycleGAN: a generative model for molecular optimization},
  author={Maziarka, {\L}ukasz and Pocha, Agnieszka and Kaczmarczyk, Jan and Rataj, Krzysztof and Danel, Tomasz and Warcho{\l}, Micha{\l}},
  journal={Journal of Cheminformatics},
  volume={12},
  number={1},
  pages={1--18},
  year={2020},
  publisher={BioMed Central}
}

@article{wiese2020quant,
  title={Quant GANs: deep generation of financial time series},
  author={Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter},
  journal={Quantitative Finance},
  volume={20},
  number={9},
  pages={1419--1440},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{paganini2018calogan,
  title={CaloGAN: Simulating 3D high energy particle showers in multilayer electromagnetic calorimeters with generative adversarial networks},
  author={Paganini, Michela and de Oliveira, Luke and Nachman, Benjamin},
  journal={Physical Review D},
  volume={97},
  number={1},
  pages={014021},
  year={2018},
  publisher={APS}
}

@article{fekri2019generating,
  title={Generating energy data for machine learning with recurrent generative adversarial networks},
  author={Fekri, Mohammad Navid and Ghosh, Ananda Mohon and Grolinger, Katarina},
  journal={Energies},
  volume={13},
  number={1},
  pages={130},
  year={2019},
  publisher={MDPI}
}

@article{gagne2020machine,
  title={Machine learning for stochastic parameterization: Generative adversarial networks in the Lorenz'96 model},
  author={Gagne, David John and Christensen, Hannah M and Subramanian, Aneesh C and Monahan, Adam H},
  journal={Journal of Advances in Modeling Earth Systems},
  volume={12},
  number={3},
  pages={e2019MS001896},
  year={2020},
  publisher={Wiley Online Library}
}


@inproceedings{nie2017medical,
  title={Medical image synthesis with context-aware generative adversarial networks},
  author={Nie, Dong and Trullo, Roger and Lian, Jun and Petitjean, Caroline and Ruan, Su and Wang, Qian and Shen, Dinggang},
  booktitle={Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20},
  pages={417--425},
  year={2017},
  organization={Springer}
}

@article{Liang,
author = {Liang, Tengyuan},
title = {How Well Generative Adversarial Networks Learn Distributions},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {228},
numpages = {41},
}

@article{chen2022minimax,
  title={Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs},
  author={Chen, Sitan and Li, Jerry and Li, Yuanzhi and Meka, Raghu},
  journal={arXiv preprint arXiv:2201.07206},
  year={2022}
}

@article{chae2023likelihood,
  title={A Likelihood Approach to Nonparametric Estimation of a Singular Distribution Using Deep Generative Models},
  author={Chae, Minwoo and Kim, Dongha and Kim, Yongdai and Lin, Lizhen},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={77},
  pages={1--42},
  year={2023}
}

@article{kwon2023minimax,
  title={Minimax optimal density estimation using a shallow generative model with a one-dimensional latent variable},
  author={Kwon, Hyeok Kyu and Chae, Minwoo},
  journal={arXiv preprint arXiv:2305.06755},
  year={2023}
}

@article{Block,
  author       = {Adam Block and
                  Youssef Mroueh and
                  Alexander Rakhlin},
  title        = {Generative Modeling with Denoising Auto-Encoders and Langevin Sampling},
  journal      = {CoRR},
  volume       = {abs/2002.00107},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.00107},
  eprinttype    = {arXiv},
  eprint       = {2002.00107},
  timestamp    = {Mon, 10 Feb 2020 15:12:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-00107.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{debortoli2022riemannian,
      title={Riemannian Score-Based Generative Modelling}, 
      author={De Bortoli, Valentin  and Mathieu, Emile and Hutchinson, Michael and Thornton, James and Teh, Yee Whye and Doucet, Arnaud},
      year={2022},
      eprint={2202.02763},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Bortoli1,
 author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17695--17709},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion Schr\"{o}dinger Bridge with Applications to Score-Based Generative Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{oko2023diffusion,
  title={Diffusion Models are Minimax Optimal Distribution Estimators},
  author={Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2303.01861},
  year={2023}
}

@article{Biau,
author = {G{\'e}rard Biau and Beno{\^i}t Cadre and Maxime Sangnier and Ugo Tanielian},
title = {{Some theoretical properties of GANS}},
volume = {48},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {1539 -- 1566},
keywords = {adversarial principle, central limit theorem, generative models, Jensen–Shannon divergence, neural networks},
year = {2020},
doi = {10.1214/19-AOS1858},
URL = {https://doi.org/10.1214/19-AOS1858}
}

@article{BiauST21,
  author       = {G{\'{e}}rard Biau and
                  Maxime Sangnier and
                  Ugo Tanielian},
  title        = {Some Theoretical Insights into Wasserstein GANs},
  journal      = {J. Mach. Learn. Res.},
  volume       = {22},
  pages        = {119:1--119:45},
  year         = {2021},
  url          = {http://jmlr.org/papers/v22/20-553.html},
  timestamp    = {Mon, 31 Jan 2022 17:23:36 +0100},
  biburl       = {https://dblp.org/rec/journals/jmlr/BiauST21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yan2018deeplesion,
  title={DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning},
  author={Yan, Ke and Wang, Xiaosong and Lu, Le and Summers, Ronald M},
  journal={Journal of medical imaging},
  volume={5},
  number={3},
  pages={036501--036501},
  year={2018},
  publisher={Society of Photo-Optical Instrumentation Engineers}
}

%cccc
@article{chen2016variational,
  title={Variational Inference for Gaussian Process Models with Linear Complexity},
  author={Chen, Y. and Hoffman, M. D. and Colmenarejo, S. G. and Deniz, O. and Li, N. and Paisley, J. and Sontag, D. and Wainwright, M. J.},
  journal={International Conference on Machine Learning},
  pages={290--299},
  year={2016}
}

@misc{belomestny2023rates,
      title={Rates of convergence for density estimation with generative adversarial networks}, 
      author={Denis Belomestny and Eric Moulines and Alexey Naumov and Nikita Puchkin and Sergey Samsonov},
      year={2023},
      eprint={2102.00199},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

%dddd
@article{kingma_welling_2013,
  title={Auto-encoding variational bayes},
  author={Kingma, D. P. and Welling, M.},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{Boissard,
author = {Emmanuel Boissard and Thibaut Le Gouic},
title = {{On the mean speed of convergence of empirical and occupation measures in Wasserstein distance}},
volume = {50},
journal = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
number = {2},
publisher = {Institut Henri Poincaré},
pages = {539 -- 563},
keywords = {Functional quantization, Markov chains, measure theory, Optimal transportation, Transportation inequalities, Wasserstein metrics},
year = {2014},
doi = {10.1214/12-AIHP517},
URL = {https://doi.org/10.1214/12-AIHP517}
}

@article{dudley_1969,
author = {R. M. Dudley},
title = {{The Speed of Mean Glivenko-Cantelli Convergence}},
volume = {40},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {40 -- 50},
abstract = {},
year = {1969},
doi = {10.1214/aoms/1177697802},
URL = {https://doi.org/10.1214/aoms/1177697802}
}


%gggg
@article{goodfellow2014generative,
  title={Generative Adversarial Networks},
  author={Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  pages={2672--2680},
  year={2014}
}

@article{grathwohl2018ffjord,
  title={FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models},
  author={Grathwohl, W. and Chen, R. T. Q. and Betterncourt, J. and Sutskever, I. and Duvenaud, D. and Zhang, R.},
  journal={International Conference on Learning Representations},
  year={2018}
}


%hhhh
@article{huang_etal_2022,
  title={An Error Analysis of Generative Adversarial Networks for Learning Distributions},
  author={Huang, J. and Jiao, Y. and Li, ZH. and Liu, Sh.  and Wang, Y. and Yang, Y.},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={1--43},
  year={2022}
}

%llll
@article{li2018learning,
  title={Learning Independent Causal Mechanisms},
  author={Li, Y. and Sch{\"o}lkopf, B. and Janzing, D.},
  journal={International Conference on Machine Learning},
  pages={2935--2944},
  year={2018}
}


@InProceedings{pmlr-v206-ting-li23a,
  title = 	 {Mode-Seeking Divergences: Theory and Applications to GANs},
  author =       {Li, C. T. and Farnia, F.},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {8321--8350},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/ting-li23a/ting-li23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/ting-li23a.html},
  abstract = 	 {Generative adversarial networks (GANs) represent a game between two neural network machines designed to learn the distribution of data. It is commonly observed that different GAN formulations and divergence/distance measures used could lead to considerably different performance results, especially when the data distribution is multi-modal. In this work, we give a theoretical characterization of the mode-seeking behavior of general f-divergences and Wasserstein distances, and prove a performance guarantee for the setting where the underlying model is a mixture of multiple symmetric quasiconcave distributions. This can help us understand the trade-off between the quality and diversity of the trained GANs’ output samples. Our theoretical results show the mode-seeking nature of the Jensen-Shannon (JS) divergence over standard KL-divergence and Wasserstein distance measures. We subsequently demonstrate that a hybrid of JS-divergence and Wasserstein distance measures minimized by Lipschitz GANs mimics the mode-seeking behavior of the JS-divergence. We present numerical results showing the mode-seeking nature of the JS-divergence and its hybrid with the Wasserstein distance while highlighting the mode-covering properties of KL-divergence and Wasserstein distance measures. Our numerical experiments indicate the different behavior of several standard GAN formulations in application to benchmark Gaussian mixture and image datasets.}
}


@article{loaiza2017maximum,
  title={Maximum Entropy Generators for Energy-Based Models},
  author={Loaiza-Ganem, G. and Welling, M.},
  journal={arXiv preprint arXiv:1702.08211},
  year={2017}
}

%kkkk
@article{kingma2014autoencoding,
  title={Auto-Encoding Variational Bayes},
  author={Kingma, D. P. and Welling, M.},
  journal={International Conference on Learning Representations},
  year={2014}
}

%mmmm
@article{mescheder2017adversarial,
  title={Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks},
  author={Mescheder, L. and Nowozin, S. and Geiger, A.},
  journal={International Conference on Machine Learning},
  pages={2391--2400},
  year={2017}
}

%nnnn

@InProceedings{pmlr-v206-naganuma23a,
  title = 	 {Conjugate Gradient Method for Generative Adversarial Networks},
  author =       {Naganuma, H. and Iiduka, H.},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4381--4408},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/naganuma23a/naganuma23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/naganuma23a.html},
  abstract = 	 {One of the training strategies of generative models is to minimize the Jensen–Shannon divergence between the model distribution and the data distribution. Since data distribution is unknown, generative adversarial networks (GANs) formulate this problem as a game between two models, a generator and a discriminator. The training can be formulated in the context of game theory and the local Nash equilibrium (LNE). It does not seem feasible to derive guarantees of stability or optimality for the existing methods. This optimization problem is far more challenging than the single objective setting. Here, we use the conjugate gradient method to reliably and efficiently solve the LNE problem in GANs. We give a proof and convergence analysis under mild assumptions showing that the proposed method converges to a LNE with three different learning rate update rules, including a constant learning rate. Finally, we demonstrate that the proposed method outperforms stochastic gradient descent (SGD) and momentum SGD in terms of best Frechet inception distance (FID) score and outperforms Adam on average.}
}




%pppp
@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}

%rrrr

@article{rezende2015variational,
  title={Variational Inference with Normalizing Flows},
  author={Rezende, D. J. and Mohamed, S. and Wierstra, D.},
  journal={arXiv preprint arXiv:1505.05770},
  year={2015}
}


%ssss
@InProceedings{schreuder_brunel_dalalyan_2021,
  title = 	 {Statistical guarantees for generative models without domination},
  author =       {Schreuder, Nicolas and Brunel, Victor-Emmanuel and Dalalyan, Arnak},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {1051--1071},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/schreuder21a/schreuder21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/schreuder21a.html},
  abstract = 	 {In this paper, we introduce a convenient framework for studying (adversarial) generative models from a statistical perspective. It consists in modeling the generative device as a smooth transformation of the unit hypercube of a dimension that is much smaller than that of the ambient space and measuring the quality of the generative model by means of an integral probability metric. In the particular case of integral probability metric defined through a smoothness class, we establish a risk bound quantifying the role of various parameters. In particular, it clearly shows the impact of dimension reduction on the error of the generative model.}
}

@article{song2019generative,
  title={Generative Models for Graph-Based Protein Design},
  author={Song, W. and Aqvist, J. and Lindorff-Larsen, K.},
  journal={Journal of Chemical Theory and Computation},
  volume={15},
  pages={2323--2332},
  year={2019}
}


%wwww
@article{weed2022estimation,
author = {Jonathan Niles-Weed and Philippe Rigollet},
title = {{Estimation of Wasserstein distances in the Spiked Transport Model}},
volume = {28},
journal = {Bernoulli},
number = {4},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {2663 -- 2688},
keywords = {High-dimensional statistics, Optimal transport, Wasserstein distance},
year = {2022},
doi = {10.3150/21-BEJ1433},
URL = {https://doi.org/10.3150/21-BEJ1433}
}

%xxxx
@article{xi_reddy_2023,
  title={Indeterminacy in Generative Models: Characterization and Strong Identifiability},
  author={Xi, Q. and Bloem-Reddy, B.},
  journal={arXiv preprint arXiv:2206.00801},
  year={2023}
}

%zzzz
@inproceedings{zhu_park_isola_efros_2017,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, J.-Y. and Park, T. and Isola, P. and Efros, A. A.},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2868--2876},
  year={2017}
}

