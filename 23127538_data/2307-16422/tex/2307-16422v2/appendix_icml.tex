\section*{Appendix}\label{app}
% Start the appendix part
%\parttoc % Insert the appendix TOC
% \tableofcontents


The purpose of this appendix is twofold: to present the proofs of all the mathematical claims presented in the main paper and to provide some additional experimental results. We illustrate the effect of the penalization parameter $\lambda$ on the trained generator using the \ref{LIPERMe} framework. The code
that can be used to reproduce all the experiments can be found here \url{https://github.com/TigranGalstyan/LIPERM_annotated_deep_learning_paper_implementations/tree/liperm/labml_nn/gan/wasserstein/gradient_penalty/liperm}.



\section{Proof of the upper bound on the risk}


The proof of \Cref{thm:upper}, provided below, relies on repeated use of the triangle inequality, the near-minimization property of \ref{LIPERMe}, as well as on exploiting Lipschitz continuity assumptions. The final bound is derived by combining these inequalities with the approximation bound
in the Wasserstein-$1$ distance of the uniform distribution
by its empirical counterpart.

This result demonstrates that under mild assumptions, the generator trained by \ref{LIPERMe}
achieves the optimal rate for any choice of $\lambda$. Therefore, there is flexibility in selecting $\lambda$ to enforce dissimilarity with the training examples without compromising accuracy.

\subsection{Proof of \Cref{thm:upper}}\label{appA:1}

Let $g$ be an arbitrary element from $\mathcal{G}$. The proof begins by using the triangle inequality multiple times, resulting in the following sequence of inequalities:
\begin{align}
   {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon}\sharp\, \mathcal{U}_d, P^*) & \leq  {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon}\sharp\, \mathcal{U}_d, P_{n,X}) + {\sf d}_{\mathcal{F}}(P_{n,X}, P^*) \nonumber \\
   & = {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon}\sharp\, \mathcal{U}_d, P_{n,X}) + \lambda\, \textup{pen}_{\mathcal{H}}(\widehat{g}_{n, \epsilon}) - \lambda\, \textup{pen}_{\mathcal{H}}(\widehat{g}_{n, \epsilon}) + {\sf d}_{\mathcal{F}} (P_{n,X}, P^*) \nonumber \\
   & \leq {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P_{n,X}) + \lambda\, \textup{pen}_{\mathcal H}( g) + \epsilon - \lambda\, \textup{pen}_{\mathcal{H}}(\widehat{g}_{n, \epsilon}) + {\sf d}_{\mathcal{F}} (P_{n,X}, P^*) \nonumber \\
   & \leq {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) + \lambda\, \textup{pen}_{\mathcal H}( g) + \epsilon - \lambda\, \textup{pen}_{\mathcal{H}}(\widehat{g}_{n, \epsilon}) + 2{\sf d}_{\mathcal{F}} (P_{n,X}, P^*).
\end{align}

In the second inequality, we use the fact that $\widehat{g}_{n,\eps}$ is an $\eps$-minimizer of (\ref{LIPERM}), whereas in the last line, the triangle inequality is applied. As the left-inverse penalty is always nonnegative, the last display implies:
\begin{align}\label{eq:oracle-ineq}
    {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon}\sharp\, \mathcal{U}_d, P^*) &\le \inf_{g\in \mathcal{G}} \big\{ {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) +
    \lambda\, \textup{pen}_{\mathcal H}(g) \big\} + 2{\sf d}_{\mathcal{F}} (P_{n,X}, P^*) + \epsilon \\
    &\le \inf_{g \in \mathcal{G}}  \big\{ {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P^*) + \lambda\, \textup{pen}_{\mathcal H}
    (g) \big\} + 2{\sf d}_{\mathcal{F}} (P_{n,X}, g^*\sharp\mathcal U_d) + 2\sigma^* + \epsilon.\label{eq:8}
\end{align}
The last line follows from \AssA{L^*}{\sigma^*} and the triangle inequality. By applying \AssA{L^*}{\sigma^*} again, we can establish the existence of independent random variables $\bU_i\sim\mathcal{U}_d$ such that $\mathbb{E}[\|\bX_i-g(\bU_i)\|]\leqslant \sigma^*$, for every $i\in[n]$. This implies that
\begin{align}
    {\sf d}_{\mathcal{F}} (P_{n,X}, g^*\sharp\,
    \mathcal U_d) & = \sup_{f\in\mathcal{F}} \Big|\frac1n \sum_{i=1}^n
    f(\bX_i) - \mathbb E[f\circ g^*(\bU)]\Big|\\
    &= \sup_{f\in\mathcal{F}} \bigg\{\Big|
    \frac1n \sum_{i=1}^n\big(
    f(\bX_i) - f\circ g^*(\bU_i)\big) +
    \frac1n \sum_{i=1}^n\big(f\circ g^*(\bU_i) -
    \mathbb E[f\circ g^*(\bU)]\big)\Big|\bigg\}\\
    &\leqslant \frac1n \sum_{i=1}^n\big\| \bX_i -
    g^*(\bU_i)\big\| + \sup_{\psi \in\text{Lip}_{L^*}}
    \Big|
    \frac1n \sum_{i=1}^n\big(
    \psi(\bU_i) - \mathbb E[\psi(\bU)]\Big|.
\end{align}
Here, we used the fact that $\mathcal{F}\subseteq \text{Lip}_1([0,1]^D\to \mathbb{R})$ and that the composition of a 1-Lipschitz-continuous and an $L^*$-Lipschitz-continuous functions is itself $L^*$-Lipschitz continuous. Taking the expectation and employing the dual formulation of the Wasserstein-$1$ distance, we arrive at
\begin{align}
    \mathbb E[{\sf d}_{\mathcal{F}} (P_{n,X},
    g^*\sharp\, \mathcal U_d)]
    &\leqslant \frac1n \sum_{i=1}^n\mathbb E[\big\| \bX_i -
    g^*(\bU_i)\big\|] + L^*\mathbb E[\wass_1( P_{n,U},
    \mathcal U_d)]\\
    &\leqslant \sigma^* + L^*\mathbb E[\wass_1( P_{n,U},
    \mathcal U_d)].\label{eq:9}
\end{align}
Here, $\hat{P}_{n,U}$ is the empirical distribution of the sample $\bU_1,\ldots,\bU_n$. Finally, using the well-known bound on the error of the empirical distribution in the Wasserstein-$1$ distance  (for example, see \citep[Proposition 1]{weed2022estimation}), we obtain:
\begin{align}\label{wass1}
    \mathbb E[\wass_1( P_{n,U},
    \mathcal U_d)] \leqslant \frac{c\sqrt{d}}{n^{1/d}}.
\end{align}
Combining this bound with (\ref{eq:8}) and
(\ref{eq:9}), we get the stated upper bound.

\section{Proofs for the deviation of the generative distribution from the empirical distribution}

\subsection{Lower bounding the distance between the uniform distribution and any discrete distribution}

\def\ba{\boldsymbol{a}}
\def\by{\boldsymbol{y}}

\begin{proposition}\label{prop:lower_bound}
    Assume that $\mathcal{F}$ contains all the $1$-Lipschitz-continuous functions from $\mathbb{R}^d $ to $\mathbb{R}$. For any set  of points $\ba_1,\ldots,\ba_n\in [0,1]^d$
    and any set of weights $w_1,\ldots,w_n\geq 0$ summing to one, we have
    \begin{align}
        {\sf d}_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{\ba_i}\Big)
        \geqslant
        \frac{1}{2 + 2(2{\sf V}_d n)^{1/d}}\ ,
    \end{align}
    where $\mathcal U_d$ is the uniform distribution on $[0,1]^d$
     with ${\sf V}_d = \frac{\pi^{{d}/{2}}}{\Gamma(\frac{d}{2} +1)}$ being the volume of the unit ball\footnote{$\Gamma$ is Euler's gamma function.} in $\mathbb R^d$.
     %and $C_d = 2{\sf V}_d^{1/d}$
\end{proposition}

\begin{proof}
     Let $A = \{\ba_1,\ldots, \ba_n\}$ and  define the function
        \begin{align}
            g_A:[0,1]^d\to\mathbb [0,1],\qquad g_A(\bx) =
            \min\big(1,\min_{\ba\in A}\|\bx-\ba\|\big), \label{eq:condgA}
        \end{align}
    We start the proof by noticing that for any $A \subseteq \mathbb{R}^d$ the function $g_A$ is $1$-Lipschitz. Therefore, we have $\{g_A: A \subseteq \mathbb{R}^d\} \subseteq \text{Lip}_1 \subseteq \mathcal{F}$. Therefore,
    \begin{align}
        {\sf d}_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{\ba_i}\Big) &\geq \sup_A \bigg(\int_{[0,1]^d} g_A(\bx)\,d\bx - \sum_{i=1}^n w_i g_A(\ba_i) \bigg)= \sup_A \int_{[0,1]^d} g_A(\bx)\,d\bx.
    \end{align}
    We denote by $B(\bx_0, r)$ the ball in $\mathbb{R}^d$ with center $\bx_0$ and radius $r$, $B(\bx_0, r) = \{ \bx \in \mathbb{R}^d : \| \bx - \bx_0 \| \le r\}$. Let us define $A_i = B(\ba_i,r)$, for $i=1,\ldots,n$.
    Using this notation we arrive at
    \begin{align}
        \int_{[0,1]^d} g_A(\bx)\,d\bx
        &\geq
        \int_{(\cup_{i=1}^n A_i)^c\cap [0,1]^d} \min\big(1,\,\min_{\by\in A}\|\bx-\by\|\big)\,d\bx\\
        &\ge
        \int_{(\cup_{i=1}^n A_i)^c\cap [0,1]^d} \min\big(1,r \big)\,d\bx.
        %\geq (1/2)\min(1,\varepsilon_n).
        \label{eq:minepsn}
    \end{align}

We now state an auxiliary lemma, the proof of which is deferred to the end of the section.
\begin{lemma}\label{lem:2}
Let $S = [0,1]^d$ with $d \in \mathbb{N}$ and let $\mu$ be a probability
measure on $[0,1]^d$ admitting a density (with respect to the Lebesgue measure) bounded by some constant $b<\infty$. For any $r>0$ and $k\in \mathbb N$, if $B_1,\ldots,B_k \subseteq \mathbb R^d$ are balls of radius $r$ such that
\begin{gather}
    \mu\big(B_1 \cup \ldots \cup B_k\big) > {1}/{2}
\end{gather}
then,
\begin{align}
    r^d > \frac{1}{2bk \,{\sf V}_d} \cdot r^{-d},\quad \text{ where }\quad {\sf V}_d = {\rm Vol}(B(\mathbf 0,1)).\label{ineq:L2:1}
\end{align}
\end{lemma}

Let us choose $r = (2n{\sf V_d})^{-1/d}$. This value of $r$
does not satisfy (\ref{ineq:L2:1}). In view of \Cref{lem:2},
this implies that
\begin{gather}
    \mu\big(A_1 \cup \ldots \cup A_n\big)
    \leqslant {1}/{2}.
\end{gather}
Since $\mu = \mathcal U_d$ is a probability measure on $[0,1]^d$,
we get
\begin{gather}
    \mu\big((A_1 \cup \ldots \cup A_n)^c\cap [0,1]^d\big)
    > {1}/{2}.
\end{gather}
Combining this with (\ref{eq:minepsn}), we arrive at
\begin{align}
        {\sf d}_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{a_i}\Big)
        \geq (1/2)\min(1,(2{\sf V}_dn)^{-1/d} ).
\end{align}
In other terms, if $n\geq (2{\sf V}_d)^{-1}$, then
\begin{align}
        {\sf d}_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{\ba_i}\Big)
        \geq \frac12 (2{\sf V}_d n)^{-1/d} = \frac{1}{2^{(d+1)/d}} ({\sf V}_d n)^{-1/d}  %= \frac{1}{2 C_d n^{1/d}}, \text{ with } C_d = 2{\sf V}_d^{1/d},
\end{align}
otherwise
\begin{align}
        {\sf d}_\mathcal{F}\Big(\mathcal U_d, \sum_{i=1}^n w_i\delta_{\ba_i}\Big)
        \geq \frac12.
\end{align}
Putting together the inequalities from the last two displays concludes the proof of the desired lower bound.
\end{proof}

\begin{proof}[Proof of \Cref{lem:2}]
Let $\nu$ be the Lebesgue measure on $[0,1]^d$.
We know that
    $\mu(A) = \int_A \varphi(x)\, \nu(dx)$ with a probability density function $\varphi$ satisfying $0\leq \varphi(x)\leq b$ for all $x\in [0,1]^d$. Therefore,
\begin{align}
    \frac{1}{2} < \mu(B_1 \cup \ldots \cup B_k) & \leq \sum\limits_{j=1}^k \mu(B_j)
     = \sum\limits_{j=1}^k \int_{B_j} \varphi(x)\,\nu(dx)
     \leq \sum\limits_{j=1}^k b \cdot \nu(B_j) \label{eq:2.1}
\end{align}
Moreover, we know that $\nu(B_j) = {\sf V}_d r^d$ for all $j = 1, \dots k$. Combining this inequality with (\ref{eq:2.1}), we get
\begin{align}
    \frac{1}{2} <  k b \, {\sf V}_d \, r^d.
\end{align}
This yields $r^d > \frac{1}{2 bk \, {\sf V}_d}$.
\end{proof}
%\fi


\subsection{Proof of \Cref{thm:lower-bound-hard}}

    Recall that
    \begin{align}
        {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, P_{n,X}) = \sup_{f \in \mathcal{F}} \bigg| \int_{[0,1]^d} f({g}(\bu))\,\rmd\bu - \frac{1}{n} \sum\limits_{i=1}^n f(\bX_i) \bigg|.
    \end{align}
    In addition, there exists an $L_{\mathcal H}$-Lipschitz-continuous function $h$ such that $ h\circ g = \Id_d$ and
    \begin{align}
        \| h(\bx)- h(\bx')\|\leqslant L_{\mathcal H}\|\bx- \bx'\|
        \qquad \forall \bx,\bx'\in\mathbb R^D.
    \end{align}
    In order to establish the lower bound on ${\sf d}_{\mathcal{F}}({g} \sharp
    \mathcal{U}_d, P_{n,X})$, we use the fact that $\mathcal F$
    contains all the functions of the form $\psi \circ {h}$, where
    $\psi:[0,1]^d \to \mathbb{R}$ is any $(1/L_{\mathcal H})$-Lipschitz-continuous function.
    Indeed, since $h$ is $L_{\mathcal H}$-Lipschitz, the function $\psi\circ h$
    belongs to $\text{Lip}_1\subseteq \mathcal F$. Therefore,
    \begin{align}
        {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, P_{n,X})
        &\geqslant \sup_{\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}} \bigg\{ \int_{[0,1]^d} (\psi\circ \underbrace{ h\circ {g}}_{=\Id_d})(\bu)\,\rmd\bu - \frac{1}{n} \sum\limits_{i=1}^n (\psi\circ h)(\bX_i) \bigg\}\\
        & = \sup_{\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}} \bigg\{\int_{[0,1]^d} \psi (\bx)\,\rmd\bx -
        \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg\},
    \end{align}
    where we have used the notation $\bZ_i = h(\bX_i)$, for $i=1,\ldots,n$.
    Clearly, $\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}$ is equivalent to
    $L_{\mathcal H}\psi \in \text{Lip}_1$. This implies that
    \begin{align}
        {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, P_{n,X})
        &\geqslant \frac1{L_{\mathcal H}}\sup_{\psi \in \text{Lip}_1} \bigg\{\int_{[0,1]^d} \psi (\bu)\,\rmd\bu - \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg\}.
    \end{align}
    The right-hand side of the inequality above is precisely the
    $\wass_1$ distance between the uniform measure on $[0,1]^d$ and
    the empirical distribution of $\bZ_1, \dots, \bZ_n$. Therefore, we arrive at
    \begin{align}
    {\sf d}_{\mathcal{F}}({g} \sharp\, \mathcal{U}_d, P_{n,X}) \ge \frac{1}{L_{\mathcal H}} \wass_1\Big(\mathcal{U}_d, \frac{1}{n}\sum_{i=1}^n \delta_{\bZ_i}\Big) \ge \frac{1}{2L_{\mathcal H}(1 + (2{\sf V}_d n)^{1/d})},
\end{align}
where the last inequality follows from \Cref{prop:lower_bound}.



\subsection{Proof of \Cref{thm:lower-bound-pen}}

    Let $\hat h_n$ be a function from $\mathcal H$
    attaining the minimum $\min_{h\in\mathcal H}
    \|h\circ\hat g_{n, \epsilon} - \Id_d\|_{\mathbb L_q}^q$.
    Since $\mathcal{F}$ contains all
    $1$-Lipschitz continuous functions, it also
    contains all the functions of the form
    $f = \psi \circ \hat{h}_n$, for $\psi \in \textup{Lip}(1/L_{\mathcal H})$. Using the
    notation $\bZ_i = \hat h_n(\bX_i)$, this
    implies that
    \begin{align}
        {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon} \sharp\, \mathcal{U}_d, P_{n,X})
        &= \sup_{f \in \mathcal{F}} \bigg| \int_{[0,1]^d} f(\widehat{g}_{n, \epsilon}(\bx))d \bx - \frac{1}{n} \sum\limits_{i=1}^n f(\bX_i) \bigg| \\
        &\geqslant \sup_{\psi \in \text{Lip}_{L_{\mathcal H}^{-1}}} \bigg\{ \int_{[0,1]^d} (\psi\circ  \hat{h}_n\circ \widehat{g}_{n, \epsilon})(\bx)\,d\bx - \frac{1}{n} \sum\limits_{i=1}^n (\psi\circ \hat{h}_n)(\bX_i) \bigg\}\\
        &\geqslant \frac1{L_{\mathcal H}}
        \sup_{\psi \in \text{Lip}_1} \bigg\{ \int_{[0,1]^d} (\psi\circ \underbrace{ \hat{h}_n\circ \widehat{g}_{n, \epsilon}, }_{\approx\Id_d})(\bu)\,d\bu - \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg\}.
        \label{eq:5}
    \end{align}
    By adding and subtracting the term $\int_{
    [0,1]^d} \psi(\bu)\,d\bu$, we arrive at
    \begin{align}
        {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon} \sharp\, \mathcal{U}_d, P_{n,X})
        & \ge \frac1{L_{\mathcal{H}}}
        \sup_{\psi \in \text{Lip}_1}
        \bigg|\int_{[0,1]^d} \psi (\bu)\,d\bu - \frac{1}{n} \sum\limits_{i=1}^n \psi(\bZ_i) \bigg|
        - \frac1{L_{\mathcal{H}}}  \sup_{\psi \in \text{Lip}_1}
        \big\|\psi (\hat{h}_n \circ \widehat{g}_{n, \epsilon}
        ) - \psi\big\|_{\mathbb L_1}\\
        &\geqslant \frac1{L_{\mathcal{H}}} \Big(
        \wass_1(\mathcal{U}_d,\hat P_{n,Z})
        -
        \big\|\hat{h}_n \circ \widehat{g}_{n, \epsilon} -
        \Id_d\big\|_{\mathbb L_1}\Big)\\
        &\geq \frac1{L_{\mathcal{H}}} \Big(
        \wass_1(\mathcal{U}_d,\hat P_{n,Z})
        -   \textup{pen}_{\mathcal H}(\hat g_{n, \epsilon})^{1/q}\Big).
        \label{eq:decomposition1}
    \end{align}
    Here, the second inequality follows from the
    Lipschitz-continuity of $\psi$, whereas the
    last inequality is a consequence of the facts that
    the $\mathbb L_1$ norm on $[0,1]^d$ is dominated
    by the $\mathbb L_q$ norm given $q\ge 1$, and  that $\hat h_n$
    is a minimizer of $\|h\circ \hat g_n - \Id_d\|_q$
    over $\mathcal H$.
    We use the same lower bound here
    \begin{align}\label{eq:lower-bound-term1}
        \wass_1(\mathcal{U}_d,\hat P_{n,Z})
        \ge \frac{1}{2+2(2{\sf V}_dn)^{1/d}}.
    \end{align}
    To complete the proof we need to find an upper bound on the second term of the right-hand side of (\ref{eq:decomposition1}).
    Given (\ref{LIPERM}), we know that for any
    $g \in \mathcal{G}$ it holds
    \begin{align}
        {\sf d}_{\mathcal{F}}(\widehat{g}_{n, \epsilon}\sharp\, \mathcal{U}_d, P_{n,X}) + \lambda\, \textup{pen}_{\mathcal H}( \widehat{g}_{n, \epsilon}) &\le {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P_{n,X}) + \lambda\, \textup{pen}_{\mathcal H}(g) + \epsilon.
    \end{align}
    Since the distance ${\sf d}_{\mathcal F}$ is
    always nonnegative, by choosing $g$ from
    $\mathcal G_0$ the last term of the last
    display vanishes and we get
    \begin{align}\label{eq:penalty-bound}
        \lambda\,\textup{pen}_{\mathcal H}(\widehat{g}_{n, \epsilon})
        \leq  \inf_{g \in \mathcal{G}_0}
        {\sf d}_{\mathcal{F}}(g\sharp\, \mathcal{U}_d, P_{n,X}) + \epsilon.
    \end{align}
    Combining inequalities (\ref{eq:decomposition1}),
    (\ref{eq:lower-bound-term1}) and
    (\ref{eq:penalty-bound})
    we obtain the claim of the theorem.


\subsection{Proof of inequality (\ref{eq:6})}
\label{AppB:4}

Using the fact that $g^*\in \mathcal G_{\mathcal H}$ and
the triangle inequality, we get
\begin{align}
    \inf_{\mathcal{G}_{\mathcal H}} \mathbb E\big[
\wass_1(g\sharp\, \mathcal{U}_d, P_{n,X})\big]
&\leq \mathbb E\big[
\wass_1(g^*\sharp\, \mathcal{U}_d, P_{n,X})
\big]\\
&\leq \mathbb E\big[\wass_1(g^*\sharp\, \mathcal{U}_d, g^*\sharp P_{n,U})\big] + \mathbb E\big[\wass_1(g^*\sharp P_{n,U}, P_{n,X}) \big].\label{eq:6:1}
\end{align}
Let $\bU_i\sim \mathcal U_d$ be $n$ iid random vectors
drawn from the uniform distribution (they will be defined
more specifically later in the proof). Let $ P_{n,U}$ be
the empirical distribution of $\bU_1,\ldots,\bU_n$. Recall
that   (see, for example, \citep[Proposition 1]{weed2022estimation})
\begin{align}\label{wass1b}
    \mathbb E[\wass_1(\mathcal U_d,P_{n,U})] \leqslant \frac{c\sqrt{d}}{n^{1/d}}.
\end{align}
This implies that
\begin{align}
    \mathbb E\big[\wass_1(g^*\sharp\, \mathcal{U}_d, g^*\sharp P_{n,U})\big] & \leqslant L^*
    \mathbb E\big[\wass_1(\mathcal{U}_d,  P_{n,U})\big]
    \leqslant \frac{cL^*\sqrt{d}}{n^{1/d}}. \label{eq:6:2}
\end{align}
On the other hand, it is clear that
\begin{align}
    \mathbb E[\wass_1 ( g^*\sharp P_{n,U}, P_{n,X})] &\leq \mathbb E\bigg[(1/n)\sum_{i=1}^n \|g^*(\bU_i)  -\bX_i\|
    \bigg]\\
    & =  \mathbb E\big[\|g^*(\bU_1)  -\bX_1\|\big].\label{eq:6:3}
\end{align}
If we assume now that $\bU_1$ is chosen in such a way
that the joint distribution of $g^*(\bU_1)$ and $\bX_1$
is the optimal coupling between the marginal distributions
of these two random vectors, we get
\begin{align}
    \mathbb E\big[\|g^*(\bU_1)  -\bX_1\|\big] =
    \wass_1(g^*\sharp \mathcal U_d,P^*) \leqslant \sigma^*.
    \label{eq:6:4}
\end{align}
Combining (\ref{eq:6:3}) and (\ref{eq:6:4}), we get
\begin{align}
    \mathbb E[\wass_1 ( g^*\sharp P_{n,U}, P_{n,X})] &\leq
    \sigma^*.\label{eq:6:5}
\end{align}
Finally, inequalities (\ref{eq:6:2}) and (\ref{eq:6:5}),
in conjunction with (\ref{eq:6:1}), yield
\begin{align}
    \inf_{\mathcal{G}_{\mathcal H}} \mathbb E\big[
\wass_1(g\sharp\, \mathcal{U}_d, P_{n,X})\big]
&\leq \frac{cL^*\sqrt{d}}{n^{1/d}} + \sigma^*.
\label{eq}
\end{align}


\section{Proof of the risk bound when trained with approximated functional classes }

Although \Cref{prop:3} was stated for the case $q=2$
only, we provide the proof for any $q\geqslant 1$.
Replacing $q$ by $2$ in the final expression of
this proof leads to the claim of the proposition.

\subsection{Proof of \Cref{prop:3}}
We consider a trained generator $\widehat{g}_{n,\varepsilon}^0$ satisfying
\begin{align}\label{ineq:P2:1}
    {\sf d}_{\mathcal{F}_0} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X}) &+ \textup{pen}_{\mathcal{H}_0} (\widehat{g}_{n,\varepsilon}^0)  \leq {\sf d}_{\mathcal{F}_0} (g \sharp\, U_d, P_{n,X}) + \textup{pen}_{\mathcal{H}_0} (g)
    + \varepsilon,\qquad
    \text{for all } g \in \mathcal{G}_0
\end{align}
and our goal is to upper bound the
expression ${\sf d}_{\mathcal{F}} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X})$.
Using \Cref{lemma:1}, we have
\begin{align}
    {\sf d}_{\mathcal{F}} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X})
    &\leqslant  {\sf d}_{\mathcal{F}_0} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X}) + 2\delta_{\mathcal F}. \label{ineq:P2:2}
\end{align}
The first term of the
right-hand side can be upper bounded
as follows:
\begin{align}
    {\sf d}_{\mathcal{F}_0} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X})
    &\leq {\sf d}_{\mathcal{F}_0} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H_0}(\widehat{g}^0_{n,\varepsilon}) \\
    &\leq \inf_{g_0\in\mathcal G_0}\Big({\sf d}_{\mathcal{F}_0} (g_0\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H_0}(g_0)\Big) + \varepsilon\\
    &\leq \inf_{g_0\in\mathcal G_0}\Big({\sf d}_{\mathcal{F}_0} (g_0\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H}(g_0)\Big) + \varepsilon + q d^{(q-1)/2}\delta_{\mathcal H}\\
    &\leq \inf_{g_0\in\mathcal G_0}\Big({\sf d}_{\mathcal{F}} (g_0\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H}(g_0)\Big) + \varepsilon + q d^{(q-1)/2}\delta_{\mathcal H}.\label{ineq:P2:3}
\end{align}
where we used the positiveness of the penalty function
for the first inequality, inequality (\ref{ineq:P2:1})
for the second inequality, \Cref{lemma:2} for the
third inequality and the fact that $\mathcal F_0\subseteq \mathcal F$ for the fourth inequality.

The last step is to use \Cref{lemma:3}, which allows to
upper bound the inf over $\mathcal G_0$ by the inf over
$\mathcal G$, modulo an additive error term proportional
to $\delta_{\mathcal G}$. More precisely,
\begin{align}
    \inf_{g_0\in\mathcal G_0}\Big({\sf d}_{\mathcal{F}} (g_0\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H}(g_0)\Big)
    &\leqslant
    \inf_{g\in\mathcal G}\Big({\sf d}_{\mathcal{F}} (g\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H}(g)\Big)  + \big(
    1+ \lambda q d^{(q-1)/2} L_{\mathcal H}\big)\delta_{\mathcal G}.
    \label{ineq:P2:4}
\end{align}
Combining (\ref{ineq:P2:2}), (\ref{ineq:P2:3}) and
(\ref{ineq:P2:4}), we get the inequality
\begin{align}
    {\sf d}_{\mathcal{F}} (\widehat{g}^0_{n,\varepsilon}\sharp\, U_d, P_{n,X})
    \leqslant \inf_{g\in\mathcal G}\Big({\sf d}_{\mathcal{F}} (g\sharp\, U_d, P_{n,X}) + \lambda \textup{pen}_{\mathcal H}(g)\Big)  + \eps + \underbrace{2\delta_{\mathcal F} +  q d^{(q-1)/2}\delta_{\mathcal H}+\big(
    1+ \lambda q d^{(q-1)/2} L_{\mathcal H}\big)\delta_{\mathcal G}}_{=:\delta}.
\end{align}
This completes the proof.




\subsection{Impact of approximating $\mathcal F$ on the IPM}

\begin{lemma}\label{lemma:1} If $\mathcal{F}_0$ is such that  $\inf\limits_{f_0\in\mathcal{F}_0}  \| f - f_0\| \leq \delta$ for every $f\in\mathcal{F}$, then
\begin{align}
    {\sf d}_\mathcal{F} (P,Q) - {\sf d}_{\mathcal{F}_0} (P,Q) \leq 2\delta \quad \textup{for all distributions} \quad P, Q.
\end{align}
\end{lemma}

\begin{proof} Recall the definition of ${\sf d}_\mathcal{F} (P, Q) = \sup_{f\in\mathcal{F}} \big|\mathbb{E}_P[f(X)]-\mathbb{E}_Q[f(X)]\big|$. This implies that
\begin{align}
    {\sf d}_{\mathcal{F}} (P,Q) - {\sf d}_{\mathcal{F}_0} (P,Q)  &
    =  \sup_{f\in \mathcal{F}} \inf_{f_0 \in \mathcal{F}_0} \Big(\underbrace{\big| \mathbb{E}_P[f(X)] - \mathbb{E}_Q[f(X)] \big|}_{\text{independent of $f_0$}} - \underbrace{\big| \mathbb{E}_P[f_0(X)] - \mathbb{E}_Q[f_0(X)] \big|}_{\text{independent of $f$}}
    \Big)\\
    &\leq \sup_{f\in \mathcal{F}} \inf_{f_0 \in \mathcal{F}_0} \big| \mathbb{E}_P[f(X)] - \mathbb{E}_Q[f(X)] - \mathbb{E}_P[f_0(X)] - \mathbb{E}_Q[f_0(X)] \big|\quad{\color{gray}(|a|-|b|\leqslant |a-b|)}\\
    &\leq \sup_{f\in\mathcal F}\inf_{f_0 \in \mathcal{F}_0} \Big(\big| \mathbb{E}_P[f(X)] - \mathbb{E}_P[f_0(X)]\big| + \big|\mathbb{E}_Q[f(X)]  - \mathbb{E}_Q[f_0(X)] \big|\Big) \qquad {\color{gray} (\text{triangle ineq.})}\\
    &\leq \sup_{f\in\mathcal F}\inf_{f_0 \in \mathcal{F}_0} \Big( \mathbb{E}_P[\underbrace{|f(X) - f_0(X)|}_{\leqslant \|f-f_0\|_\infty}] + \mathbb{E}_Q[\underbrace{|f(X) - f_0(X)|}_{\leqslant \|f-f_0\|_\infty}] \Big)\\
    &\leq \sup_{f\in\mathcal F}\inf_{f_0 \in \mathcal{F}_0}
    \|f-f_0\|_\infty \leq 2\delta.
\end{align}
This completes the proof of the lemma.
\end{proof}

\subsection{Impact of approximating $\mathcal H$ on the Left-Inverse-Penalty}

Before analyzing the sensitivity of the left-inverse-penalty to the deviations from $\mathcal H$, we need
an auxiliary lemma.
\begin{lemma}\label{prop:aq_bq}
If $a,b$ are arbitrary numbers from some interval $[0,C]$,
and $q\geqslant 1$, then
\begin{align}
    |a^q - b^q| \leq  qC^{q-1}|b-a|.
\end{align}
\end{lemma}
\begin{proof}
Let us first assume that $c\in [0,1]$. For any $q\geqslant 1$,
\begin{align}
    |c^q - 1| \leqslant q|c-1|.
\end{align}
Then for $a,b\in \mathbb R$ such that $0\leqslant a\leqslant b$, we have
\begin{align}
    |a^q - b^q| = b^q\big|(a/b)^q - 1 \big| \leq
    q b^q\Big|\frac{a}{b} - 1 \Big| = qb^{q-1}|b-a|.
\end{align}
The claim of the lemma follows by upper bounding
$b$ by $C$.
\end{proof}

\begin{lemma}\label{lemma:2} If
$\mathcal H_0$ is such that $\min\limits_{h_0\in\mathcal{H}_0} \|h-h_0\|_\infty
\leq \delta$ for all $h \in \mathcal{H}$, then
\begin{align}
    \textup{pen}_{\mathcal{H}_0} (g) - \textup{pen}_{\mathcal{H}} (g)\leq qd^{(q-1)/2}\delta,\qquad
    \text{for all }g\in\mathcal G.
\end{align}
\end{lemma}

\begin{proof}
Recall that
$\textup{pen}_{\mathcal{H}} (g) = \min_{h\in \mathcal{H}} \|h\circ g - \Id_d\|_{\mathbb L_q}^q $. This yields
\begin{align}
    \textup{pen}_{\mathcal{H}_0} (g) - \textup{pen}_{\mathcal{H}} (g)
    &=  \min_{h_0\in \mathcal{H}_0} \| h_0\circ g - \Id_d \|_{\mathbb L_q}^q - \min_{h\in \mathcal{H}} \| h\circ g - \Id_d \|_{\mathbb L_q}^q  \\[5pt]
    &=  \max_{h\in \mathcal{H}} \min_{h_0\in \mathcal{H}_0} \Big(\| h_0\circ g - \Id_d \|_{\mathbb L_q}^q - \| h\circ g - \Id_d \|_{\mathbb L_q}^q \Big)
\end{align}
We apply \Cref{prop:aq_bq} with
\begin{align}
    a &=  \| h_0\circ g - \Id_d \|_{\mathbb L_q}
    =\bigg(\int_{[0,1]^d} \big\|\underbrace{h_0(g(u))}_{\in [0,1]^d} - \underbrace{u}_{\in[0,1]^d}\|^q\,\rmd u\bigg)^{1/q}\leqslant \sqrt{d}\\
    b &=  \| h\circ g - \Id_d \|_{\mathbb L_q}\leqslant \sqrt{d}.
\end{align}
This leads to
\begin{align}
    \| h_0\circ g - \Id_d \|_{\mathbb L_q}^q
    - \| h\circ g - \Id_d \|_{\mathbb L_q}^q
    &\leqslant q d^{(q-1)/2} \Big| \| h_0\circ g - \Id_d \|_{\mathbb L_q}
    - \| h\circ g - \Id_d \|_{\mathbb L_q}\Big|
    \quad{\color{gray}(|\|a\|-\|b\||\leqslant \|a-b\|)}\\
    &\leqslant q d^{(q-1)/2} \| h_0\circ g - \Id_d - h\circ g + \Id_d \|_{\mathbb L_q}\\
    &\leqslant q d^{(q-1)/2} \| h_0\circ g  - h\circ g  \|_{\mathbb L_q}\\
    &= q d^{(q-1)/2} \| h_0  - h \|_{\infty}\leqslant q d^{(q-1)/2} \delta.
\end{align}
This completes the proof of the lemma
\end{proof}

\subsection{Impact of approximating $\mathcal G$ on
the Penalized Empirical Risk}

As in the main text, here also we assume that
the elements of the set $\mathcal H$ are all
Lipschitz-continuous with a Lipschitz constant
bounded by $L_{\mathcal H}$.

\begin{lemma}\label{lemma:3}
    Let $P$ be an arbitrary distribution and
    $\mathcal F\subseteq \textup{Lip}_1(\mathbb R^D\to\mathbb R)$.
    If $\mathcal G_0$ is such that $\min_{g_0\in \mathcal G_0} \|g-g_0\|_\infty \leqslant \delta$ for every
    $g\in\mathcal G$, then the following is true:
    \begin{align}
        \min_{g_0\in \mathcal G_0}\Big({\sf d}_{\mathcal{F}} (g_0\sharp\, U_d, P)
        + \lambda \textup{pen}_{\mathcal H}(g_0)\Big)
        \leqslant \inf_{g\in \mathcal G}
        {\sf d}_{\mathcal{F}} (g\sharp\, U_d, P)
        + \lambda \textup{pen}_{\mathcal H}(g) +
        \big( 1 + \lambda q d^{(q-1)/2} L_{\mathcal H}\big)\delta.
    \end{align}
\end{lemma}
\begin{proof}
    Let $g$ be any function from $\mathcal G$ and
    $g_0$ be the element of $\mathcal G_0$ satisfying
    $\|g-g_0\|_\infty \leqslant \delta$. On the one hand, for this function $g_0$, we have
    \begin{align}
        {\sf d}_{\mathcal{F}} (g_0\sharp\, U_d, P) -
        {\sf d}_{\mathcal{F}} (g\sharp\, U_d, P) &=
        \sup_{f\in\mathcal F}\Big| \mathbb E_{\mathcal U_d}[f(g_0(U))] - \mathbb E_P[f(X)]\Big| - \sup_{f\in\mathcal F}\Big| \mathbb E_{\mathcal U_d}[f(g(U))] - \mathbb E_P[f(X)]\Big|\\
        &\leqslant \sup_{f\in\mathcal F}\Big| \mathbb E_{\mathcal U_d}\Big[f(g_0(U)) - f(g(U))\Big]\Big| \\
        &\leqslant \sup_{f\in\mathcal F} \mathbb E_{\mathcal U_d}\Big[\Big|f(g_0(U)) - f(g(U))\Big|\Big] \\
        &\leqslant \sup_{f\in\mathcal F} \mathbb E_{\mathcal U_d}\Big[\big\|g_0(U) - g(U)\big\|\Big] \leqslant \delta.\label{L4:ineq2}
    \end{align}
    Here, the inequality of the second line follows from
    $\sup |F|- \sup |G| \leqslant \sup (|F|-|G|) \leqslant
    \sup |F-G|$, while the first inequality of the last line
    is a consequence of the assumption that the functions
    from $\mathcal F$ are all $1$-Lipschitz.

    On the other hand,
    \begin{align}
        \textup{pen}_{\mathcal H} (g_0) -
        \textup{pen}_{\mathcal H} (g) &=
        \min_{h\in\mathcal H} \|h\circ g_0 - \Id_d\|_{\mathbb L_q}^q - \min_{h\in\mathcal H} \|h\circ g - \Id_d\|_{\mathbb L_q}^q\\
        &\leq \|h^*\circ g_0 - \Id_d\|_{\mathbb L_q}^q -  \|h^*\circ g - \Id_d\|_{\mathbb L_q}^q,
    \end{align}
    where $h^*$ is the minimizer of $\|h\circ g -
    \Id_d\|_{\mathbb L_q}^q$ over $\mathcal H$. Combining with \Cref{prop:aq_bq}, we get
    \begin{align}
        \textup{pen}_{\mathcal H} (g_0) -
        \textup{pen}_{\mathcal H} (g) &\leqslant
        q d^{(q-1)/2} \Big|\|h^*\circ g_0 - \Id_d\|_{\mathbb L_q} - \|h^*\circ g - \Id_d\|_{\mathbb L_q}\Big|\\
        &\leqslant q d^{(q-1)/2} \|h^*\circ g_0 - \Id_d - h^*\circ g + \Id_d\|_{\mathbb L_q}\\
        &= q d^{(q-1)/2} \|h^*\circ g_0 - h^*\circ g \|_{\mathbb L_q}\\
        &\leqslant q d^{(q-1)/2} \|h^*\circ g_0 - h^*\circ g \|_\infty\\
        &\leqslant q d^{(q-1)/2} L_{\mathcal H}
        \| g_0 - g \|_\infty\\
        &\leqslant q d^{(q-1)/2} L_{\mathcal H} \delta.\label{L4:ineq3}
    \end{align}
    Combining (\ref{L4:ineq2}) and (\ref{L4:ineq3}), we get
    that for any $g\in\mathcal G$, there is a $g_0\in\mathcal G_0$ such that
    \begin{align}
        {\sf d}_{\mathcal{F}} (g_0\sharp\, U_d, P)
        + \lambda \textup{pen}_{\mathcal H}(g_0)
        \leqslant
        {\sf d}_{\mathcal{F}} (g\sharp\, U_d, P)
        + \lambda \textup{pen}_{\mathcal H}(g) +
        \big( 1 + \lambda q d^{(q-1)/2} L_{\mathcal H}\big)\delta.
    \end{align}
    This completes the proof of the lemma.
\end{proof}



\section{Numerical experiments}

This section contains some  tables and plots that we could not
include in the main paper due to the space restrictions. Recall
that our experiments were done on three data sets: Swiss roll, MNIST and CIFAR-10. In these experiments, we trained a distribution using the Wasserstein GAN with a left inverse penalty.


% Figure environment removed


% Figure environment removed



\begin{table}[h!]
    \begin{minipage}{0.65\textwidth}
        \centering
        \newcommand{\inputnum}{2}
        % Hidden layer neurons' number
        \newcommand{\hiddennum}{5}
        % Output layer neurons' number
        \newcommand{\outputnum}{2}
        \begin{tcolorbox}
        \hspace*{-15pt}
        \begin{tikzpicture}[scale=0.9,font=\footnotesize]
        % Input Layer
        \foreach \i in {1,...,\inputnum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=orange!30] (Input-\i) at (0,-\i) {\tiny $U^\i$};
        }

        % Hidden Layer 1
        \foreach \i in {1,...,\hiddennum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennum-\inputnum)*5 mm
            ] (Hidden1-\i) at (2.5,-\i) {\tiny ReLU};
        }

        % Hidden Layer 2
        \foreach \i in {1,...,\hiddennum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennum-\inputnum)*5 mm
            ] (Hidden2-\i) at (5,-\i) {\tiny ReLU};
        }

        % Hidden Layer 3
        \foreach \i in {1,...,\hiddennum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennum-\inputnum)*5 mm
            ] (Hidden3-\i) at (7.5,-\i) {\tiny ReLU};
        }

        % Output Layer
        \foreach \i in {1,...,\outputnum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=purple!50,
                yshift=(\outputnum-\inputnum)*5 mm
            ] (Output-\i) at (10,-\i) {\tiny$X^\i$};
        }

        % Connect neurons In-Hidden1
        \foreach \i in {1,...,\inputnum}
        {
            \foreach \j in {1,...,\hiddennum}
            {
                \draw[->, shorten >=1pt] (Input-\i) -- (Hidden1-\j);
            }
        }

        % Connect neurons Hidden1-Hidden2
        \foreach \i in {1,...,\hiddennum}
        {
            \foreach \j in {1,...,\hiddennum}
            {
                \draw[->, shorten >=1pt] (Hidden1-\i) -- (Hidden2-\j);
            }
        }

        % Connect neurons Hidden2-Hidden3
        \foreach \i in {1,...,\hiddennum}
        {
            \foreach \j in {1,...,\hiddennum}
            {
                \draw[->, shorten >=1pt] (Hidden2-\i) -- (Hidden3-\j);
            }
        }

        % Connect neurons Hidden3-Out
        \foreach \i in {1,...,\hiddennum}
        {
            \foreach \j in {1,...,\outputnum}
            {
                \draw[->, shorten >=1pt] (Hidden3-\i) -- (Output-\j);
            }
        }

        % Bottom Row with Text
        \node [below=1cm of Input-2, align=center] {
        Input layer \\ dim = 2};
        \node [below=of Hidden1-4, align=center] {1st hid.\ layer \\ dim = 512};
        \node [below=of Hidden2-4, align=center] {2nd hid.\ layer\\ dim = 512};
        \node [below=of Hidden3-4, align=center] {3rd hid.\ layer\\ dim = 512};
        \node [below=1cm of Output-2, align=center] {Output layer\\ dim = 2};

        \node [below=2cm of Hidden2-4, align=center] {\sc\color{blue}\large The generator network};
        \end{tikzpicture}
        \end{tcolorbox}
    \end{minipage}
    \hspace*{10pt}
    \begin{minipage}{0.35\textwidth}
    \begin{tabular}{c|c}
        \toprule
        \textbf{Layer} & \textbf{Operation} \\
        \midrule
        \multirow{ 2}{*}{1} & Linear + ReLU \\
         & $\text{latent\_DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{2} & Linear + ReLU \\
        & $\text{DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{3} & Linear + ReLU \\
        & $\text{DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{Out} & Linear \\
         & $\text{DIM} \rightarrow \text{out\_DIM}$\\
        \bottomrule
    \end{tabular}
    \end{minipage}%

    \bigskip
%    \caption{Neural network architecture for the generator for Swiss Roll dataset. \text{latent\_DIM} = 2, DIM = 512.}
%   \label{tab:generator_toy}
    \begin{minipage}{0.65\textwidth}
        \centering
        \newcommand{\inputnum}{2}
        % Hidden layer neurons' number
        \newcommand{\hiddennum}{5}
        % Output layer neurons' number
        \newcommand{\outputnum}{1}
        \begin{tcolorbox}
        \hspace*{-15pt}
        \begin{tikzpicture}[scale=0.9,font=\footnotesize]
        % Input Layer
        \foreach \i in {1,...,\inputnum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=orange!30] (Input-\i) at (0,-\i) {\tiny $X^\i$};
        }

        % Hidden Layer 1
        \foreach \i in {1,...,\hiddennum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennum-\inputnum)*5 mm
            ] (Hidden1-\i) at (2.5,-\i) {\tiny ReLU};
        }

        % Hidden Layer 2
        \foreach \i in {1,...,\hiddennum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennum-\inputnum)*5 mm
            ] (Hidden2-\i) at (5,-\i) {\tiny ReLU};
        }

        % Hidden Layer 3
        \foreach \i in {1,...,\hiddennum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennum-\inputnum)*5 mm
            ] (Hidden3-\i) at (7.5,-\i) {\tiny ReLU};
        }

        % Output Layer
        \foreach \i in {1,...,\outputnum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=purple!50,
                yshift=(\outputnum-\inputnum)*5 mm
            ] (Output-\i) at (10,-\i) {\tiny$Y$};
        }

        % Connect neurons In-Hidden1
        \foreach \i in {1,...,\inputnum}
        {
            \foreach \j in {1,...,\hiddennum}
            {
                \draw[->, shorten >=1pt] (Input-\i) -- (Hidden1-\j);
            }
        }

        % Connect neurons Hidden1-Hidden2
        \foreach \i in {1,...,\hiddennum}
        {
            \foreach \j in {1,...,\hiddennum}
            {
                \draw[->, shorten >=1pt] (Hidden1-\i) -- (Hidden2-\j);
            }
        }

        % Connect neurons Hidden2-Hidden3
        \foreach \i in {1,...,\hiddennum}
        {
            \foreach \j in {1,...,\hiddennum}
            {
                \draw[->, shorten >=1pt] (Hidden2-\i) -- (Hidden3-\j);
            }
        }

        % Connect neurons Hidden3-Out
        \foreach \i in {1,...,\hiddennum}
        {
            \foreach \j in {1,...,\outputnum}
            {
                \draw[->, shorten >=1pt] (Hidden3-\i) -- (Output-\j);
            }
        }

        % Bottom Row with Text
        \node [below=1cm of Input-2, align=center] {
        Input layer \\ dim = 2};
        \node [below=of Hidden1-4, align=center] {1st hid.\ layer \\ dim = 512};
        \node [below=of Hidden2-4, align=center] {2nd hid.\ layer\\ dim = 512};
        \node [below=of Hidden3-4, align=center] {3rd hid.\ layer\\ dim = 512};
        \node [below=1cm of Output-2, align=center] {Output layer\\ dim = 1};

        \node [below=2cm of Hidden2-4, align=center] {\sc\color{blue}\large The critics network};
        \end{tikzpicture}
        \end{tcolorbox}
    \end{minipage}
    \hspace*{10pt}
    \begin{minipage}{0.35\textwidth}
    \begin{tabular}{c|c}
        \toprule
        \textbf{Layer} & \textbf{Operation} \\
        \midrule
        \multirow{ 2}{*}{1} & Linear + ReLU \\
         & $\text{out\_DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{2} & Linear + ReLU \\
        & $\text{DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{3} & Linear + ReLU \\
        & $\text{DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{Out} & Linear \\
         & $\text{DIM} \rightarrow 1$\\
        \bottomrule
    \end{tabular}
    \end{minipage}%

    \bigskip

    \begin{minipage}{0.65\textwidth}
        \centering
        \newcommand{\inputnum}{2}
        % Hidden layer neurons' number
        \newcommand{\hiddennuma}{5}
        \newcommand{\hiddennumb}{5}
        % Output layer neurons' number
        \newcommand{\outputnum}{2}
        \begin{tcolorbox}
        \hspace*{-15pt}
        \begin{tikzpicture}[scale=0.9,font=\footnotesize]
        % Input Layer
        \foreach \i in {1,...,\inputnum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=orange!30] (Input-\i) at (0,-\i) {\tiny $X^\i$};
        }

        % Hidden Layer 1
        \foreach \i in {1,...,\hiddennuma}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennuma-\inputnum)*5 mm
            ] (Hidden1-\i) at (2.5,-\i) {\tiny ReLU};
        }

        % Hidden Layer 2
        \foreach \i in {1,...,\hiddennumb}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennumb-\inputnum)*5 mm
            ] (Hidden2-\i) at (5,-\i) {\tiny ReLU};
        }

        % Hidden Layer 3
        \foreach \i in {1,...,\hiddennumb}
        {
            \node[circle,
                minimum size = 6mm,
                fill=teal!50,
                yshift=(\hiddennumb-\inputnum)*5 mm
            ] (Hidden3-\i) at (7.5,-\i) {\tiny ReLU};
        }

        % Output Layer
        \foreach \i in {1,...,\outputnum}
        {
            \node[circle,
                minimum size = 6mm,
                fill=purple!50,
                yshift=(\outputnum-\inputnum)*5 mm
            ] (Output-\i) at (10,-\i) {\tiny$U^\i$};
        }

        % Connect neurons In-Hidden1
        \foreach \i in {1,...,\inputnum}
        {
            \foreach \j in {1,...,\hiddennuma}
            {
                \draw[->, shorten >=1pt] (Input-\i) -- (Hidden1-\j);
            }
        }

        % Connect neurons Hidden1-Hidden2
        \foreach \i in {1,...,\hiddennuma}
        {
            \foreach \j in {1,...,\hiddennumb}
            {
                \draw[->, shorten >=1pt] (Hidden1-\i) -- (Hidden2-\j);
            }
        }

        % Connect neurons Hidden2-Hidden3
        \foreach \i in {1,...,\hiddennumb}
        {
            \foreach \j in {1,...,\hiddennumb}
            {
                \draw[->, shorten >=1pt] (Hidden2-\i) -- (Hidden3-\j);
            }
        }

        % Connect neurons Hidden3-Out
        \foreach \i in {1,...,\hiddennumb}
        {
            \foreach \j in {1,...,\outputnum}
            {
                \draw[->, shorten >=1pt] (Hidden3-\i) -- (Output-\j);
            }
        }

        % Bottom Row with Text
        \node [below=1cm of Input-2, align=center] {
        Input layer \\ dim = 2};
        \node [below=of Hidden1-4, align=center] {1st hid.\ layer \\ dim = 512};
        \node [below=of Hidden2-4, align=center] {2nd hid.\ layer\\ dim = 512};
        \node [below=of Hidden3-4, align=center] {3rd hid.\ layer\\ dim = 512};
        \node [below=1cm of Output-2, align=center] {Output layer\\ dim = 2};

        \node [below=2.5cm of Hidden2-4, align=center] {\sc\color{blue}\large The left inverse network};
        \end{tikzpicture}
    \end{tcolorbox}
    \end{minipage}
    \hspace{10pt}
    \begin{minipage}{0.35\textwidth}
    \begin{tabular}{c|c}
        \toprule
        \textbf{Layer} & \textbf{Operation} \\
        \midrule
        \multirow{ 2}{*}{1} & Linear + LeakyReLU \\
         & $\text{out\_DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{2} & Linear + LeakyReLU \\
        & $\text{DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{3} & Linear + LeakyReLU \\
        & $\text{DIM} \rightarrow \text{DIM}$ \\
        \midrule
        \multirow{ 2}{*}{Out} & Linear \\
         & $\text{DIM} \rightarrow \text{latent\_DIM}$\\
        \bottomrule
    \end{tabular}
    \end{minipage}%

    \caption{Neural network architectures for the generator $g$, the critic $f$ and the left inverse $h$ used in the experiments conducted on Swiss Roll datasets. In this case, \text{latent\_DIM} = \text{out\_DIM} = 2, DIM = 512.}
    \label{tab:discriminator_toy}
\end{table}








% Figure environment removed

\begin{table}[htbp]
  \centering
  \begin{subtable}[b]{\textwidth}
    \centering
      \caption{Discriminator Network Architecture used in our experiments on MNIST data set. }
\vspace{10pt}
 \begin{tabular}{l|c|c|c|c}
    \toprule

      & Conv2d +  &  Conv2d +  & Conv2d +  &\\
     \bf Layer & LeakyReLu &  BatchNorm2d +  & BatchNorm2d +  & Conv2d\\
      & &  LeakyRelu & LeakyRelu & \\
     \midrule
     Input dim         & $28\times 28$ & $14\times 14$ & $7\times 7$ & $3\times 3$\\
     Nb input channels & 1 &  256 & 512 & 1024\\
     Kernel size       & 4 &  4   & 3 & 3\\
     Stride            & 2 &  2   & 2 & 1\\
     Padding           & 1 &  1   & 0 & 0\\
     Output dim        & $14\times 14$ &  $7\times 7$ & $3\times 3$ & $1\times 1$\\
     Nb output channels& 256 &  512 & 1024 & 1\\
     %\midrule
     % \rowcolor{BlanchedAlmond}
     % Negative slope & 0.2 & 0.2 & 0.2 & -\\
     \bottomrule
 \end{tabular}
\vspace{25pt}
  \end{subtable}
  \begin{subtable}[b]{\textwidth}
  \centering
  \caption{Generator Network Architecture used in our experiments on MNIST data set. }
\vspace{10pt}
 \begin{tabular}{l|c|c|c|c}
    \toprule

      & ConvTranspose2d   &  ConvTranspose2d  & ConvTranspose2d  &\\
     \bf Layer & +BatchNorm2d &  +BatchNorm2d  & +BatchNorm2d  & ConvTranspose2d\\
      & +ReLu &  +Relu & +Relu & \\
     \midrule
     Input dim         & $1\times 1$ & $3\times 3$ & $7\times 7$ & $3\times 3$\\
     Nb input channels & 100 &  1024 & 512 & 256\\
     Kernel size       & 3 &  3   & 4 & 4\\
     Stride            & 1 &  2   & 2 & 2\\
     Padding           & 0 &  0   & 1 & 1\\
     Output dim        & $3\times 3$ &  $7\times 7$ & $14\times 14$ & $28\times 28$\\
     Nb output channels& 1024 &  512 & 256 & 1\\
     %\midrule
     \bottomrule
 \end{tabular}
\vspace{25pt}
 \end{subtable}
 \begin{subtable}[b]{\textwidth}
      \centering
  \caption{Inverse Generator Network Architecture used in our experiments on MNIST data set. }
\vspace{10pt}
 \begin{tabular}{l|c|c|c|c|c}
    \toprule

      & Conv2d +  &  Conv2d +  & Conv2d +  & &\\
     \bf Layer & LeakyReLu &  BatchNorm2d +  & BatchNorm2d +  & Conv2d & Linear\\
      & &  LeakyRelu & LeakyRelu & &\\
     \midrule
     Input dim         & $28\times 28$ & $14\times 14$ & $7\times 7$ & $3\times 3$ & 100\\
     Nb input channels & 1 &  256 & 512 & 1024 & 1\\
     Kernel size       & 4 &  4   & 3 & 3 & -\\
     Stride            & 2 &  2   & 2 & 1 & -\\
     Padding           & 1 &  1   & 0 & 0 & -\\
     Output dim        & $14\times 14$ &  $7\times 7$ & $3\times 3$ & $1\times 1$ & 100\\
     Nb output channels& 256 &  512 & 1024 & 100 & 1\\
     %\midrule
     % \rowcolor{BlanchedAlmond}
     % Negative slope & 0.2 & 0.2 & 0.2 & - & -\\
     \bottomrule
 \end{tabular}
 \end{subtable}
 \vspace{10pt}
\caption{Neural network architectures for the generator $g$, the critic $f$, and the
 lest inverse $h$ used in the experiments conducted on MNIST dataset. The negative slope parameter of the LeakyReLU is set to $0.2$.}
 \label{table:MNIST_architecture}
\end{table}


% \end{document}

% % Figure environment removed

% % Figure environment removed

