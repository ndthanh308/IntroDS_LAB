% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{rnn1}
A.~Graves, ``Sequence transduction with recurrent neural networks,''
  \emph{arXiv preprint arXiv:1211.3711}, 2012.

\bibitem{rnn2}
T.~N. Sainath, Y.~He, B.~Li, A.~Narayanan, R.~Pang \emph{et~al.}, ``A streaming
  on-device end-to-end model surpassing server-side conventional model quality
  and latency,'' in \emph{International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, 2020, pp. 6059--6063.

\bibitem{rnn3}
J.~Li, R.~Zhao, Z.~Meng, Y.~Liu, W.~Wei \emph{et~al.}, ``Developing {RNN-T}
  models surpassing high-performance hybrid models with customization
  capability,'' in \emph{21st Annual Conference of the International Speech
  Communication Association, {INTERSPEECH}}, 2020, pp. 3590--3594.

\bibitem{rnn4}
C.-F. Yeh, J.~Mahadeokar, K.~Kalgaonkar, Y.~Wang, D.~Le, M.~Jain, K.~Schubert,
  C.~Fuegen, and M.~L. Seltzer, ``Transformer-transducer: End-to-end speech
  recognition with self-attention,'' \emph{arXiv preprint arXiv:1910.12977},
  2019.

\bibitem{DBLP:conf/interspeech/KuangGKLLYP22}
F.~Kuang, L.~Guo, W.~Kang, L.~Lin, M.~Luo \emph{et~al.}, ``Pruned {RNN-T} for
  fast, memory-efficient {ASR} training,'' in \emph{23rd Annual Conference of
  the International Speech Communication Association, {INTERSPEECH}}, 2022, pp.
  2068--2072.

\bibitem{li2019improving}
J.~Li, R.~Zhao, H.~Hu, and Y.~Gong, ``Improving rnn transducer modeling for
  end-to-end speech recognition,'' in \emph{IEEE Automatic Speech Recognition
  and Understanding Workshop, {ASRU}}, 2019, pp. 114--121.

\bibitem{wang2022accelerating}
Y.~Wang, Z.~Chen, C.~Zheng, Y.~Zhang, W.~Han \emph{et~al.}, ``Accelerating
  rnn-t training and inference using ctc guidance,'' \emph{arXiv preprint
  arXiv:2210.16481}, 2022.

\bibitem{DBLP:conf/icml/GravesFGS06}
A.~Graves, S.~Fern{\'{a}}ndez, F.~J. Gomez, and J.~Schmidhuber, ``Connectionist
  temporal classification: labelling unsegmented sequence data with recurrent
  neural networks,'' in \emph{Machine Learning, Proceedings of the Twenty-Third
  International Conference, {ICML}}, vol. 148, 2006, pp. 369--376.

\bibitem{DBLP:conf/interspeech/KimLTZS21}
J.~Kim, H.~Lu, A.~Tripathi, Q.~Zhang, and H.~Sak, ``Reducing streaming {ASR}
  model delay with self alignment,'' in \emph{22nd Annual Conference of the
  International Speech Communication Association, {INTERSPEECH}}, 2021, pp.
  3440--3444.

\bibitem{DBLP:conf/icassp/YuCLCSHNHGWP21}
J.~Yu, C.~Chiu, B.~Li, S.~Chang, T.~N. Sainath \emph{et~al.}, ``Fastemit:
  Low-latency streaming {ASR} with sequence-level emission regularization,'' in
  \emph{{IEEE} International Conference on Acoustics, Speech and Signal
  Processing, {ICASSP}}, 2021, pp. 6004--6008.

\bibitem{DBLP:conf/interspeech/SainathPRGS20}
T.~N. Sainath, R.~Pang, D.~Rybach, B.~Garc{\'{\i}}a, and T.~Strohman,
  ``Emitting word timings with end-to-end models,'' in \emph{21st Annual
  Conference of the International Speech Communication Association,
  {INTERSPEECH}}, 2020, pp. 3615--3619.

\bibitem{DBLP:conf/icassp/InagumaGLLG20}
H.~Inaguma, Y.~Gaur, L.~Lu, J.~Li, and Y.~Gong, ``Minimum latency training
  strategies for streaming sequence-to-sequence {ASR},'' in \emph{{IEEE}
  International Conference on Acoustics, Speech and Signal Processing,
  {ICASSP}}, 2020, pp. 6064--6068.

\bibitem{DBLP:conf/icassp/ShrivastavaGCZS21}
H.~Shrivastava, A.~Garg, Y.~Cao, Y.~Zhang, and T.~N. Sainath, ``Echo state
  speech recognition,'' in \emph{International Conference on Acoustics, Speech
  and Signal Processing, {ICASSP}}, 2021, pp. 5669--5673.

\bibitem{DBLP:conf/icassp/Dong020}
L.~Dong and B.~Xu, ``{CIF:} continuous integrate-and-fire for end-to-end speech
  recognition,'' in \emph{{IEEE} International Conference on Acoustics, Speech
  and Signal Processing, {ICASSP}}, 2020, pp. 6079--6083.

\bibitem{dong2020comparison}
L.~Dong, C.~Yi, J.~Wang, S.~Zhou, S.~Xu \emph{et~al.}, ``A comparison of
  label-synchronous and frame-synchronous end-to-end models for speech
  recognition,'' \emph{arXiv preprint arXiv:2005.10113}, 2020.

\bibitem{DBLP:conf/asru/YuLGLYXGHZ21}
F.~Yu, H.~Luo, P.~Guo, Y.~Liang, Z.~Yao \emph{et~al.}, ``Boundary and context
  aware training for cif-based non-autoregressive end-to-end {ASR},'' in
  \emph{Automatic Speech Recognition and Understanding Workshop, {ASRU}}, 2021,
  pp. 328--334.

\bibitem{DBLP:conf/interspeech/GaoZ0Y22}
Z.~Gao, S.~Zhang, I.~McLoughlin, and Z.~Yan, ``Paraformer: Fast and accurate
  parallel transformer for non-autoregressive end-to-end speech recognition,''
  in \emph{23rd Annual Conference of the International Speech Communication
  Association, {INTERSPEECH}}, 2022, pp. 2063--2067.

\bibitem{DBLP:conf/asru/HiguchiCFIKLNWW21}
Y.~Higuchi, N.~Chen, Y.~Fujita, H.~Inaguma, T.~Komatsu \emph{et~al.}, ``A
  comparative study on non-autoregressive modelings for speech-to-text
  generation,'' in \emph{Automatic Speech Recognition and Understanding
  Workshop, {ASRU}}, 2021, pp. 47--54.

\bibitem{funnel}
Z.~Dai, G.~Lai, Y.~Yang, and Q.~Le, ``Funnel-transformer: Filtering out
  sequential redundancy for efficient language processing,'' in \emph{Annual
  Conference on Neural Information Processing Systems, {NeurIPS}}, 2020.

\bibitem{DBLP:conf/icassp/ZhangLLSC22}
C.~Zhang, B.~Li, Z.~Lu, T.~N. Sainath, and S.~Chang, ``Improving the fusion of
  acoustic and text representations in {RNN-T},'' in \emph{International
  Conference on Acoustics, Speech and Signal Processing, {ICASSP}}, 2022, pp.
  8117--8121.

\bibitem{DBLP:conf/icassp/GravesMH13}
A.~Graves, A.~Mohamed, and G.~E. Hinton, ``Speech recognition with deep
  recurrent neural networks,'' in \emph{International Conference on Acoustics,
  Speech and Signal Processing, {ICASSP}}, 2013, pp. 6645--6649.

\bibitem{DBLP:conf/icassp/VarianiRA020}
E.~Variani, D.~Rybach, C.~Allauzen, and M.~Riley, ``Hybrid autoregressive
  transducer {(HAT)},'' in \emph{International Conference on Acoustics, Speech
  and Signal Processing, {ICASSP}}, 2020, pp. 6139--6143.

\bibitem{DBLP:conf/icassp/SaonTBK21}
G.~Saon, Z.~T{\"{u}}ske, D.~Bola{\~{n}}os, and B.~Kingsbury, ``Advancing {RNN}
  transducer technology for speech recognition,'' in \emph{International
  Conference on Acoustics, Speech and Signal Processing, {ICASSP}}, 2021, pp.
  5654--5658.

\bibitem{espnet}
S.~Watanabe, T.~Hori, S.~Karita, T.~Hayashi, J.~Nishitoba \emph{et~al.},
  ``Espnet: End-to-end speech processing toolkit,'' in \emph{19th Annual
  Conference of the International Speech Communication Association,
  {INTERSPEECH}}, 2018, pp. 2207--2211.

\bibitem{wenet}
Z.~Yao, D.~Wu, X.~Wang, B.~Zhang, F.~Yu \emph{et~al.}, ``Wenet: Production
  oriented streaming and non-streaming end-to-end speech recognition toolkit,''
  in \emph{22nd Annual Conference of the International Speech Communication
  Association, {INTERSPEECH}}, 2021, pp. 4054--4058.

\bibitem{Wenetspeech}
B.~Zhang, H.~Lv, P.~Guo, Q.~Shao, C.~Yang \emph{et~al.}, ``{WENETSPEECH:} {A}
  10000+ hours multi-domain mandarin corpus for speech recognition,'' in
  \emph{International Conference on Acoustics, Speech and Signal Processing,
  {ICASSP}}, 2022, pp. 6182--6186.

\bibitem{DBLP:conf/iscslp/YouFSY22}
Z.~You, S.~Feng, D.~Su, and D.~Yu, ``3m: Multi-loss, multi-path and multi-level
  neural networks for speech recognition,'' in \emph{13th International
  Symposium on Chinese Spoken Language Processing, {ISCSLP}}, 2022, pp.
  170--174.

\bibitem{Aishell}
H.~Bu, J.~Du, X.~Na, B.~Wu, and H.~Zheng, ``{AISHELL-1:} an open-source
  mandarin speech corpus and a speech recognition baseline,'' in \emph{20th
  Conference of the Oriental Chapter of the International Coordinating
  Committee on Speech Databases and Speech {I/O} Systems and Assessment,
  {O-COCOSDA}}, 2017, pp. 1--5.

\bibitem{sp}
T.~Ko, V.~Peddinti, D.~Povey, and S.~Khudanpur, ``Audio augmentation for speech
  recognition,'' in \emph{16th Annual Conference of the International Speech
  Communication Association, {INTERSPEECH}}, 2015, pp. 3586--3589.

\bibitem{spec}
D.~S. Park, W.~Chan, Y.~Zhang, C.~Chiu, B.~Zoph \emph{et~al.}, ``Specaugment:
  {A} simple data augmentation method for automatic speech recognition,'' in
  \emph{20th Annual Conference of the International Speech Communication
  Association, {INTERSPEECH}}, 2019, pp. 2613--2617.

\bibitem{cmvn}
O.~Viikki and K.~Laurila, ``Cepstral domain segmental feature vector
  normalization for noise robust speech recognition,'' \emph{Speech Commun.},
  vol.~25, no. 1-3, pp. 133--147, 1998.

\bibitem{DBLP:conf/interspeech/BotrosSDG0H21}
R.~Botros, T.~N. Sainath, R.~David, E.~Guzman, W.~Li, and Y.~He, ``Tied {\&}
  reduced {RNN-T} decoder,'' in \emph{Interspeech 2021, 22nd Annual Conference
  of the International Speech Communication Association, {INTERSPEECH}}, 2021,
  pp. 4563--4567.

\bibitem{DBLP:journals/corr/KingmaB14}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{DBLP:conf/icassp/YangHNACPPGGYLH22}
Y.~Yang, M.~Hira, Z.~Ni, A.~Astafurov, C.~Chen \emph{et~al.}, ``Torchaudio:
  Building blocks for audio and speech processing,'' in \emph{International
  Conference on Acoustics, Speech and Signal Processing, {ICASSP}}, 2022.

\end{thebibliography}
