\section{Related Work}

\subsection{Remote Collaboration}
\subsubsection{Mixed Reality Remote Collaboration}
Recent advances in mixed reality technologies have enabled immersive remote collaboration that was not possible with traditional desktop interfaces. 
Prior research has explored various approaches for immersive telepresence, such as holographic teleportation (e.g., \textit{Holoportation}~\cite{orts2016holoportation}, \textit{Virtual Makerspaces}~\cite{radu2021virtual}, \textit{Loki}~\cite{thoravi2019loki}), virtual avatars (e.g., \textit{CollaboVR}~\cite{he2020collabovr}, \textit{Mini-Me}~\cite{piumsomboon2018mini}, \textit{Shoulder of Giant}~\cite{piumsomboon2019shoulder}, \textit{ARTEMIS}~\cite{gasques2021artemis}), and projected video stream (e.g., \textit{Room2Room}~\cite{pejsa2016room2room}, \textit{3D-Board}~\cite{zillner20143d}).
These systems allow remote users to be spatially co-located in the same shared space, which greatly enhances collaborative experiences~\cite{bai2020user, cao2020exploratory}. 
For example, by showing virtual hands and bodies in 3D space, the local users can more easily understand the intention of the remote users for various physical tasks such as block assembly~\cite{zhang2022real}, origami~\cite{kim2019evaluating, kim2020combination}, mechanical tasks~\cite{oyama2021augmented, oyama2021integrating}, and physiotherapy education~\cite{faridan2023chameleoncontrol}.
However, current holographic telepresence lacks the physical embodiment of the remote user, which significantly reduces the sense of co-presence~\cite{lee2018physical}. This limitation also constraints rich physical affordances which we naturally employ in co-located physical collaboration~\cite{leithinger2014physical, siu2018investigating}.

\subsubsection{Robotic Telepresence}
To address this limitation, past research has explored robotic telepresence that aims to physically embody remote users by adding a robotic body to a 2D video screen (e.g., \textit{MeBot}~\cite{adalgeirsson2010mebot},  \textit{RemoteCode}~\cite{sakashita2022remotecode}) or by replicating the remote user with a humanoid or non-humanoid robot (e.g., \textit{TELESAR V}~\cite{fernando2012design}, \textit{Telenoid}~\cite{ogawa2011exploring}, \textit{You as a Puppet}~\cite{sakashita2017you}, \textit{GestureMan}~\cite{kuzuoka2000gestureman}, \textit{Geminoid}~\cite{sakamoto2007android}).
The robotic telepresence can greatly enhance user engagement by enabling physical interactions such as  gestures~\cite{adalgeirsson2010mebot} and body movement~\cite{nakanishi2011zoom, rae2014bodies, lee2011now}.
For example, mobile robots allow remote users to move freely around a table to interact with local users and objects for remote education (e.g., \textit{RobotAR}~\cite{villanueva2021robotar}, \textit{ASTEROIDS}~\cite{li2022asteroids}).
Beyond a screen-based representation, \textit{VROOM}~\cite{jones2020vroom, jones2021belonging} overlays a holographic avatar on a telepresence robot that enriches non-verbal communication such as gestures or eye-contact. 
% However, robotic telepresence embodies remote users themselves, while our focus is on embodying remote users through synchronized actuated environments, as we discuss next.

\subsubsection{Physical Telepresence}
An alternative approach to adding physical embodiment to remote users is using \textit{synchronized distributed physical objects}~\cite{brave1998tangible}, rather than embodying users themselves with robotic telepresence. 
Such an approach was originally explored through \textit{InTouch}~\cite{brave1997intouch}, \textit{ComTouch}~\cite{chang2002comtouch}, and \textit{PsyBench}~\cite{brave1998tangible}, in which synchronized tangible tokens embody the remote user's motion and behavior.
This idea has evolved into a concept of \textit{physical telepresence}~\cite{leithinger2014physical}, which synchronizes physical shape rendering with the remote users' visual appearance.
For instance, Leithinger et al.~\cite{leithinger2014physical} uses a shape-changing display~\cite{follmer2013inform} to physically render a remote user's hand and surrounding objects with screen-based visual feedback. 
Recent works have also expanded this concept by combining a virtual avatar with a motorized X-Y plotter to actuate a single token (e.g., \textit{Physical-Virtual Table}~\cite{lee2018physical}).
However, the existing approach using shape displays lacks deployability due to the dedicated hardware requirement, and X-Y plotters lack scalability and generalizability due to a single point actuation and limited interaction area.
More closely related to our work, a few researchers have explored the use of mobile robots for tangible remote collaboration in VR (e.g., \textit{PhyShare}~\cite{he2017physhare}) and mixed reality environments (e.g., \textit{Siu et al.}~\cite{siu2018investigating}).
However, this approach of using multiple mobile robots has not been fully explored yet, as these prior works do not present the comprehensive design space and have not conducted any user evaluation to understand the benefits and limitations of this approach.
Beyond these prior works, we contribute to 1) an exploration of the broader design space with a demonstration of comprehensive applications, and 2) a holistic user evaluation through condition experiments.
% Beyond these prior works, we contribute to 1) an exploration of the broader design space, 2) a demonstration of comprehensive applications, and 3) a holistic user evaluation through condition experiments.

\subsection{Bi-Directional Virtual-Physical Interaction}
Outside the context of remote collaboration, past HCI research has also explored bi-directional virtual-physical interaction by leveraging augmented reality and actuated environments~\cite{suzuki2022augmented}.
For example, systems like \textit{Kobito}~\cite{aoki2005kobito}, \textit{Augmented Coliseum}~\cite{kojima2006augmented}, and \textit{IncreTable}~\cite{leitner2008incretable} explore the synchronous coupling between AR and actuated physical objects, which can enrich visual feedback and affordances of robots and  actuated tangible interfaces.
These interfaces typically employ robot motion (e.g., \textit{exTouch}~\cite{kasahara2013extouch}), actuated tangible tokens (e.g., \textit{PICO}~\cite{patten2007mechanical}, 
\textit{Reactile}~\cite{suzuki2018reactile}, \textit{Actuated Workbench}~\cite{pangaro2002actuated}), IoT devices (e.g., \textit{MechARSpace}~\cite{zhu2022mecharspace}, \textit{WIKA}~\cite{jeong2020wika}, \textit{Kim et al.}~\cite{kim2018does}) to synchronize between virtual and physical outputs in a bi-directional manner.
Similar to our work, \textit{Sketched Reality}~\cite{kaimoto2022sketched} and \textit{Physica}~\cite{li2023physica} explores bi-directional interaction between embedded virtual objects and tabletop robots. 
Our system extends their work in the context of holographic tangible remote collaboration in mixed reality environments. 

\subsection{Actuated Tangible User Interfaces}
Actuated tangible user interfaces were originally developed to address the challenge of digital-physical discrepancies in conventional tangible interfaces~\cite{poupyrev2007actuation}.
Towards this goal, HCI researchers have explored a variety of actuated tangible user interfaces~\cite{poupyrev2007actuation} and shape-changing user interfaces~\cite{rasmussen2012shape, coelho2011shape, alexander2018grand}, using magnetic actuation~\cite{patten2007mechanical}, ultrasonic waves~\cite{marshall2012ultra}, magnetic levitation~\cite{lee2011zeron}, and wheeled and vibrating robots~\cite{nowacka2013touchbugs}.
Rosenfeld et al.~\cite{rosenfeld2004physical} introduced the concept of using physical mobile robots as an actuated tangible user interface.
% Recently, \textit{Zooids}~\cite{le2016zooids} introduces the swarm user interface, which employs a swarm of tabletop robots as actuated tangible objects. 
This concept has been expanded through various systems such as 
\textit{Zooids}~\cite{le2016zooids},
\textit{ShapeBots}~\cite{suzuki2019shapebots}, \textit{HERMITS}~\cite{nakagaki2020hermits}, \textit{Rolling Pixels}~\cite{lee2020rolling}, and \textit{(Dis) Appearables}~\cite{nakagaki2022dis}. 
Swarm user interfaces can also provide haptic sensations~\cite{kim2019swarmhaptics, suzuki2017fluxmarker, suzuki2021hapticbots, zhao2017robotic} and actuate everyday objects~\cite{kim2020user, farajian2022swarm}. 
Inspired by these works, we also leverage multiple tabletop robots for our actuated interfaces.


% % Figure environment removed
