\section{\system{}: System Design}
This section introduces \system{}, a system that augments holographic telepresence with multiple tabletop robots. As illustrated in Figure~\ref{fig:system-setup}, \system{} consists of three main components: 1) \textit{capturing a remote user} with the Azure Kinect depth camera, 2) \textit{holographic rendering and hand tracking} with Microsoft Hololens 2 headset, and 3) \textit{synchronized actuation} with Sony Toio tabletop mobile robots.

\subsubsection*{\textbf{Capturing a Remote User with a Depth Camera}}
The Azure Kinect RGB-D camera is used to capture the remote user's body. The camera is positioned in front of the remote user with a tripod stand. 
The Kinect camera is connected to the local PC (G-Tune, Intel Core i7-11800H 2.30GHz CPU, NVIDIA GeForce RTX 3060 GPU, 64GB RAM) via a USB cable.
The depth information is captured through the Azure Kinect Sensor SDK running on the local PC. The depth sensor first generates a point cloud with a resolution of 640 x 576, which is then converted into a real-time colored 3D  mesh using the Azure Kinect Examples for Unity package~\footnote{\url{https://assetstore.unity.com/packages/tools/149700}}. 
% The captured mesh data is converted to \todo{xxx} data structure, then is sent to another PC in a remote environment.
Mesh data is captured with 30 FPS and the size of each mesh data is approximately 20 MB. 



% To reduce the latency, we use a UDP communication through an Ethernet cable. 
% The Azure Kinect sensor and mesh processing software run with \todo{xxx} FPS and the latency of UDP communication is \todo{xxx} ms.


\subsubsection*{\textbf{Holographic Rendering and Hand Tracking}}
In our setup, both local and remote users wear the Microsoft Hololens 2 mixed reality headset, which has a diagonal field of view of 52 degrees. 
% The local user can see the remote user's holographic mesh captured via Azure Kinect. 
% Once the captured mesh data is received from the remote PC, the local PC (G-Tune, Intel Core i7-11800H 2.30GHz CPU, NVIDIA GeForce RTX 3070 GPU, 64GB RAM) decodes the data to 3D colored mesh in our Unity application using Azure Kinect for Unity library. 
% The processed mesh is rendered in Hololens 2 through Holographic 
The remote user's holographic mesh generated by the local PC is rendered in Hololens 2 through Holographic 
Remoting~\footnote{\url{https://learn.microsoft.com/en-us/windows/mixed-reality/develop/native/holographic-remoting-player}}, enabling high-quality and low-latency (60 FPS) rendering over an Ethernet connection, allowing the local user to view the mesh. 
Hololens 2 is also used to track the user's hand movements using the MRTK hand-tracking library. 
Tracked hands are used to 1) grab virtual robots to manipulate and synchronize the physical one in the remote environment, or 2) move robots based on the finger position to physicalize the virtual hand. 
These processes are executed on Unity running on the remote PC (G-Tune, Intel Core i7-11800H 2.30GHz CPU, NVIDIA GeForce RTX 3070 GPU, 64GB RAM) and the local PC, respectively, connected with Hololens 2 through Holographic Remoting. 
% These tracking data is sent to the 
% These processes are executed on Unity running on the remote PC (G-Tune, Intel Core i7-11800H 2.30GHz CPU, NVIDIA GeForce RTX 3070 GPU, 64GB RAM) and the local PC, respectively, through Holographic Remoting. 


% Figure environment removed

% Figure environment removed


\subsubsection*{\textbf{Synchronized Actuation with Tabletop Mobile Robots}}
Our system uses Sony Toio~\footnote{\url{https://www.sony.com/en/SonyInfo/design/stories/toio/}} as tabletop mobile robots. Each robot measures 3.2 cm × 3.2 cm × 2.5 cm and can move at a speed of up to 35 cm/sec for straight-line movement and 1500 deg/sec for rotation. The robot has a built-in camera that can scan patterns printed on a mat (Toio Tracking Mat) to detect their position and orientation. The size of the tracking mat has 55 cm × 55 cm of covered area, but it can be extended by aligning multiple mats. Each Toio robot is controlled using the Toio SDK for Unity~\footnote{\url{https://github.com/morikatron/toio-sdk-for-unity}} on a PC and continuously sends its position and orientation to the PC via Bluetooth\textregistered \  standard Ver. 4.2 every 10 ms. 
For the controlling algorithm, we adapt to the open-source library~\cite{nakagaki2020hermits} and rewrite the algorithm for our Unity application.
% To start using our application, the local user first calibrates the position of the Toio mat by doing  \todo{xxx} calibration process. 
To start using our application, the local user first performs a manual calibration to align the remote user's holographic mesh with the Toio mat.
This alignment can be bypassed in subsequent uses saving the relative position between the Toio mat and the avatar mesh. By placing a QR code on the Toio mat to acquire the mat's position and leveraging the relative position, we can display the avatar mesh in the appropriate position.

After the calibration, each robot's position is controlled through the following three ways: 1) physical Toio movement in the remote environment, 2) virtual object movement in the remote user's Hololens, or 3) finger position movement of the remote user. 
When both users have a physical Toio setup, the system can simply synchronize the position of each environment. 
On the other hand, when only the local user is equipped with the Toio robot, then the remote user can manipulate virtual Toios by grasping and manipulating virtual Toio objects rendered in the Hololens, while the local user manipulates physical Toios. 
Alternatively, the remote user can manipulate these Toio robots with hand and finger tracking. 
For the finger binding, we use the thumb, index, and/or pinky finger positions, depending on the available number of robots
The position data for each robot is sent between the remote and local PCs through UDP communication.
% In our implementation, the latency and average positional error for Toio movement is \todo{xxx} and \todo{xxx}, respectively.
In our implementation, we set the Toio robot's speed up to 17.5 cm/sec taking into account the balance between the speed and accuracy.
Therefore, if the remote user attempts to move the local Toio robot at speed higher than this, it may lead to positional errors.
Considering the tradeoff between precise movements and jittering, we set the default tolerance to 1.1 cm for all interactions, except for the miniature body interaction, where we set it to 0.4 cm since accuracy with the avatar’s body was more crucial than some small jittering.
Finally, to increase the reproducibility, we make our software open source~\footnote{\url{https://github.com/KeiichiIhara/HoloBots}}.

